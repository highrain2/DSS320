{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a94e08",
   "metadata": {},
   "source": [
    "## Introduction to Classification\n",
    "In the ***Analysis*** phase, we will take the cleansed data from the ***Process*** phase, and start building a classifier.  For classification, we will use the ***sci-kit learn*** package in Anaconda.  Install that by going to your Anaconda command line and typing ***conda install scikit-learn***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62218e98",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Let's start by growing a decision tree from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b999bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Workshop Functions\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from Wksp722_functions import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba341e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Salutation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Embarked Salutation  \n",
       "0      0         A/5 21171   7.2500        S        Mr.  \n",
       "1      0          PC 17599  71.2833        C       Mrs.  \n",
       "2      0  STON/O2. 3101282   7.9250        S      Miss.  \n",
       "3      0            113803  53.1000        S       Mrs.  \n",
       "4      0            373450   8.0500        S        Mr.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "df = pd.read_csv(\"titanic_train_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ab864",
   "metadata": {},
   "source": [
    "### Need for additional data processing\n",
    "The scikit-learn machine learning package can't take **categorical** data as input.  As a result, we will not be able to use the 'Name' or 'Ticket' columns.  \n",
    "\n",
    "For 'Sex', we can replace 'male' as 0 and 'female' as 1.  Similarly we can replace 'Embarked' as either 0,1, or 2 based on the value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e9859a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Salutation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n",
       "\n",
       "      Ticket     Fare Embarked Salutation  \n",
       "0  A/5 21171   7.2500        S        Mr.  \n",
       "1   PC 17599  71.2833        C       Mrs.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the categorical variable 'Sex' to numerical 0 and 1 using mapping\n",
    "mapping = {'male':0, 'female':1}\n",
    "df.loc[:,'Sex'] = df.loc[:,'Sex'].map(mapping)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c63802",
   "metadata": {},
   "source": [
    "For 'Embarked' there are 3 possible values: S, C, and Q.  Rather than assign them values of 0,1,2 respectively, let's use one-hot encoding to create 3 new columns for each value.  In the 'S' column, the value will be a 1 if the original 'Embarked' column has a 'S' as the value for that passenger, and a '0' otherwise.  Similarly for C and Q columns.  \n",
    "\n",
    "Let's also do the same with the Salutation.  \n",
    "\n",
    "Finally, let's drop 'PassengerId' as it's really just an index and will not help train the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8941f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp = pd.get_dummies(df.loc[:,['Embarked','Salutation']])\n",
    "\n",
    "df = pd.concat([df,dfTemp], axis=1)\n",
    "df.drop(['PassengerId','Embarked','Name','Ticket','Salutation'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b62ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0dc4e2",
   "metadata": {},
   "source": [
    "This seems like we just added a lot of extra columns, but it will make little difference to the computational speed for this dataset. \n",
    "\n",
    "I've saved all the changes we did in a function that we can use in the future.  It is located in the class library file.  This step is optional, but can be useful if you have test sets that you need to process in the exact same way as your training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4989c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#titanicNumericalConverter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15403221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(0).to_csv('titanic_train_columns.csv') # <-- we'll need this information later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95a2c9",
   "metadata": {},
   "source": [
    "### Growing our first classifier - Decision Tree\n",
    "Now that all the variables are **numerical**, we can use scikit-learn to grow our classifiers.  We'll start by designating one of the columns as the **target variable (y)** and the others will be the **input (x)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc39be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=\"Survived\")\n",
    "y = df.loc[:,'Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of survivors is less than the number that didn't survive.  \n",
    "# Though this is an imbalance, there are several hundred examples of each and should suffice for training our algorithms.\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc2c8c",
   "metadata": {},
   "source": [
    "Now we'll split the dataframe into a training and test set.  We'll choose the training set to be 70% of the original size and the test set to be 30%.  \n",
    "\n",
    "We'll use a built-in function from the scikit-learn (sklearn) package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432f8d7",
   "metadata": {},
   "source": [
    "The scikit-learn package has many different kinds of classifiers.  They generally share many common features in how you use them, which makes it easy to call compare the performance of different classifiers.\n",
    "\n",
    "**The usual sequence of steps of using most classifiers in scikit-learn** is:\n",
    "* split data into training and testing sets using ***train_test_split***\n",
    "* import and set the desired classifier\n",
    "* fit the model to the training data\n",
    "* predict for the test set using the ***predict*** function\n",
    "* measure success using accuracy, f1, precision, recall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree1 = DecisionTreeClassifier(max_depth =3, random_state = 1)\n",
    "tree1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb48bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "\n",
    "featureNames = x_train.columns\n",
    "targetNames = ['Did Not Survive','Survived']\n",
    "tree.plot_tree(tree1, feature_names=featureNames,  \n",
    "                   class_names=targetNames,\n",
    "                   filled=True)\n",
    "\n",
    "fig.savefig('tree1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c128d2",
   "metadata": {},
   "source": [
    "***Curiosity points (5 Points)***\n",
    "Another cool package for visualization is dtreeviz.  Install the package with ***conda install dtreeviz***.\n",
    "\n",
    "See the kaggle notebook at this link (https://www.kaggle.com/code/immu123/decision-tree-visualization-with-dtreeviz?scriptVersionId=101370052) and try visualizing your tree this way.  Below is an example of what the package can do with the iris dataset:\n",
    "\n",
    "<div>\n",
    "<img src = \"dtreeviz_example.PNG\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8244f",
   "metadata": {},
   "source": [
    "Some branches lead to a leaf, but many are still mixed at their terminal nodes.  Let's see the accuracy with a relatively short tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894116a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = tree1.predict(x_test)\n",
    "\n",
    "print(\"Tree1 Confusion Matrix \\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Tree1 Classification Report \\n\", metrics.classification_report(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Tree1 Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69616a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grow a full length decision tree and check accuracy\n",
    "tree2 = DecisionTreeClassifier(max_depth=None, random_state = 1)\n",
    "tree2.fit(x_train, y_train)\n",
    "y_pred2 = tree2.predict(x_test)\n",
    "print(\"Tree2 Accuracy:\",metrics.accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5463bad8",
   "metadata": {},
   "source": [
    "***Curiosity Points (5 points)*** \n",
    "Why did the accuracy drop when we grew a larger tree?  \n",
    "(hint: use tree2.tree_.max_depth to see how large your tree grew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a0a4e",
   "metadata": {},
   "source": [
    "## Random Forest classification\n",
    "\n",
    "In this section, let's use the Random Forest algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cab287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF1 = RandomForestClassifier(n_estimators=125, max_depth=None, oob_score=True, random_state=1)\n",
    "RF1.fit(x_train, y_train)\n",
    "\n",
    "y_pred = RF1.predict(x_test)\n",
    "print(\"RF1 Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abcef2",
   "metadata": {},
   "source": [
    "### Variable Importance\n",
    "Let's extract and plot the variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = RF1.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=x_train.columns) #cast the list into a Pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a98a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c6d9d",
   "metadata": {},
   "source": [
    "I created an additional function that plots the test error and optional Out of Bag (OOB) error as the forest grows.  It is located in the class library file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def EnsembleGrowthErrorPlot(clf,x_train,y_train,x_test,y_test,min_estimators=5,max_estimators=200,oob=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450796dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleGrowthErrorPlot(RF1, x_train, y_train, x_test, y_test, min_estimators =5, max_estimators =200, oob =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203fd727",
   "metadata": {},
   "source": [
    "We see that there is a large drop in error around 25 trees, and a slighter drop in error around 120 trees.  Also there is good agreement between the test and out-of-bag error rates.  \n",
    "\n",
    "Let's see how other things effect accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa77240",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error = []\n",
    "for i in range(2,25,1):\n",
    "        RF2 = RandomForestClassifier(n_estimators=125, max_depth=None, oob_score=True, random_state=1, min_samples_split=i)\n",
    "        RF2.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = RF2.predict(x_test)\n",
    "        acc = metrics.accuracy_score(y_test, y_pred)\n",
    "        test_error.append((i,1-acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c125ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Test set Error rate as a function of minimum number of observations in a node before it can be split\n",
    "fig, axes = plt.subplots()\n",
    "x,test_error_plot = zip(*test_error)\n",
    "axes.plot(x, test_error_plot)\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.ylabel(\"error percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf4d94",
   "metadata": {},
   "source": [
    "Based on our observations, a good Random Forest for our dataset will have:\n",
    "* at least 25 decision trees. Let's choose 50 just in case\n",
    "* min_samples_split between 10 and 13.  Let's choose 12.  \n",
    "\n",
    "Let's regrow a new Random Forest with these settings and save so it can be loaded again in a new Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363904a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Final = RandomForestClassifier(n_estimators=50, min_samples_split=12, max_depth=None, oob_score=True, random_state=1)\n",
    "RF_Final.fit(x_train, y_train)\n",
    "\n",
    "y_pred = RF_Final.predict(x_test)\n",
    "print(\"RF_Final Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2812c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the Random Forest to be used later in another Jupyter Notebook\n",
    "import pickle\n",
    "\n",
    "pickle.dump(RF_Final,open('RF_Final.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2362e",
   "metadata": {},
   "source": [
    "***Curiosity points (5 points)***\n",
    "Play around with the ***max_depth*** variable to the RandomForestClassifier and see if that makes a difference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d52a8",
   "metadata": {},
   "source": [
    "### Boosted decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "AB1 = AdaBoostClassifier(n_estimators=125, random_state=1)\n",
    "AB1.fit(x_train, y_train)\n",
    "\n",
    "y_pred = AB1.predict(x_test)\n",
    "print(\"AB1 (Adaboost) Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleGrowthErrorPlot(AB1, x_train, y_train, x_test, y_test, min_estimators=5, max_estimators =200, oob =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d33cf",
   "metadata": {},
   "source": [
    "Generally, Adaboost suffers from **overfitting** when you increase the number of trees beyond a certain point.  For our test set, it looks like this number was around 50 trees, but the effect is slight.  Let's see if Gradient boost will be better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8aa4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GB1 = GradientBoostingClassifier(n_estimators=125,learning_rate=1.0, random_state=1, max_depth=3)\n",
    "\n",
    "EnsembleGrowthErrorPlot(GB1, x_train, y_train, x_test, y_test, min_estimators =5, max_estimators =200, oob =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66c73a",
   "metadata": {},
   "source": [
    "Gradient Boost shows less susceptability to overfittiing, but a higher error rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e03d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
