<!DOCTYPE html>
<!-- saved from url=(0058)https://learn.cce.af.mil/article/common-misconceptions-bda -->
<html lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta data-n-head="ssr" data-hid="description" name="description" content="MIT Horizon Platform"><meta data-n-head="ssr" name="viewport" content="width=device-width, initial-scale=1"><title>Common Misconceptions | MIT Horizon</title><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/9fc7f3d.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/c60dc67.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/c5d4f58.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/7dfbd03.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/834a546.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/80fc188.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/6c1e899.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/2148f53.js.download" as="script"><link rel="preload" href="./Common Misconceptions _ MIT Horizon_files/6bb4c98.js.download" as="script"><style data-vue-ssr-id="4b70edf3:0 162f72a5:0 2b5ea15a:0 5c45cfc8:0 0ed17058:0 9b23a37c:0 e0762344:0 98c604d2:0 e45e51e2:0 d5f7e78e:0 41d1ec36:0 11720cff:0 c4ab8424:0 0117ea5e:0 5892bb08:0 e71c8ce8:0 631c9f56:0 1c7d3052:0 90fd23f0:0 1191d07c:0 c496ce3e:0 6c52a1be:0 167e8dca:0 88a0300e:0 4e209261:0 eb5d6faa:0 26bf448a:0 3b7cbd64:0 2c059276:0 77e02d79:0 103bcfa0:0 6132f894:0 49b8dba8:0 097b0b3c:0 84d8ea28:0 d6849f4a:0 057d7257:0 071db6da:0 73b97e28:0 5a9ef3a3:0">html{box-sizing:border-box;font-size:62.5%}*,:after,:before{box-sizing:inherit;font-family:var(--font-avenir)}body{color:var(--color-black)}body,h1,h2,h3,h4,p{margin:0;padding:0}a{font-family:inherit;-webkit-text-decoration:none;text-decoration:none}a,button{cursor:pointer;-webkit-user-select:none;-moz-user-select:none;user-select:none}button{background-color:transparent;border:none;margin:0;padding:0;transition:all .3s ease}button:active{opacity:.8}ul{list-style-position:inside;list-style-type:none}ol,ul{margin:0;padding:0}ol{list-style-position:outside}blockquote{margin:0;padding:0}figure,hr{margin:0}em,strong{font-family:inherit}
:root{--color-black:#000;--color-corduroy:#6f7271;--color-mexican-red:#a31f34;--color-mine-shaft:#333;--color-mono:#4c4d4d;--color-mystic:#e5edeb;--color-seashell:#f1f1f1;--color-swiss-coffee:#d2cecc;--color-white:#fff;--color-ziggurate:#b6dce1;--color-gray-course:#f3f3f3;--color-gray-line:#d8d8d8;--font-avenir:"Montserrat","Lato",sans-serif;--font-georgia:"Tinos","Lora",serif;--font-charter:"Lora",serif;--color-grey-suit:#8b8b8c;--color-grey-suit-alternative:#757575;--color-blue-whale:#1e344a;--background-color-primary-hover:#7d1526;--background-color-primary-disabled:#e0a6af;--color-secondary-hover:#006838;--border-color-secondary:#34724a;--background-color-secondary:#c1e6d3;--background-color-secondary-hover:#6ea887;--color-secondary-disabled:#4d5156;--background-color-secondary-disabled:#c4c4c4;--color-unchecked:#c4c4c4;--color-tiles:#a0a0a0}
@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Gelasio-400-1.529db7f.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Gelasio-400-2.d696cb7.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Gelasio-400-3.6f583d6.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Gelasio-500-4.c6ebc56.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Gelasio-500-5.04f95dd.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Gelasio-500-6.c532a93.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Gelasio-600-7.432251c.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Gelasio-600-8.14e96ca.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Gelasio-600-9.3db8a62.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Gelasio-700-10.0828cf1.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Gelasio-700-11.2d09f33.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Gelasio-700-12.37d9aa7.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Gelasio-400-13.fd2022e.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Gelasio-400-14.94e6192.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Gelasio-400-15.1c1d935.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Gelasio-500-16.31cf616.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Gelasio-500-17.360732e.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Gelasio-500-18.7ef6bad.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Gelasio-600-19.568966c.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Gelasio-600-20.4992155.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Gelasio-600-21.39f2e9e.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Gelasio-700-22.5e1ae38.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Gelasio-700-23.7d4a292.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Gelasio";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Gelasio-700-24.83cc6dc.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Lato-300-25.eaf671b.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Lato-300-26.716309a.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lato-400-27.4bde07f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lato-400-28.e1b3b59.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lato-700-29.a48b0f0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lato-700-30.de69cf9.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Lato-900-31.4b5f220.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Lato";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Lato-900-32.1c6c655.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lora-400-33.890c2f6.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lora-400-34.961fdc4.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lora-400-35.cf841e4.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lora-400-36.100631f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Lora-400-37.78be9c1.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lora-700-38.890c2f6.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lora-700-39.961fdc4.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lora-700-40.cf841e4.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lora-700-41.100631f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Lora";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Lora-700-42.78be9c1.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-43.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-44.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-45.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-46.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-47.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-48.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-49.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-50.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-51.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-52.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-53.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-54.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-55.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-56.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-57.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-58.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-59.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-60.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-61.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-62.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-63.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-64.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-65.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-66.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-67.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-68.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-69.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-70.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-71.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-72.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-73.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-74.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-75.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-76.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-77.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-78.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-79.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-80.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-81.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-82.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-83.fa6a4c9.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-84.ae1ea3d.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-85.12e5ff2.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-86.49b88e0.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:italic;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-87.fda4d0b.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-88.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-89.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-90.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-91.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:100;src:url(/_nuxt/fonts/Montserrat-100-92.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-93.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-94.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-95.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-96.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:200;src:url(/_nuxt/fonts/Montserrat-200-97.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-98.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-99.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-100.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-101.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:300;src:url(/_nuxt/fonts/Montserrat-300-102.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-103.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-104.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-105.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-106.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Montserrat-400-107.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-108.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-109.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-110.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-111.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:500;src:url(/_nuxt/fonts/Montserrat-500-112.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-113.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-114.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-115.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-116.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:600;src:url(/_nuxt/fonts/Montserrat-600-117.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-118.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-119.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-120.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-121.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Montserrat-700-122.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-123.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-124.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-125.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-126.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:800;src:url(/_nuxt/fonts/Montserrat-800-127.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-128.3ca6742.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-129.59c9b83.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-130.0ce1283.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-131.9dd150f.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Montserrat";font-style:normal;font-weight:900;src:url(/_nuxt/fonts/Montserrat-900-132.ac0d285.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-133.bfa66bf.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-134.a1b43a2.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-135.4f4fed8.woff2) format("woff2");unicode-range:u+1f??}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-136.6e504c7.woff2) format("woff2");unicode-range:u+0370-03ff}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-137.ca8d6a1.woff2) format("woff2");unicode-range:u+0590-05ff,u+200c-2010,u+20aa,u+25cc,u+fb1d-fb4f}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-138.bea6c0f.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-139.44bd6c1.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-140.e37a957.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-141.c8dd104.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-142.aecd62a.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-143.f754383.woff2) format("woff2");unicode-range:u+1f??}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-144.a3a68bc.woff2) format("woff2");unicode-range:u+0370-03ff}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-145.870c52c.woff2) format("woff2");unicode-range:u+0590-05ff,u+200c-2010,u+20aa,u+25cc,u+fb1d-fb4f}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-146.ffc57ae.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-147.ee7a866.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Tinos";font-style:italic;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-148.0a5d6a4.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-149.065c651.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-150.cff8f70.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-151.560aaf8.woff2) format("woff2");unicode-range:u+1f??}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-152.dcf4137.woff2) format("woff2");unicode-range:u+0370-03ff}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-153.e011713.woff2) format("woff2");unicode-range:u+0590-05ff,u+200c-2010,u+20aa,u+25cc,u+fb1d-fb4f}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-154.8b10e18.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-155.79808f9.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:400;src:url(/_nuxt/fonts/Tinos-400-156.c5bb415.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-157.53d5615.woff2) format("woff2");unicode-range:u+0460-052f,u+1c80-1c88,u+20b4,u+2de0-2dff,u+a640-a69f,u+fe2e-fe2f}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-158.a8db001.woff2) format("woff2");unicode-range:u+0301,u+0400-045f,u+0490-0491,u+04b0-04b1,u+2116}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-159.da415e5.woff2) format("woff2");unicode-range:u+1f??}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-160.6e7c2dc.woff2) format("woff2");unicode-range:u+0370-03ff}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-161.18d62f4.woff2) format("woff2");unicode-range:u+0590-05ff,u+200c-2010,u+20aa,u+25cc,u+fb1d-fb4f}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-162.811ea57.woff2) format("woff2");unicode-range:u+0102-0103,u+0110-0111,u+0128-0129,u+0168-0169,u+01a0-01a1,u+01af-01b0,u+1ea0-1ef9,u+20ab}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-163.6ffddc6.woff2) format("woff2");unicode-range:u+0100-02af,u+1e??,u+2020,u+20a0-20ab,u+20ad-20cf,u+2113,u+2c60-2c7f,u+a720-a7ff}@font-face{font-display:swap;font-family:"Tinos";font-style:normal;font-weight:700;src:url(/_nuxt/fonts/Tinos-700-164.9e9a603.woff2) format("woff2");unicode-range:u+00??,u+0131,u+0152-0153,u+02bb-02bc,u+02c6,u+02da,u+02dc,u+2000-206f,u+2074,u+20ac,u+2122,u+2191,u+2193,u+2212,u+2215,u+feff,u+fffd}
.nuxt-progress{background-color:#fff;height:2px;left:0;opacity:1;position:fixed;right:0;top:0;transition:width .1s,opacity .4s;width:0;z-index:999999}.nuxt-progress.nuxt-progress-notransition{transition:none}.nuxt-progress-failed{background-color:red}
#layout-default[data-v-2efbf2a4]{display:flex;flex-direction:column;min-height:100vh}#layout-default[data-v-2efbf2a4] .sr-only{clip:rect(0,0,0,0);border:0;height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#layout-default[data-v-2efbf2a4] .sr-only:active,#layout-default[data-v-2efbf2a4] .sr-only:focus{background-color:#000;border:4px solid #ff0;border-radius:15px;color:#fff;font-size:1.2em;height:auto;left:auto;margin:10px 35%;overflow:auto;padding:5px;text-align:center;top:auto;width:30%;z-index:999}
#header[data-v-572edd00]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;background-color:var(--color-black);display:grid;display:-ms-grid;grid-template-columns:1fr minmax(330px,1170px) 1fr;position:sticky;top:0;z-index:1}#header[data-v-572edd00],.content[data-v-572edd00]{align-items:center}.content[data-v-572edd00]{display:flex;grid-column:2;-ms-grid-column:2;height:56px;padding:0 30px;position:relative}.mit-logo[data-v-572edd00]{margin-left:15px}.c-Ribbon[data-v-572edd00]{background-color:#fff;color:#000;font-size:15px;font-weight:400;height:28px;left:-31px;line-height:28px;position:fixed;text-align:center;text-transform:uppercase;top:11px;transform:rotate(-45deg);width:110px;z-index:1075}@media screen and (max-width:1125px){#header[data-v-572edd00],.content[data-v-572edd00]{height:48px}.content[data-v-572edd00]{justify-content:center;padding:0 20px}}
#header-menu[data-v-f1571146]{display:none}#header-menu[data-v-f1571146]:focus{border:4px solid}.icon[data-v-f1571146]{fill:var(--color-white);height:13px;width:16px}@media screen and (max-width:1125px){#header-menu[data-v-f1571146]{display:flex;left:20px;position:absolute}}
#header-logo[data-v-cc0202ae]{line-height:0}.icon[data-v-cc0202ae]{fill:var(--color-white);height:18px;width:204px}@media screen and (max-width:1125px){.icon[data-v-cc0202ae]{margin-left:20px}}
.st0[data-v-c9a8a4ae]{display:none}.st1[data-v-c9a8a4ae]{display:inline}.st2[data-v-c9a8a4ae]{fill:#231f20}.st3[data-v-c9a8a4ae]{display:inline}.st3[data-v-c9a8a4ae],.st4[data-v-c9a8a4ae]{fill:#a31f34}.st5[data-v-c9a8a4ae]{fill:#fff}
#header-navigation[data-v-39b4582a]{flex-grow:1;margin-left:25px;overflow:hidden;white-space:nowrap}.link[data-v-39b4582a]{align-items:center;border-bottom:2px solid transparent;color:var(--color-swiss-coffee);display:inline-flex;font-size:1.6rem;height:30px;justify-content:center;letter-spacing:.04rem;min-width:70px;opacity:.8;transition:all .4s ease}.link.active[data-v-39b4582a],.link[data-v-39b4582a]:hover{border-bottom-color:var(--color-white);color:var(--color-white);opacity:1}.link+.link[data-v-39b4582a]{margin-left:15px}@media screen and (max-width:1125px){#header-navigation[data-v-39b4582a]{display:none}}
#header-search[data-v-2af08a1e]{align-items:center;background-color:transparent;border:1px solid var(--color-white);display:flex;height:32px;justify-content:center;justify-self:start;margin-left:auto;overflow:hidden;padding:0 10px;width:150px}#header-search input[data-v-2af08a1e]{background:none;border:none;color:var(--color-white);font-size:1.5rem;font-weight:100;margin-left:8px;width:calc(100% - 20px)}#header-search input[data-v-2af08a1e]::-moz-placeholder{color:var(--color-white);font-size:1.5rem;font-weight:100}#header-search input[data-v-2af08a1e]::placeholder{color:var(--color-white);font-size:1.5rem;font-weight:100}.label[data-v-2af08a1e]{align-items:center;display:flex}.icon[data-v-2af08a1e]{fill:var(--color-white);stroke:var(--color-white);height:14px;width:14px}@media screen and (max-width:1125px){#header-search[data-v-2af08a1e]{display:none}}
#header-account[data-v-c651f032]{align-items:center;color:var(--color-white);display:inline-flex;font-size:1.5rem;font-weight:300;height:40px;margin-left:25px}.icon[data-v-c651f032]{fill:var(--color-white);margin-left:10px;width:14px}@media screen and (max-width:1125px){#header-account[data-v-c651f032]{display:none}}
.banner[data-v-76081bd4]{background-color:var(--color-mexican-red)}
div[data-v-3e85744e]{background-color:var(--color-blue-whale)}.banner-message[data-v-3e85744e]{padding:10px;text-align:center}.banner-message a[data-v-3e85744e]{color:var(--color-white);font-size:1.6rem}
#sidebar[data-v-04c72ac8]{height:100vh;overflow:hidden;position:fixed;transition:all .2s ease;width:0;z-index:2}#sidebar.open[data-v-04c72ac8]{width:100vw}.mask[data-v-04c72ac8]{background-color:var(--color-black);cursor:pointer;height:100%;opacity:.3;width:100%}.content[data-v-04c72ac8]{background-color:var(--color-white);display:flex;flex-direction:column;height:100%;position:absolute;top:0;width:224px}.separator[data-v-04c72ac8]{border:0;border-top:4px solid var(--color-seashell);margin:0;padding:0;width:100%}
#main[data-v-4aa1c1da]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:1fr minmax(330px,1170px) 1fr;grid-template-rows:1fr minmax(760,375px)}.connected-articles-wrapper[data-v-4aa1c1da]{--column-two-gap:20px;--column-two-width:250px;grid-column-gap:40px;-ms-grid-columns:220px 1fr 250px;background-color:var(--color-seashell);display:grid;display:-ms-grid;grid-column:1/4;-ms-grid-column:2;grid-row:2;grid-template-columns:1fr 1fr 1fr;padding:47px 100px}.connected-articles[data-v-4aa1c1da]{grid-column-gap:15px;-moz-column-gap:15px;column-gap:15px;display:grid;grid-column:2;grid-row:1/3;grid-template-columns:repeat(3,1fr);max-width:600px}.connected-article[data-v-4aa1c1da]{background-color:var(--color-mystic);display:flex;flex-direction:column;max-width:190px;transition:box-shadow .3s ease-in-out;width:100%}.connected-article-image[data-v-4aa1c1da]{display:flex;position:relative}.connected-article-image[data-v-4aa1c1da]:after{background-color:rgba(0,0,0,.5);bottom:0;content:"";display:block;height:100%;left:0;position:absolute;right:0;top:0;transition:background-color .3s ease-in-out;width:100%;z-index:100}.connected-article[data-v-4aa1c1da]:hover{box-shadow:6px 6px 10px rgba(0,0,0,.1);cursor:pointer}.connected-article:hover .connected-article-image[data-v-4aa1c1da]:after{background-color:transparent;cursor:pointer}.connected-article-title[data-v-4aa1c1da]{font-size:20px;font-weight:600;margin:10px 14.5px}.connected-article-time[data-v-4aa1c1da],.connected-article-title[data-v-4aa1c1da]{color:var(--color-black);font-family:var(--font-avenir);font-style:normal;line-height:30px}.connected-article-time[data-v-4aa1c1da]{font-size:14px;margin:0 14.5px}.article[data-v-4aa1c1da]{--column-two-gap:20px;--column-two-width:250px;grid-column-gap:20px;grid-row-gap:30px;-ms-grid-columns:220px 1fr 250px;display:grid;display:-ms-grid;grid-column:2;-ms-grid-column:2;grid-template-columns:220px 1fr 250px;grid-template-columns:220px 1fr var(--column-two-width);padding:30px}:root .article>*+*[data-v-4aa1c1da],_[data-v-4aa1c1da]:-ms-fullscreen{margin-top:30px}@media screen and (max-width:960px){.connected-articles-wrapper[data-v-4aa1c1da]{grid-column-gap:0;grid-column: 0 1fr 0;padding:47px 30px}.article[data-v-4aa1c1da]{grid-column-gap:0;grid-template-columns:0 1fr}}@media screen and (max-width:760px){.connected-articles-wrapper[data-v-4aa1c1da]{height:auto}.connected-articles[data-v-4aa1c1da]{grid-template-columns:1fr;grid-template-rows:3fr;justify-items:center;row-gap:15px}}
.article .markdown .highlight-text{color:var(--color-mexican-red);color:#a31f34;font-family:var(--font-avenir);font-size:20px;font-weight:700;line-height:30px;-webkit-text-decoration:none;text-decoration:none;transition:all .3s ease}.article .markdown p{font-family:Lora,serif;font-family:var(--font-charter);font-size:1.7rem;line-height:3rem}.article .markdown p+p{margin-top:23px}.article .markdown li{font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem}.article .markdown ol li{margin-left:20px}.article .markdown a{color:var(--color-mexican-red);color:#a31f34;font-family:var(--font-charter);font-size:1.7rem;font-weight:500;-webkit-text-decoration:underline;text-decoration:underline;transition:all .3s ease}.article .markdown a:active,.article .markdown a:focus{opacity:.4}.article .markdown a:hover{-webkit-text-decoration:underline;text-decoration:underline}.article .markdown em{font-family:var(--font-charter);font-family:Lora,serif}.article .markdown .glossary-term{border-bottom:3px solid var(--color-swiss-coffee);font-family:Lora,serif;padding:0 0 1px}.article .markdown .glossary-term:hover{border-bottom:3px solid var(--color-mexican-red)}.article .markdown .glossary-term .glossary-definition{background-color:#fff;border:3px solid var(--color-mexican-red);display:none;font-family:Lora,serif;font-size:1.7rem;line-height:3rem;margin-top:10px;padding:15px 20px;position:absolute;transform:translateX(100px);width:400px;z-index:2}.article .markdown .glossary-term:hover .glossary-definition{display:block}@media screen and (max-width:960px){.article .markdown .glossary-term .glossary-definition{transform:translateX(calc(50vw - 230px))}}.badges{display:flex;flex-wrap:wrap;margin-top:10px!important}.badge{background-color:var(--color-swiss-coffee);color:var(--color-black);display:flex;font-family:var(--font-avenir);font-size:16px;font-weight:400;line-height:20px;margin-right:20px;margin-top:10px;padding:5px 10px}
.article-breadcrumbs[data-v-6e2f4a60]{-ms-grid-column-span:3;align-items:center;display:flex;flex-basis:100%;flex-wrap:wrap;grid-column:1/span 3;-ms-grid-column:1;-ms-grid-row:1}.link[data-v-6e2f4a60]{font-size:1.6rem;height:25px;margin-right:5px}.topic[data-v-6e2f4a60]:not(:nth-last-child(2)):after{content:", "}@media screen and (max-width:960px){.article-breadcrumbs[data-v-6e2f4a60]{align-items:flex-start;flex-direction:column}.link[data-v-6e2f4a60]{height:auto;min-height:2rem}.link+.link[data-v-6e2f4a60]{margin-top:8px}}
.horizon-nuxt-link{align-items:center;color:var(--color-mexican-red);display:inline-flex;font-size:1.4rem;font-weight:500;transition:all .3s ease}.horizon-nuxt-link:active,.horizon-nuxt-link:focus{opacity:.4}.horizon-nuxt-link:hover{-webkit-text-decoration:underline;text-decoration:underline}.horizon-nuxt-link>.icon{fill:var(--color-mexican-red);stroke:var(--color-mexican-red);margin-left:6px;width:.9rem}
.article-header[data-v-04cf394c]{grid-column:2;-ms-grid-column:2;-ms-grid-row:2}.article-header>*+*[data-v-04cf394c]{margin-top:20px}
.article-title[data-v-de2ee628]{align-self:center;color:var(--color-black);font-size:3.7rem;font-weight:500;line-height:4.3rem}
.author[data-v-0c35bf98],.date[data-v-0c35bf98],.reading-time[data-v-0c35bf98],.separator[data-v-0c35bf98]{color:var(--color-corduroy);font-size:1.6rem;font-weight:400}.separator[data-v-0c35bf98]{margin:0 8px}.separator[data-v-0c35bf98]:before{content:"\25E6"}
.article-navigation[data-v-7e9f4ce0]{grid-column:1;-ms-grid-column:1;grid-row:3;-ms-grid-row:3}.navigation[data-v-7e9f4ce0]{display:flex;flex-direction:column;padding:10px 10px 0 0;position:sticky;top:112px}:root .navigation[data-v-7e9f4ce0],_[data-v-7e9f4ce0]:-ms-fullscreen{margin-right:15px}.heading[data-v-7e9f4ce0]{font-size:1.7rem;font-weight:600}.link[data-v-7e9f4ce0]{font-size:1.6rem;margin-top:1.7rem}@media screen and (max-width:960px){.article-navigation[data-v-7e9f4ce0]{display:none}}
.horizon-link{align-items:center;color:var(--color-mexican-red);display:inline;font-size:1.4rem;font-weight:500;transition:all .3s ease}.horizon-link:active,.horizon-link:focus{opacity:.4}.horizon-link:hover{-webkit-text-decoration:underline;text-decoration:underline}
.article-content[data-v-576aadda]{-ms-grid-column-span:2;grid-column:2/span 2;-ms-grid-column:2;grid-row:3;-ms-grid-row:3}.article-content[data-v-576aadda] a{-webkit-user-select:inherit;-moz-user-select:inherit;user-select:inherit}@media screen and (max-width:960px){.article-content[data-v-576aadda]{grid-column:2}}
.article-content>.article-heading+.article-paragraph:first-of-type,.article-content>:not(.article-subheading)+*{margin-top:23px}.article-content>.sidebar:first-child+*{margin-top:0}
@media screen and (min-width:961px){.article-blockquote,.article-callout,.article-divider,.article-heading,.article-paragraph,.article-subheading,.article-term,.article-video.body,.article-web{max-width:calc(100% - var(--column-two-width) - var(--column-two-gap))}:root .article-blockquote,:root .article-callout,:root .article-divider,:root .article-heading,:root .article-paragraph,:root .article-player,:root .article-subheading,:root .article-term,:root .article-video.body,:root .article-web,_:-ms-fullscreen{max-width:calc(100% - 270px)}.article-list{max-width:calc(100% - var(--column-two-width) - var(--column-two-gap) - 20px)}:root .article-list,_:-ms-fullscreen{max-width:calc(100% - 290px)}}.knowledge-check{background-color:var(--color-gray-course);display:flex;flex-direction:column;max-width:calc(100% - var(--column-two-width) - var(--column-two-gap));padding:15px 18px 22px}.knowledge-check-title{display:flex;font-family:var(--font-avenir);font-size:12px;font-style:normal;font-weight:600;line-height:12px;margin-bottom:16px;margin-right:39px;text-transform:uppercase}.knowledge-check-question-wrapper{display:flex;justify-content:space-between}.knowledge-check-hint,.knowledge-check-question{font-family:var(--font-avenir);font-size:15px;font-style:normal;font-weight:600;line-height:25px;max-width:280px;width:100%}.knowledge-check-hint{margin-bottom:9px}.knowledge-check-answers{align-self:flex-end;display:flex;flex-direction:column;max-width:248px}.knowledge-check-answer-wrapper{border:1px solid var(--color-black);display:flex;margin-bottom:16px;padding:10px}.knowledge-check-answer{font-family:var(--font-avenir);font-size:13px;font-style:normal;font-weight:600;line-height:18px;margin-right:12px}.knowledge-section-actions{display:flex;justify-content:space-between}.knowledge-sections-indicators{align-items:center;display:flex}.knowledge-sections-count{margin-right:8px}.knowledge-sections-tiles{display:flex}.knowledge-sections-tile{background-color:var(--color-tiles);height:10px;margin-right:3px;width:25px}.knowledge-sections-tile:first-child{border-radius:90px 0 0 90px}.knowledge-sections-tile:last-child{border-radius:0 90px 90px 0}.knowledge-sections-tile-active{background-color:var(--color-black)}@media screen and (max-width:960px){.knowledge-check{max-width:none}.knowledge-check-question-wrapper{flex-direction:column;justify-items:flex-start}.knowledge-check-question{margin-bottom:20px}.knowledge-check-answers{align-self:flex-start;margin-bottom:20px}}
.article-player{margin-bottom:-100px;min-width:301px;width:100%}.listen{font-size:1.5em;padding:3px}@media screen and (min-width:961px){.article-player{width:66%}}
.c1podcast-container[data-v-0a1aeb67]{border:1px solid #eee;display:flex;flex-direction:column;height:200px;padding:12px;width:600px}.c1podcast-toprow[data-v-0a1aeb67]{align-items:center;display:flex;flex-direction:row;justify-content:space-between}.c1podcast-toprow-text[data-v-0a1aeb67]{margin-left:25px;overflow:hidden}.c1podcast-toprow-title[data-v-0a1aeb67]{color:#666;font-size:25px;font-weight:800;transform:translateX(0);transition:2s;white-space:nowrap}.c1podcast-toprow-title[data-v-0a1aeb67]:hover{transform:translateX(-100%)}.c1podcast-toprow-artist[data-v-0a1aeb67]{color:#666;font-size:15px;font-weight:600;opacity:.7}.c1podcast-artwork[data-v-0a1aeb67]{border-radius:5px;height:125px;width:125px}.plyr-container[data-v-0a1aeb67]{height:20px;width:100%}.plyr-simplified[data-v-0a1aeb67]{height:20px;margin-bottom:100px;width:500px}
.article-heading[data-v-2d715583]{position:relative}.article-heading[data-v-2d715583]:not(:first-of-type){margin-top:40px}.heading[data-v-2d715583]{border-bottom:6px solid var(--color-mexican-red);color:var(--color-black);display:inline;font-size:2.6rem;font-weight:600;line-height:4.5rem;padding-right:20px}.target[data-v-2d715583]{position:absolute;top:-80px}
.article-paragraph h4{font-family:var(--font-charter);font-size:1.7rem;font-weight:700;line-height:3rem}.article-paragraph>p+h4{margin-top:23px}
.article-paragraph[data-v-eec403c2]{color:var(--color-black);font-family:var(--font-charter);font-size:1.7rem;line-height:3rem}
.article-image.body[data-v-4b0c7309]{width:calc(100% - var(--column-two-width) - var(--column-two-gap))}:root .article-image.body[data-v-4b0c7309],_[data-v-4b0c7309]:-ms-fullscreen{width:calc(100% - 270px)}.article-image.full[data-v-4b0c7309]{width:100%}.article-image.body>.image[data-v-4b0c7309],.article-image.full>.image[data-v-4b0c7309]{-o-object-fit:contain;object-fit:contain}.article-image.full>.image[data-v-4b0c7309]{-o-object-position:left;object-position:left}.article-image.sidebar[data-v-4b0c7309]{float:right;width:220px}.image[data-v-4b0c7309]{width:100%}:root .image[data-v-4b0c7309],_[data-v-4b0c7309]:-ms-fullscreen{height:100%}.caption[data-v-4b0c7309]{color:var(--color-corduroy);line-height:2.1rem;margin-top:5px;width:100%}.caption[data-v-4b0c7309],.caption[data-v-4b0c7309] a{font-family:var(--font-charter);font-size:1.4rem}.caption[data-v-4b0c7309] a{color:var(--color-mexican-red);font-weight:500;-webkit-text-decoration:underline;text-decoration:underline;transition:all .3s ease}.caption[data-v-4b0c7309] a:active,.caption[data-v-4b0c7309] a:focus{opacity:.4}.caption[data-v-4b0c7309] a:hover{-webkit-text-decoration:underline;text-decoration:underline}.caption[data-v-4b0c7309] em{font-family:var(--font-charter)}@media screen and (max-width:960px){.article-image.body[data-v-4b0c7309],.article-image.full[data-v-4b0c7309],.article-image.sidebar[data-v-4b0c7309]{float:none;width:100%}.image[data-v-4b0c7309]{max-height:50vh;max-width:100%;-o-object-fit:contain;object-fit:contain;width:auto}}.lightbox-caption-container[data-v-4b0c7309]{margin-top:1rem;min-width:100%;width:0}.lightbox-caption-container .caption[data-v-4b0c7309]{color:#fff;font-size:1.6rem}
.lightbox[data-v-632b4e38]{align-items:center;background:rgba(0,0,0,.85);display:flex;height:100%;justify-content:center;left:0;overflow:auto;padding:0 1rem;position:fixed;top:0;width:100%;z-index:1000}.lightbox__thumbnail[data-v-632b4e38]{cursor:zoom-in;height:100%;width:100%}.lightbox__thumbnail img[data-v-632b4e38]{width:100%}.lightbox__close[data-v-632b4e38]{color:#fff;cursor:pointer;font-size:5rem;height:4rem;padding:1rem;position:fixed;right:0;top:0;width:6rem}#close-image[data-v-632b4e38]{fill:var(--color-white);cursor:pointer}.lightbox__arrow--invisible[data-v-632b4e38]{visibility:hidden}.lightbox__element[data-v-632b4e38]{max-width:80%}.lightbox__arrow[data-v-632b4e38]{align-items:center;cursor:pointer;display:flex;justify-content:center;padding:0 2rem}.lightbox__arrow svg[data-v-632b4e38]{fill:#fff;pointer-events:none}.lightbox__image[data-v-632b4e38]{align-items:center;display:flex;flex:1;flex-direction:column;height:100vh;justify-content:center}.lightbox__image img[data-v-632b4e38]{background-color:#fff;max-height:85%;max-width:100%;min-width:400px}.lightbox__image img.svg[data-v-632b4e38]{width:90vw}.lightbox__image p[data-v-632b4e38]{font-family:var(--font-charter)}@media screen and (max-width:720px){.lightbox__arrow[data-v-632b4e38]{padding:0 1rem}.lightbox__image img[data-v-632b4e38]{min-width:300px}}@media screen and (max-width:500px){.lightbox__element[data-v-632b4e38]{position:relative}.lightbox__arrow[data-v-632b4e38]{height:100%;padding:0 2rem;position:absolute}.lightbox__arrow--right[data-v-632b4e38]{background:linear-gradient(90deg,transparent,rgba(0,0,0,.3));right:0}.lightbox__arrow--left[data-v-632b4e38]{background:linear-gradient(270deg,transparent,rgba(0,0,0,.3));left:0}}
.article-blockquote[data-v-7368c93e]{display:flex}.quotes[data-v-7368c93e]{color:var(--color-mexican-red);font-family:var(--font-georgia);font-size:5.6rem;font-weight:600}.content[data-v-7368c93e]{font-family:var(--font-charter);font-size:1.7rem;font-weight:400;line-height:3rem;padding:5px 0 5px 15px}
.article-subheading[data-v-7ac49cc3]{color:var(--color-black);display:block;font-family:var(--font-avenir);font-size:2.2rem;font-weight:600;line-height:2.7rem;margin-bottom:.8rem}
.article-list[data-v-0e21aa74]{color:var(--color-black);line-height:3rem;margin-left:20px}.article-list[data-v-0e21aa74],.article-list[data-v-0e21aa74] a{font-family:var(--font-charter);font-size:1.7rem}.article-list[data-v-0e21aa74] a{color:var(--color-mexican-red);font-weight:400;-webkit-text-decoration:underline;text-decoration:underline;transition:all .3s ease}.article-list[data-v-0e21aa74] a:active,.article-list[data-v-0e21aa74] a:focus{opacity:.4}.article-list[data-v-0e21aa74] a:hover{-webkit-text-decoration:underline;text-decoration:underline}.article-list[data-v-0e21aa74] ul{list-style-type:disc}.article-list[data-v-0e21aa74] ol,.article-list[data-v-0e21aa74] ul{list-style-position:outside;margin-left:20px}.article-list[data-v-0e21aa74] li{color:var(--color-black);font-family:var(--font-charter);font-weight:400;margin-bottom:.8rem}
#footer[data-v-44437d65]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;background-color:var(--color-black);display:grid;display:-ms-grid;grid-template-columns:1fr minmax(330px,1170px) 1fr;width:100%}.content[data-v-44437d65]{align-items:center;display:flex;flex-direction:row;grid-column:2;-ms-grid-column:2;justify-content:space-between;padding:30px}@media screen and (max-width:960px){.content[data-v-44437d65]{flex-direction:column-reverse;padding:20px 30px}}
#footer-about[data-v-65e247b0]{display:inline-flex}.logo[data-v-65e247b0]{fill:var(--color-white);height:22px;width:42px}.copyright[data-v-65e247b0]{color:var(--color-white);font-size:.8rem;line-height:1.2rem;margin-left:10px}
#footer-navigation[data-v-458174f5]{align-items:center;display:flex;flex-grow:1;justify-content:flex-end;margin-left:35px;overflow:hidden;white-space:nowrap}.link[data-v-458174f5]{align-items:center;border-bottom:2px solid transparent;color:var(--color-swiss-coffee);display:inline-flex;font-size:1.6rem;height:30px;justify-content:center;letter-spacing:.04rem;min-width:70px;opacity:.8;transition:all .4s ease}.link.active[data-v-458174f5],.link[data-v-458174f5]:hover{border-bottom-color:var(--color-white);color:var(--color-white);opacity:1}.link+.link[data-v-458174f5]{margin-left:20px}@media screen and (max-width:960px){#footer-navigation[data-v-458174f5]{flex-direction:column;margin:0 0 35px}.link+.link[data-v-458174f5]{margin:5px 0 0}}</style>
  <style type="text/css">#layout-error[data-v-16bfe682]{display:flex;flex-direction:column;flex-grow:1}#main[data-v-16bfe682]{-ms-grid-columns:1fr minmax(330px,860px) 1fr;background-color:var(--color-white);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:1fr minmax(330px,860px) 1fr}.content[data-v-16bfe682]{grid-column:2;-ms-grid-column:2;margin-top:80px}.heading[data-v-16bfe682]{color:var(--color-black);display:flex;flex-direction:column}.code[data-v-16bfe682]{color:var(--color-corduroy);font-size:5.2rem;font-weight:200;line-height:5rem}.details[data-v-16bfe682]{font-size:3.4rem}.message[data-v-16bfe682]{font-size:1.6rem;margin-top:15px}</style><style type="text/css">.a[data-v-88504162]{fill:#2f363d}.b[data-v-88504162]{fill:#494949}.c[data-v-88504162]{fill:#a31f34}</style><style type="text/css">@media screen and (min-width:800px){aside[data-v-08dd614c]{max-width:295px}}aside[data-v-08dd614c]{margin-right:64px}.events[data-v-08dd614c]{margin-top:32px}.header[data-v-08dd614c]{border-bottom:1px solid var(--color-black);font-size:16px;font-weight:400;font-weight:700;line-height:27px;margin-bottom:22px;padding-bottom:4px;text-transform:uppercase}.module-info-wrapper[data-v-08dd614c]{align-items:flex-start;display:flex;flex-direction:column;margin-bottom:35px;width:100%}.event[data-v-08dd614c],.module[data-v-08dd614c]{align-items:flex-start;display:flex;position:relative}.checkmark[data-v-08dd614c]{flex-shrink:0;margin-right:10px}.module-info[data-v-08dd614c]{border-bottom:1px solid var(--color-black);margin-top:5px;padding-bottom:5px;width:100%}.title[data-v-08dd614c]{color:var(--color-grey-suit-alternative)!important;font-family:var(--font-avenir);font-size:16px;font-style:normal;font-weight:600;line-height:22px;-webkit-text-decoration:underline;text-decoration:underline}.active .subtitle[data-v-08dd614c],.active .title[data-v-08dd614c]{color:var(--color-black)!important}.subtitle[data-v-08dd614c]{color:var(--color-grey-suit-alternative)!important;font-size:12px;font-style:normal;font-weight:500;letter-spacing:.4px;line-height:16px;margin-top:4px}.module-info-quicklinks[data-v-08dd614c]{display:flex;margin-top:11px}.module-info-quicklink-title[data-v-08dd614c]{color:var(--color-grey-suit-alternative);font-size:9px;font-style:normal;font-weight:700;letter-spacing:.4px;line-height:12px;margin-right:30px;text-transform:uppercase;width:75px}.module-info-quicklink[data-v-08dd614c]{color:var(--color-grey-suit-alternative)!important;font-size:14px;font-style:normal;font-weight:500;letter-spacing:.4px;line-height:19px}</style><style type="text/css">#header[data-v-ae0b1104]{background-color:var(--color-white);margin:32px 32px 16px}#header[data-v-ae0b1104],.content[data-v-ae0b1104]{align-items:center}.content[data-v-ae0b1104]{display:flex;margin:0 auto;position:relative}.menu[data-v-ae0b1104]{display:none}.menu hr[data-v-ae0b1104]{width:100%}h1[data-v-ae0b1104]{margin-right:auto}h1 svg[data-v-ae0b1104]{width:350px}.air-force-university[data-v-ae0b1104],.logout[data-v-ae0b1104]{font-size:1.8rem}.logout[data-v-ae0b1104]{align-items:center;color:var(--color-mexican-red);display:flex;font-weight:300;justify-content:center}.main-header-section[data-v-ae0b1104]{display:flex;margin-bottom:10px;margin-top:10px;width:100%}.main-header-section>.header-link[data-v-ae0b1104]{display:flex;margin-left:0}.main-header-section>.dropdown[data-v-ae0b1104]{display:block}.content .logout[data-v-ae0b1104]{margin-left:20px;padding:4px}.logout[data-v-ae0b1104]:focus,.logout[data-v-ae0b1104]:hover{-webkit-text-decoration:underline;text-decoration:underline}.icon[data-v-ae0b1104]{fill:var(--color-mexican-red);height:1.8rem;margin-left:10px;width:1.8rem}#header-menu[data-v-ae0b1104]{display:none}#header-menu .icon[data-v-ae0b1104]{width:2rem}.dropdown[data-v-ae0b1104]{color:var(--color-mexican-red);display:inline-block;display:block;font-size:1.8rem;font-weight:300;margin-left:20px;position:relative}.dropdown-content[data-v-ae0b1104]{background-color:#f9f9f9;box-shadow:0 8px 16px 0 rgba(0,0,0,.2);display:none;min-width:160px;padding:12px 16px;position:absolute;z-index:99999999}.dropdown:hover .dropdown-content[data-v-ae0b1104]{display:block}.header-link[data-v-ae0b1104]{align-items:center;color:var(--color-mexican-red);display:flex;font-size:1.8rem;font-weight:300;justify-content:center;margin-left:20px;padding:2px 0;-webkit-text-decoration:none!important;text-decoration:none!important}.dropdown .icon[data-v-ae0b1104]{height:11px;margin-left:2px;transform:rotate(90deg);width:10.5px}.header-link-active[data-v-ae0b1104],.header-link.header-link-active[data-v-ae0b1104]{border-bottom:2px solid var(--color-mexican-red)}@media screen and (max-width:800px){.content[data-v-ae0b1104]{-ms-grid-columns:1fr auto;grid-template-columns:1fr auto}.dropdown[data-v-ae0b1104],.header-link[data-v-ae0b1104]{display:none}.menu[data-v-ae0b1104]{align-items:flex-start;display:flex;flex-direction:column;gap:8px;margin-top:16px}.content .air-force-university[data-v-ae0b1104],.content .logout[data-v-ae0b1104]{display:none}#header-menu[data-v-ae0b1104]{display:block}}</style><style type="text/css">.impact-spotlight-navigation-wrapper[data-v-0c18d328]{justify-content:center}.impact-spotlight-navigation[data-v-0c18d328],.impact-spotlight-navigation-wrapper[data-v-0c18d328]{align-items:center;background-color:var(--color-blue-whale);display:flex;height:65px;width:100%}.impact-spotlight-navigation[data-v-0c18d328]{justify-content:space-between;max-width:650px}.impact-spotlight-next-link-wrapper[data-v-0c18d328],.impact-spotlight-previous-link-wrapper[data-v-0c18d328]{align-items:center;display:flex}.impact-spotlight-previous-link-wrapper>img[data-v-0c18d328]{margin-right:14px;transform:rotate(180deg)}.impact-spotlight-next-link-wrapper>img[data-v-0c18d328]{margin-left:14px}.impact-spotlight-next-link[data-v-0c18d328],.impact-spotlight-previous-link[data-v-0c18d328]{align-items:center;color:var(--color-white);display:flex;font-family:var(--font-avenir);font-size:14px;font-style:normal;font-weight:600;letter-spacing:.4px;line-height:19px;max-width:244px;text-align:right}.impact-spotlight-previous-link[data-v-0c18d328]{font-weight:400;text-align:left}</style><style type="text/css">#layout-default[data-v-65a9d880]{display:flex;flex-direction:column;min-height:100vh}#layout-default[data-v-65a9d880] .sr-only{clip:rect(0,0,0,0);border:0;height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}#layout-default[data-v-65a9d880] .sr-only:active,#layout-default[data-v-65a9d880] .sr-only:focus{background-color:#000;border:4px solid #ff0;border-radius:15px;color:#fff;font-size:1.2em;height:auto;left:auto;margin:10px 35%;overflow:auto;padding:5px;text-align:center;top:auto;width:30%;z-index:999}#layout-air-force[data-v-65a9d880]{display:flex;flex-direction:column;margin:0 auto;max-width:1251px;min-height:100vh}</style><style type="text/css">#layout-air-force a{color:var(--color-mexican-red)}#layout-air-force div.error{background:var(--color-mexican-red)}#layout-air-force div.invert-colors{color:var(--color-white)}#layout-air-force div.invert-colors a{color:var(--color-swiss-coffee)}#layout-air-force a:focus,#layout-air-force a:hover{-webkit-text-decoration:underline;text-decoration:underline}</style><style type="text/css">#header[data-v-7fcd3572]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;background-color:var(--color-black);display:grid;display:-ms-grid;grid-template-columns:1fr minmax(330px,1170px) 1fr;position:sticky;top:0;z-index:1}#header[data-v-7fcd3572],.content[data-v-7fcd3572]{align-items:center;height:56px}.content[data-v-7fcd3572]{display:flex;grid-column:2;-ms-grid-column:2;padding:0 30px;position:relative}.mit-logo[data-v-7fcd3572]{margin-left:auto}@media screen and (max-width:1125px){#header[data-v-7fcd3572],.content[data-v-7fcd3572]{height:48px}.content[data-v-7fcd3572]{justify-content:center;padding:0 20px}.mit-logo[data-v-7fcd3572]{display:none}}</style><style type="text/css">#sidebar-account[data-v-59ce8692]{align-items:center;color:var(--color-black);display:inline-flex;font-size:1.5rem;font-weight:300;height:40px;margin:10px 24px 0}.icon[data-v-59ce8692]{fill:var(--color-mine-shaft);margin-left:15px;width:14px}</style><style type="text/css">#sidebar-logo[data-v-7a52c02e]{align-items:center;background:var(--color-black);display:flex;height:90px;margin:0 0 2px;width:100%}.link[data-v-7a52c02e]{line-height:0;margin-left:25px}.icon[data-v-7a52c02e]{fill:var(--color-black);width:80%}.close-button-box[data-v-7a52c02e]{height:20px;left:202px;position:relative;top:-27px;width:20px}#close-menu[data-v-7a52c02e]{fill:var(--color-white);cursor:pointer}</style><style type="text/css">#sidebar-navigation[data-v-e9a9f0c8]{margin:20px 24px}.link[data-v-e9a9f0c8]{align-items:center;display:flex;font-size:1.5rem;letter-spacing:.04rem;overflow:hidden;transition:all .6s ease;white-space:nowrap}.link[data-v-e9a9f0c8],.link.active[data-v-e9a9f0c8],.link[data-v-e9a9f0c8]:hover{fill:var(--color-black);color:var(--color-black)}.link+.link[data-v-e9a9f0c8]{margin-top:12px}.icon[data-v-e9a9f0c8]{margin-right:15px}.topics[data-v-e9a9f0c8]{height:9px;width:13px}</style><style type="text/css">#sidebar-search[data-v-0d13e36e]{align-self:center;border:1px solid var(--color-black);height:34px;justify-content:center;margin:20px 24px 0;min-height:15px;overflow:hidden;padding:0 10px}#sidebar-search[data-v-0d13e36e],.label[data-v-0d13e36e]{align-items:center;display:flex}#sidebar-search input[data-v-0d13e36e]{background:none;border:none;color:var(--color-black);font-size:1.5rem;font-weight:100;margin-left:8px;width:calc(100% - 20px)}#sidebar-search input[data-v-0d13e36e]::-moz-placeholder{color:var(--color-black);font-size:1.5rem;font-weight:100}#sidebar-search input[data-v-0d13e36e]::placeholder{color:var(--color-black);font-size:1.5rem;font-weight:100}.icon[data-v-0d13e36e]{fill:var(--color-black);stroke:var(--color-black);height:14px;width:14px}</style><style type="text/css">.background-overlay[data-v-181e9383]{background-color:hsla(0,0%,47%,.5);height:100vh;left:0;position:fixed;top:0;width:100vw;z-index:999999}.modal-body[data-v-181e9383]{background-color:#fff;margin:auto;max-width:463px;padding:54px;position:relative;top:50%;transform:translateY(-50%);width:80%}.close-button[data-v-181e9383]{height:17px;text-align:right;transform:translate(25px,-25px)}.close-button-icon[data-v-181e9383]{cursor:pointer;height:17px;width:17px;z-index:999999}.heading>h3[data-v-181e9383]{font-family:avenir;font-size:2.4rem;font-weight:bolder;margin-bottom:30px}.icon-book[data-v-181e9383]{fill:var(--color-mexican-red);height:35px;margin-bottom:20px;width:70px}.body-text>p[data-v-181e9383]{font-size:1.8rem;margin-bottom:40px}.continue-button[data-v-181e9383]{background-color:var(--color-mexican-red);color:#fff;font-size:1.6rem;padding:10px;width:140px}@media screen and (max-width:960px){.heading>h3[data-v-181e9383]{font-size:1.6rem;font-weight:bolder;margin-bottom:20px}.body-text>p[data-v-181e9383]{font-size:1.4rem;margin-bottom:30px}.icon-book[data-v-181e9383]{width:50px}}</style><style type="text/css">.banner[data-v-db22b8b2]{-ms-grid-column-span:3;grid-column:1/4;-ms-grid-column:1;grid-row:2;-ms-grid-row:2;position:relative;text-align:center}.banner-message[data-v-db22b8b2]{color:#fff;font-size:1.6rem;font-weight:100;padding:10px}.close-banner-button[data-v-db22b8b2]{position:absolute;right:8px;top:8px}.close-banner-button[data-v-db22b8b2]:hover{cursor:pointer}.close-icon[data-v-db22b8b2]{fill:#fff;height:13px}</style><style type="text/css">.banner-message p a{color:#fff;font-size:1.6rem;font-weight:100;-webkit-text-decoration:underline;text-decoration:underline}</style><style type="text/css">.agile{position:relative}.agile--ssr .agile__slides--cloned{display:none}.agile--ssr .agile__slides>*{overflow:hidden;width:0}.agile--ssr .agile__slides>:first-child{width:100%}.agile--rtl .agile__actions,.agile--rtl .agile__dots,.agile--rtl .agile__slides,.agile--rtl .agile__track{flex-direction:row-reverse}.agile :active,.agile :focus,.agile:active,.agile:focus{outline:none}.agile__list{display:block;overflow:hidden;position:relative;width:100%}.agile__track{display:flex;flex-direction:row;flex-wrap:nowrap}.agile__actions{display:flex;justify-content:space-between}.agile--no-nav-buttons .agile__actions{justify-content:center}.agile__slides{align-items:center;display:flex;flex-direction:row;flex-grow:1;flex-shrink:1;flex-wrap:nowrap;justify-content:flex-start}.agile--disabled .agile__slides{display:block;width:100%}.agile__slide{display:block;flex-grow:1;flex-shrink:0}.agile__slide,.agile__slide *{-webkit-user-drag:none}.agile--fade .agile__slide{opacity:0;position:relative;z-index:0}.agile--fade .agile__slide--active{opacity:1;z-index:2}.agile--fade .agile__slide--expiring{opacity:1;transition-duration:0s;z-index:1}.agile__nav-button[disabled]{cursor:default}.agile__dots{align-items:center;display:flex;list-style:none;padding:0;white-space:nowrap}.agile__dot button{cursor:pointer;display:block;font-size:0;line-height:0}</style><style type="text/css">@keyframes plyr-progress{to{background-position:25px 0;background-position:var(--plyr-progress-loading-size,25px) 0}}@keyframes plyr-popup{0%{opacity:.5;transform:translateY(10px)}to{opacity:1;transform:translateY(0)}}@keyframes plyr-fade-in{0%{opacity:0}to{opacity:1}}.plyr{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;font-feature-settings:"tnum";align-items:center;direction:ltr;display:flex;flex-direction:column;font-family:inherit;font-family:var(--plyr-font-family,inherit);font-variant-numeric:tabular-nums;font-weight:400;font-weight:var(--plyr-font-weight-regular,400);height:100%;line-height:1.7;line-height:var(--plyr-line-height,1.7);max-width:100%;min-width:200px;position:relative;text-shadow:none;transition:box-shadow .3s ease;z-index:0}.plyr audio,.plyr iframe,.plyr video{display:block;height:100%;width:100%}.plyr button{font:inherit;line-height:inherit;width:auto}.plyr:focus{outline:0}.plyr--full-ui{box-sizing:border-box}.plyr--full-ui *,.plyr--full-ui :after,.plyr--full-ui :before{box-sizing:inherit}.plyr--full-ui a,.plyr--full-ui button,.plyr--full-ui input,.plyr--full-ui label{touch-action:manipulation}.plyr__badge{background:#4a5464;background:var(--plyr-badge-background,#4a5464);border-radius:2px;border-radius:var(--plyr-badge-border-radius,2px);color:#fff;color:var(--plyr-badge-text-color,#fff);font-size:9px;font-size:var(--plyr-font-size-badge,9px);line-height:1;padding:3px 4px}.plyr--full-ui ::-webkit-media-text-track-container{display:none}.plyr__captions{animation:plyr-fade-in .3s ease;bottom:0;display:none;font-size:13px;font-size:var(--plyr-font-size-small,13px);left:0;padding:10px;padding:var(--plyr-control-spacing,10px);position:absolute;text-align:center;transition:transform .4s ease-in-out;width:100%}.plyr__captions span:empty{display:none}@media (min-width:480px){.plyr__captions{font-size:15px;font-size:var(--plyr-font-size-base,15px);padding:20px;padding:calc(var(--plyr-control-spacing,10px)*2)}}@media (min-width:768px){.plyr__captions{font-size:18px;font-size:var(--plyr-font-size-large,18px)}}.plyr--captions-active .plyr__captions{display:block}.plyr:not(.plyr--hide-controls) .plyr__controls:not(:empty)~.plyr__captions{transform:translateY(-40px);transform:translateY(calc(var(--plyr-control-spacing,10px)*-4))}.plyr__caption{background:rgba(0,0,0,.8);background:var(--plyr-captions-background,rgba(0,0,0,.8));border-radius:2px;-webkit-box-decoration-break:clone;box-decoration-break:clone;color:#fff;color:var(--plyr-captions-text-color,#fff);line-height:185%;padding:.2em .5em;white-space:pre-wrap}.plyr__caption div{display:inline}.plyr__control{background:0 0;border:0;border-radius:3px;border-radius:var(--plyr-control-radius,3px);color:inherit;cursor:pointer;flex-shrink:0;overflow:visible;padding:7px;padding:calc(var(--plyr-control-spacing,10px)*.7);position:relative;transition:all .3s ease}.plyr__control svg{fill:currentColor;display:block;height:18px;height:var(--plyr-control-icon-size,18px);pointer-events:none;width:18px;width:var(--plyr-control-icon-size,18px)}.plyr__control:focus{outline:0}.plyr__control.plyr__tab-focus{outline-color:#00b3ff;outline-color:var(--plyr-tab-focus-color,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));outline-offset:2px;outline-style:dotted;outline-width:3px}a.plyr__control{-webkit-text-decoration:none;text-decoration:none}.plyr__control.plyr__control--pressed .icon--not-pressed,.plyr__control.plyr__control--pressed .label--not-pressed,.plyr__control:not(.plyr__control--pressed) .icon--pressed,.plyr__control:not(.plyr__control--pressed) .label--pressed,a.plyr__control:after,a.plyr__control:before{display:none}.plyr--full-ui ::-webkit-media-controls{display:none}.plyr__controls{align-items:center;display:flex;justify-content:flex-end;text-align:center}.plyr__controls .plyr__progress__container{flex:1;min-width:0}.plyr__controls .plyr__controls__item{margin-left:2.5px;margin-left:calc(var(--plyr-control-spacing,10px)/4)}.plyr__controls .plyr__controls__item:first-child{margin-left:0;margin-right:auto}.plyr__controls .plyr__controls__item.plyr__progress__container{padding-left:2.5px;padding-left:calc(var(--plyr-control-spacing,10px)/4)}.plyr__controls .plyr__controls__item.plyr__time{padding:0 5px;padding:0 calc(var(--plyr-control-spacing,10px)/2)}.plyr__controls .plyr__controls__item.plyr__progress__container:first-child,.plyr__controls .plyr__controls__item.plyr__time+.plyr__time,.plyr__controls .plyr__controls__item.plyr__time:first-child{padding-left:0}.plyr [data-plyr=airplay],.plyr [data-plyr=captions],.plyr [data-plyr=fullscreen],.plyr [data-plyr=pip],.plyr__controls:empty{display:none}.plyr--airplay-supported [data-plyr=airplay],.plyr--captions-enabled [data-plyr=captions],.plyr--fullscreen-enabled [data-plyr=fullscreen],.plyr--pip-supported [data-plyr=pip]{display:inline-block}.plyr__menu{display:flex;position:relative}.plyr__menu .plyr__control svg{transition:transform .3s ease}.plyr__menu .plyr__control[aria-expanded=true] svg{transform:rotate(90deg)}.plyr__menu .plyr__control[aria-expanded=true] .plyr__tooltip{display:none}.plyr__menu__container{animation:plyr-popup .2s ease;background:hsla(0,0%,100%,.9);background:var(--plyr-menu-background,hsla(0,0%,100%,.9));border-radius:4px;bottom:100%;box-shadow:0 1px 2px rgba(0,0,0,.15);box-shadow:var(--plyr-menu-shadow,0 1px 2px rgba(0,0,0,.15));color:#4a5464;color:var(--plyr-menu-color,#4a5464);font-size:15px;font-size:var(--plyr-font-size-base,15px);margin-bottom:10px;position:absolute;right:-3px;text-align:left;white-space:nowrap;z-index:3}.plyr__menu__container>div{overflow:hidden;transition:height .35s cubic-bezier(.4,0,.2,1),width .35s cubic-bezier(.4,0,.2,1)}.plyr__menu__container:after{border:4px solid transparent;border-top-color:hsla(0,0%,100%,.9);border:var(--plyr-menu-arrow-size,4px) solid transparent;border-top-color:var(--plyr-menu-background,hsla(0,0%,100%,.9));content:"";height:0;position:absolute;right:14px;right:calc(var(--plyr-control-icon-size,18px)/2 + var(--plyr-control-spacing,10px)*.7 - var(--plyr-menu-arrow-size,4px)/2);top:100%;width:0}.plyr__menu__container [role=menu]{padding:7px;padding:calc(var(--plyr-control-spacing,10px)*.7)}.plyr__menu__container [role=menuitem],.plyr__menu__container [role=menuitemradio]{margin-top:2px}.plyr__menu__container [role=menuitem]:first-child,.plyr__menu__container [role=menuitemradio]:first-child{margin-top:0}.plyr__menu__container .plyr__control{align-items:center;color:#4a5464;color:var(--plyr-menu-color,#4a5464);display:flex;font-size:13px;font-size:var(--plyr-font-size-menu,var(--plyr-font-size-small,13px));padding:4.66667px 10.5px;padding:calc(var(--plyr-control-spacing,10px)*.7/1.5) calc(var(--plyr-control-spacing,10px)*.7*1.5);-webkit-user-select:none;-moz-user-select:none;user-select:none;width:100%}.plyr__menu__container .plyr__control>span{align-items:inherit;display:flex;width:100%}.plyr__menu__container .plyr__control:after{border:4px solid transparent;border:var(--plyr-menu-item-arrow-size,4px) solid transparent;content:"";position:absolute;top:50%;transform:translateY(-50%)}.plyr__menu__container .plyr__control--forward{padding-right:28px;padding-right:calc(var(--plyr-control-spacing,10px)*.7*4)}.plyr__menu__container .plyr__control--forward:after{border-left-color:#728197;border-left-color:var(--plyr-menu-arrow-color,#728197);right:6.5px;right:calc(var(--plyr-control-spacing,10px)*.7*1.5 - var(--plyr-menu-item-arrow-size,4px))}.plyr__menu__container .plyr__control--forward.plyr__tab-focus:after,.plyr__menu__container .plyr__control--forward:hover:after{border-left-color:currentColor}.plyr__menu__container .plyr__control--back{font-weight:400;font-weight:var(--plyr-font-weight-regular,400);margin:7px;margin:calc(var(--plyr-control-spacing,10px)*.7);margin-bottom:3.5px;margin-bottom:calc(var(--plyr-control-spacing,10px)*.7/2);padding-left:28px;padding-left:calc(var(--plyr-control-spacing,10px)*.7*4);position:relative;width:calc(100% - 14px);width:calc(100% - var(--plyr-control-spacing,10px)*.7*2)}.plyr__menu__container .plyr__control--back:after{border-right-color:#728197;border-right-color:var(--plyr-menu-arrow-color,#728197);left:6.5px;left:calc(var(--plyr-control-spacing,10px)*.7*1.5 - var(--plyr-menu-item-arrow-size,4px))}.plyr__menu__container .plyr__control--back:before{background:#dcdfe5;background:var(--plyr-menu-back-border-color,#dcdfe5);box-shadow:0 1px 0 #fff;box-shadow:0 1px 0 var(--plyr-menu-back-border-shadow-color,#fff);content:"";height:1px;left:0;margin-top:3.5px;margin-top:calc(var(--plyr-control-spacing,10px)*.7/2);overflow:hidden;position:absolute;right:0;top:100%}.plyr__menu__container .plyr__control--back.plyr__tab-focus:after,.plyr__menu__container .plyr__control--back:hover:after{border-right-color:currentColor}.plyr__menu__container .plyr__control[role=menuitemradio]{padding-left:7px;padding-left:calc(var(--plyr-control-spacing,10px)*.7)}.plyr__menu__container .plyr__control[role=menuitemradio]:after,.plyr__menu__container .plyr__control[role=menuitemradio]:before{border-radius:100%}.plyr__menu__container .plyr__control[role=menuitemradio]:before{background:rgba(0,0,0,.1);content:"";display:block;flex-shrink:0;height:16px;margin-right:10px;margin-right:var(--plyr-control-spacing,10px);transition:all .3s ease;width:16px}.plyr__menu__container .plyr__control[role=menuitemradio]:after{background:#fff;border:0;height:6px;left:12px;opacity:0;top:50%;transform:translateY(-50%) scale(0);transition:transform .3s ease,opacity .3s ease;width:6px}.plyr__menu__container .plyr__control[role=menuitemradio][aria-checked=true]:before{background:#00b3ff;background:var(--plyr-control-toggle-checked-background,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)))}.plyr__menu__container .plyr__control[role=menuitemradio][aria-checked=true]:after{opacity:1;transform:translateY(-50%) scale(1)}.plyr__menu__container .plyr__control[role=menuitemradio].plyr__tab-focus:before,.plyr__menu__container .plyr__control[role=menuitemradio]:hover:before{background:rgba(35,40,47,.1)}.plyr__menu__container .plyr__menu__value{align-items:center;display:flex;margin-left:auto;margin-right:calc(-7px - -2);margin-right:calc(var(--plyr-control-spacing,10px)*.7*-1 - -2);overflow:hidden;padding-left:24.5px;padding-left:calc(var(--plyr-control-spacing,10px)*.7*3.5);pointer-events:none}.plyr--full-ui input[type=range]{-webkit-appearance:none;background:0 0;border:0;border-radius:26px;border-radius:calc(var(--plyr-range-thumb-height,13px)*2);color:#00b3ff;color:var(--plyr-range-fill-background,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));display:block;height:19px;height:calc(var(--plyr-range-thumb-active-shadow-width,3px)*2 + var(--plyr-range-thumb-height,13px));margin:0;padding:0;transition:box-shadow .3s ease;width:100%}.plyr--full-ui input[type=range]::-webkit-slider-runnable-track{background:0 0;background-image:linear-gradient(90deg,currentColor 0,transparent 0);background-image:linear-gradient(to right,currentColor var(--value,0),transparent var(--value,0));border:0;border-radius:2.5px;border-radius:calc(var(--plyr-range-track-height,5px)/2);height:5px;height:var(--plyr-range-track-height,5px);-webkit-transition:box-shadow .3s ease;transition:box-shadow .3s ease;-webkit-user-select:none;user-select:none}.plyr--full-ui input[type=range]::-webkit-slider-thumb{-webkit-appearance:none;background:#fff;background:var(--plyr-range-thumb-background,#fff);border:0;border-radius:100%;box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2));height:13px;height:var(--plyr-range-thumb-height,13px);margin-top:-4px;margin-top:calc((var(--plyr-range-thumb-height,13px) - var(--plyr-range-track-height,5px))/2*-1);position:relative;-webkit-transition:all .2s ease;transition:all .2s ease;width:13px;width:var(--plyr-range-thumb-height,13px)}.plyr--full-ui input[type=range]::-moz-range-track{background:0 0;border:0;border-radius:2.5px;border-radius:calc(var(--plyr-range-track-height,5px)/2);height:5px;height:var(--plyr-range-track-height,5px);-moz-transition:box-shadow .3s ease;transition:box-shadow .3s ease;-moz-user-select:none;user-select:none}.plyr--full-ui input[type=range]::-moz-range-thumb{background:#fff;background:var(--plyr-range-thumb-background,#fff);border:0;border-radius:100%;box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2));height:13px;height:var(--plyr-range-thumb-height,13px);position:relative;-moz-transition:all .2s ease;transition:all .2s ease;width:13px;width:var(--plyr-range-thumb-height,13px)}.plyr--full-ui input[type=range]::-moz-range-progress{background:currentColor;border-radius:2.5px;border-radius:calc(var(--plyr-range-track-height,5px)/2);height:5px;height:var(--plyr-range-track-height,5px)}.plyr--full-ui input[type=range]::-ms-track{color:transparent}.plyr--full-ui input[type=range]::-ms-fill-upper,.plyr--full-ui input[type=range]::-ms-track{background:0 0;border:0;border-radius:2.5px;border-radius:calc(var(--plyr-range-track-height,5px)/2);height:5px;height:var(--plyr-range-track-height,5px);-ms-transition:box-shadow .3s ease;transition:box-shadow .3s ease;user-select:none}.plyr--full-ui input[type=range]::-ms-fill-lower{background:0 0;background:currentColor;border:0;border-radius:2.5px;border-radius:calc(var(--plyr-range-track-height,5px)/2);height:5px;height:var(--plyr-range-track-height,5px);-ms-transition:box-shadow .3s ease;transition:box-shadow .3s ease;user-select:none}.plyr--full-ui input[type=range]::-ms-thumb{background:#fff;background:var(--plyr-range-thumb-background,#fff);border:0;border-radius:100%;box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2));height:13px;height:var(--plyr-range-thumb-height,13px);margin-top:0;position:relative;-ms-transition:all .2s ease;transition:all .2s ease;width:13px;width:var(--plyr-range-thumb-height,13px)}.plyr--full-ui input[type=range]::-ms-tooltip{display:none}.plyr--full-ui input[type=range]:focus{outline:0}.plyr--full-ui input[type=range]::-moz-focus-outer{border:0}.plyr--full-ui input[type=range].plyr__tab-focus::-webkit-slider-runnable-track{outline-color:#00b3ff;outline-color:var(--plyr-tab-focus-color,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));outline-offset:2px;outline-style:dotted;outline-width:3px}.plyr--full-ui input[type=range].plyr__tab-focus::-moz-range-track{outline-color:#00b3ff;outline-color:var(--plyr-tab-focus-color,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));outline-offset:2px;outline-style:dotted;outline-width:3px}.plyr--full-ui input[type=range].plyr__tab-focus::-ms-track{outline-color:#00b3ff;outline-color:var(--plyr-tab-focus-color,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));outline-offset:2px;outline-style:dotted;outline-width:3px}.plyr__poster{background-color:#000;background-position:50% 50%;background-repeat:no-repeat;background-size:contain;height:100%;left:0;opacity:0;position:absolute;top:0;transition:opacity .2s ease;width:100%;z-index:1}.plyr--stopped.plyr__poster-enabled .plyr__poster{opacity:1}.plyr__time{font-size:13px;font-size:var(--plyr-font-size-time,var(--plyr-font-size-small,13px))}.plyr__time+.plyr__time:before{content:"\2044";margin-right:10px;margin-right:var(--plyr-control-spacing,10px)}@media (max-width:calc(768px - 1)){.plyr__time+.plyr__time{display:none}}.plyr__tooltip{background:hsla(0,0%,100%,.9);background:var(--plyr-tooltip-background,hsla(0,0%,100%,.9));border-radius:3px;border-radius:var(--plyr-tooltip-radius,3px);bottom:100%;box-shadow:0 1px 2px rgba(0,0,0,.15);box-shadow:var(--plyr-tooltip-shadow,0 1px 2px rgba(0,0,0,.15));color:#4a5464;color:var(--plyr-tooltip-color,#4a5464);font-size:13px;font-size:var(--plyr-font-size-small,13px);font-weight:400;font-weight:var(--plyr-font-weight-regular,400);left:50%;line-height:1.3;margin-bottom:10px;margin-bottom:calc(var(--plyr-control-spacing,10px)/2*2);opacity:0;padding:5px 7.5px;padding:calc(var(--plyr-control-spacing,10px)/2) calc(var(--plyr-control-spacing,10px)/2*1.5);pointer-events:none;position:absolute;transform:translate(-50%,10px) scale(.8);transform-origin:50% 100%;transition:transform .2s ease .1s,opacity .2s ease .1s;white-space:nowrap;z-index:2}.plyr__tooltip:before{border-left:4px solid transparent;border-left:var(--plyr-tooltip-arrow-size,4px) solid transparent;border-right:4px solid transparent;border-right:var(--plyr-tooltip-arrow-size,4px) solid transparent;border-top:4px solid hsla(0,0%,100%,.9);border-top:var(--plyr-tooltip-arrow-size,4px) solid var(--plyr-tooltip-background,hsla(0,0%,100%,.9));bottom:-4px;bottom:calc(var(--plyr-tooltip-arrow-size,4px)*-1);content:"";height:0;left:50%;position:absolute;transform:translateX(-50%);width:0;z-index:2}.plyr .plyr__control.plyr__tab-focus .plyr__tooltip,.plyr .plyr__control:hover .plyr__tooltip,.plyr__tooltip--visible{opacity:1;transform:translate(-50%) scale(1)}.plyr .plyr__control:hover .plyr__tooltip{z-index:3}.plyr__controls>.plyr__control:first-child .plyr__tooltip,.plyr__controls>.plyr__control:first-child+.plyr__control .plyr__tooltip{left:0;transform:translateY(10px) scale(.8);transform-origin:0 100%}.plyr__controls>.plyr__control:first-child .plyr__tooltip:before,.plyr__controls>.plyr__control:first-child+.plyr__control .plyr__tooltip:before{left:16px;left:calc(var(--plyr-control-icon-size,18px)/2 + var(--plyr-control-spacing,10px)*.7)}.plyr__controls>.plyr__control:last-child .plyr__tooltip{left:auto;right:0;transform:translateY(10px) scale(.8);transform-origin:100% 100%}.plyr__controls>.plyr__control:last-child .plyr__tooltip:before{left:auto;right:16px;right:calc(var(--plyr-control-icon-size,18px)/2 + var(--plyr-control-spacing,10px)*.7);transform:translateX(50%)}.plyr__controls>.plyr__control:first-child .plyr__tooltip--visible,.plyr__controls>.plyr__control:first-child+.plyr__control .plyr__tooltip--visible,.plyr__controls>.plyr__control:first-child+.plyr__control.plyr__tab-focus .plyr__tooltip,.plyr__controls>.plyr__control:first-child+.plyr__control:hover .plyr__tooltip,.plyr__controls>.plyr__control:first-child.plyr__tab-focus .plyr__tooltip,.plyr__controls>.plyr__control:first-child:hover .plyr__tooltip,.plyr__controls>.plyr__control:last-child .plyr__tooltip--visible,.plyr__controls>.plyr__control:last-child.plyr__tab-focus .plyr__tooltip,.plyr__controls>.plyr__control:last-child:hover .plyr__tooltip{transform:translate(0) scale(1)}.plyr__progress{left:6.5px;left:calc(var(--plyr-range-thumb-height,13px)*.5);margin-right:13px;margin-right:var(--plyr-range-thumb-height,13px);position:relative}.plyr__progress input[type=range],.plyr__progress__buffer{margin-left:-6.5px;margin-left:calc(var(--plyr-range-thumb-height,13px)*-.5);margin-right:-6.5px;margin-right:calc(var(--plyr-range-thumb-height,13px)*-.5);width:calc(100% + 13px);width:calc(100% + var(--plyr-range-thumb-height,13px))}.plyr__progress input[type=range]{position:relative;z-index:2}.plyr__progress .plyr__tooltip{font-size:13px;font-size:var(--plyr-font-size-time,var(--plyr-font-size-small,13px));left:0}.plyr__progress__buffer{-webkit-appearance:none;background:0 0;border:0;border-radius:100px;height:5px;height:var(--plyr-range-track-height,5px);left:0;margin-top:-2.5px;margin-top:calc((var(--plyr-range-track-height,5px)/2)*-1);padding:0;position:absolute;top:50%}.plyr__progress__buffer::-webkit-progress-bar{background:0 0}.plyr__progress__buffer::-webkit-progress-value{background:currentColor;border-radius:100px;min-width:5px;min-width:var(--plyr-range-track-height,5px);-webkit-transition:width .2s ease;transition:width .2s ease}.plyr__progress__buffer::-moz-progress-bar{background:currentColor;border-radius:100px;min-width:5px;min-width:var(--plyr-range-track-height,5px);-moz-transition:width .2s ease;transition:width .2s ease}.plyr__progress__buffer::-ms-fill{border-radius:100px;-ms-transition:width .2s ease;transition:width .2s ease}.plyr--loading .plyr__progress__buffer{animation:plyr-progress 1s linear infinite;background-image:linear-gradient(-45deg,rgba(35,40,47,.6) 25%,transparent 0,transparent 50%,rgba(35,40,47,.6) 0,rgba(35,40,47,.6) 75%,transparent 0,transparent);background-image:linear-gradient(-45deg,var(--plyr-progress-loading-background,rgba(35,40,47,.6)) 25%,transparent 25%,transparent 50%,var(--plyr-progress-loading-background,rgba(35,40,47,.6)) 50%,var(--plyr-progress-loading-background,rgba(35,40,47,.6)) 75%,transparent 75%,transparent);background-repeat:repeat-x;background-size:25px 25px;background-size:var(--plyr-progress-loading-size,25px) var(--plyr-progress-loading-size,25px);color:transparent}.plyr--video.plyr--loading .plyr__progress__buffer{background-color:hsla(0,0%,100%,.25);background-color:var(--plyr-video-progress-buffered-background,hsla(0,0%,100%,.25))}.plyr--audio.plyr--loading .plyr__progress__buffer{background-color:rgba(193,200,209,.6);background-color:var(--plyr-audio-progress-buffered-background,rgba(193,200,209,.6))}.plyr__volume{align-items:center;display:flex;max-width:110px;min-width:80px;position:relative;width:20%}.plyr__volume input[type=range]{margin-left:5px;margin-left:calc(var(--plyr-control-spacing,10px)/2);margin-right:5px;margin-right:calc(var(--plyr-control-spacing,10px)/2);position:relative;z-index:2}.plyr--is-ios .plyr__volume{min-width:0;width:auto}.plyr--audio{display:block}.plyr--audio .plyr__controls{background:#fff;background:var(--plyr-audio-controls-background,#fff);border-radius:inherit;color:#4a5464;color:var(--plyr-audio-control-color,#4a5464);padding:10px;padding:var(--plyr-control-spacing,10px)}.plyr--audio .plyr__control.plyr__tab-focus,.plyr--audio .plyr__control:hover,.plyr--audio .plyr__control[aria-expanded=true]{background:#00b3ff;background:var(--plyr-audio-control-background-hover,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));color:#fff;color:var(--plyr-audio-control-color-hover,#fff)}.plyr--full-ui.plyr--audio input[type=range]::-webkit-slider-runnable-track{background-color:rgba(193,200,209,.6);background-color:var(--plyr-audio-range-track-background,var(--plyr-audio-progress-buffered-background,rgba(193,200,209,.6)))}.plyr--full-ui.plyr--audio input[type=range]::-moz-range-track{background-color:rgba(193,200,209,.6);background-color:var(--plyr-audio-range-track-background,var(--plyr-audio-progress-buffered-background,rgba(193,200,209,.6)))}.plyr--full-ui.plyr--audio input[type=range]::-ms-track{background-color:rgba(193,200,209,.6);background-color:var(--plyr-audio-range-track-background,var(--plyr-audio-progress-buffered-background,rgba(193,200,209,.6)))}.plyr--full-ui.plyr--audio input[type=range]:active::-webkit-slider-thumb{box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2),0 0 0 3px rgba(35,40,47,.1);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2)),0 0 0 var(--plyr-range-thumb-active-shadow-width,3px) var(--plyr-audio-range-thumb-active-shadow-color,rgba(35,40,47,.1))}.plyr--full-ui.plyr--audio input[type=range]:active::-moz-range-thumb{box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2),0 0 0 3px rgba(35,40,47,.1);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2)),0 0 0 var(--plyr-range-thumb-active-shadow-width,3px) var(--plyr-audio-range-thumb-active-shadow-color,rgba(35,40,47,.1))}.plyr--full-ui.plyr--audio input[type=range]:active::-ms-thumb{box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2),0 0 0 3px rgba(35,40,47,.1);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2)),0 0 0 var(--plyr-range-thumb-active-shadow-width,3px) var(--plyr-audio-range-thumb-active-shadow-color,rgba(35,40,47,.1))}.plyr--audio .plyr__progress__buffer{color:rgba(193,200,209,.6);color:var(--plyr-audio-progress-buffered-background,rgba(193,200,209,.6))}.plyr--video{background:#000;overflow:hidden}.plyr--video.plyr--menu-open{overflow:visible}.plyr__video-wrapper{background:#000;height:100%;margin:auto;overflow:hidden;position:relative;width:100%}.plyr__video-embed,.plyr__video-wrapper--fixed-ratio{height:0;padding-bottom:56.25%}.plyr__video-embed iframe,.plyr__video-wrapper--fixed-ratio video{border:0;left:0;position:absolute;top:0}.plyr--full-ui .plyr__video-embed>.plyr__video-embed__container{padding-bottom:240%;position:relative;transform:translateY(-38.28125%)}.plyr--video .plyr__controls{background:linear-gradient(transparent,rgba(0,0,0,.75));background:var(--plyr-video-controls-background,linear-gradient(transparent,rgba(0,0,0,.75)));border-bottom-left-radius:inherit;border-bottom-right-radius:inherit;bottom:0;color:#fff;color:var(--plyr-video-control-color,#fff);left:0;padding:5px;padding:calc(var(--plyr-control-spacing,10px)/2);padding-top:20px;padding-top:calc(var(--plyr-control-spacing,10px)*2);position:absolute;right:0;transition:opacity .4s ease-in-out,transform .4s ease-in-out;z-index:3}@media (min-width:480px){.plyr--video .plyr__controls{padding:10px;padding:var(--plyr-control-spacing,10px);padding-top:35px;padding-top:calc(var(--plyr-control-spacing,10px)*3.5)}}.plyr--video.plyr--hide-controls .plyr__controls{opacity:0;pointer-events:none;transform:translateY(100%)}.plyr--video .plyr__control.plyr__tab-focus,.plyr--video .plyr__control:hover,.plyr--video .plyr__control[aria-expanded=true]{background:#00b3ff;background:var(--plyr-video-control-background-hover,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));color:#fff;color:var(--plyr-video-control-color-hover,#fff)}.plyr__control--overlaid{background:#00b3ff;background:var(--plyr-video-control-background-hover,var(--plyr-color-main,var(--plyr-color-main,#00b3ff)));border:0;border-radius:100%;color:#fff;color:var(--plyr-video-control-color,#fff);display:none;left:50%;opacity:.9;padding:15px;padding:calc(var(--plyr-control-spacing,10px)*1.5);position:absolute;top:50%;transform:translate(-50%,-50%);transition:.3s;z-index:2}.plyr__control--overlaid svg{left:2px;position:relative}.plyr__control--overlaid:focus,.plyr__control--overlaid:hover{opacity:1}.plyr--playing .plyr__control--overlaid{opacity:0;visibility:hidden}.plyr--full-ui.plyr--video .plyr__control--overlaid{display:block}.plyr--full-ui.plyr--video input[type=range]::-webkit-slider-runnable-track{background-color:hsla(0,0%,100%,.25);background-color:var(--plyr-video-range-track-background,var(--plyr-video-progress-buffered-background,hsla(0,0%,100%,.25)))}.plyr--full-ui.plyr--video input[type=range]::-moz-range-track{background-color:hsla(0,0%,100%,.25);background-color:var(--plyr-video-range-track-background,var(--plyr-video-progress-buffered-background,hsla(0,0%,100%,.25)))}.plyr--full-ui.plyr--video input[type=range]::-ms-track{background-color:hsla(0,0%,100%,.25);background-color:var(--plyr-video-range-track-background,var(--plyr-video-progress-buffered-background,hsla(0,0%,100%,.25)))}.plyr--full-ui.plyr--video input[type=range]:active::-webkit-slider-thumb{box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2),0 0 0 3px hsla(0,0%,100%,.5);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2)),0 0 0 var(--plyr-range-thumb-active-shadow-width,3px) var(--plyr-audio-range-thumb-active-shadow-color,hsla(0,0%,100%,.5))}.plyr--full-ui.plyr--video input[type=range]:active::-moz-range-thumb{box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2),0 0 0 3px hsla(0,0%,100%,.5);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2)),0 0 0 var(--plyr-range-thumb-active-shadow-width,3px) var(--plyr-audio-range-thumb-active-shadow-color,hsla(0,0%,100%,.5))}.plyr--full-ui.plyr--video input[type=range]:active::-ms-thumb{box-shadow:0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2),0 0 0 3px hsla(0,0%,100%,.5);box-shadow:var(--plyr-range-thumb-shadow,0 1px 1px rgba(35,40,47,.15),0 0 0 1px rgba(35,40,47,.2)),0 0 0 var(--plyr-range-thumb-active-shadow-width,3px) var(--plyr-audio-range-thumb-active-shadow-color,hsla(0,0%,100%,.5))}.plyr--video .plyr__progress__buffer{color:hsla(0,0%,100%,.25);color:var(--plyr-video-progress-buffered-background,hsla(0,0%,100%,.25))}.plyr:fullscreen{border-radius:0!important}.plyr:fullscreen{background:#000;height:100%;margin:0;width:100%}.plyr:fullscreen video{height:100%}.plyr:fullscreen .plyr__video-wrapper{height:100%;position:static}.plyr:fullscreen.plyr--vimeo .plyr__video-wrapper{height:0;position:relative}.plyr:fullscreen .plyr__control .icon--exit-fullscreen{display:block}.plyr:fullscreen .plyr__control .icon--exit-fullscreen+svg{display:none}.plyr:fullscreen.plyr--hide-controls{cursor:none}@media (min-width:1024px){.plyr:-webkit-full-screen .plyr__captions{font-size:21px;font-size:var(--plyr-font-size-xlarge,21px)}.plyr:fullscreen .plyr__captions{font-size:21px;font-size:var(--plyr-font-size-xlarge,21px)}}.plyr:-webkit-full-screen{background:#000;border-radius:0!important;height:100%;margin:0;width:100%}.plyr:-webkit-full-screen video{height:100%}.plyr:-webkit-full-screen .plyr__video-wrapper{height:100%;position:static}.plyr:-webkit-full-screen.plyr--vimeo .plyr__video-wrapper{height:0;position:relative}.plyr:-webkit-full-screen .plyr__control .icon--exit-fullscreen{display:block}.plyr:-webkit-full-screen .plyr__control .icon--exit-fullscreen+svg{display:none}.plyr:-webkit-full-screen.plyr--hide-controls{cursor:none}@media (min-width:1024px){.plyr:-webkit-full-screen .plyr__captions{font-size:21px;font-size:var(--plyr-font-size-xlarge,21px)}}.plyr:-moz-full-screen{background:#000;border-radius:0!important;height:100%;margin:0;width:100%}.plyr:-moz-full-screen video{height:100%}.plyr:-moz-full-screen .plyr__video-wrapper{height:100%;position:static}.plyr:-moz-full-screen.plyr--vimeo .plyr__video-wrapper{height:0;position:relative}.plyr:-moz-full-screen .plyr__control .icon--exit-fullscreen{display:block}.plyr:-moz-full-screen .plyr__control .icon--exit-fullscreen+svg{display:none}.plyr:-moz-full-screen.plyr--hide-controls{cursor:none}@media (min-width:1024px){.plyr:-moz-full-screen .plyr__captions{font-size:21px;font-size:var(--plyr-font-size-xlarge,21px)}}.plyr:-ms-fullscreen{background:#000;border-radius:0!important;height:100%;margin:0;width:100%}.plyr:-ms-fullscreen video{height:100%}.plyr:-ms-fullscreen .plyr__video-wrapper{height:100%;position:static}.plyr:-ms-fullscreen.plyr--vimeo .plyr__video-wrapper{height:0;position:relative}.plyr:-ms-fullscreen .plyr__control .icon--exit-fullscreen{display:block}.plyr:-ms-fullscreen .plyr__control .icon--exit-fullscreen+svg{display:none}.plyr:-ms-fullscreen.plyr--hide-controls{cursor:none}@media (min-width:1024px){.plyr:-ms-fullscreen .plyr__captions{font-size:21px;font-size:var(--plyr-font-size-xlarge,21px)}}.plyr--fullscreen-fallback{background:#000;border-radius:0!important;bottom:0;display:block;height:100%;left:0;margin:0;position:fixed;right:0;top:0;width:100%;z-index:10000000}.plyr--fullscreen-fallback video{height:100%}.plyr--fullscreen-fallback .plyr__video-wrapper{height:100%;position:static}.plyr--fullscreen-fallback.plyr--vimeo .plyr__video-wrapper{height:0;position:relative}.plyr--fullscreen-fallback .plyr__control .icon--exit-fullscreen{display:block}.plyr--fullscreen-fallback .plyr__control .icon--exit-fullscreen+svg{display:none}.plyr--fullscreen-fallback.plyr--hide-controls{cursor:none}@media (min-width:1024px){.plyr--fullscreen-fallback .plyr__captions{font-size:21px;font-size:var(--plyr-font-size-xlarge,21px)}}.plyr__ads{border-radius:inherit;bottom:0;cursor:pointer;left:0;overflow:hidden;position:absolute;right:0;top:0;z-index:-1}.plyr__ads>div,.plyr__ads>div iframe{height:100%;position:absolute;width:100%}.plyr__ads:after{background:#23282f;border-radius:2px;bottom:10px;bottom:var(--plyr-control-spacing,10px);color:#fff;content:attr(data-badge-text);font-size:11px;padding:2px 6px;pointer-events:none;position:absolute;right:10px;right:var(--plyr-control-spacing,10px);z-index:3}.plyr__ads:after:empty{display:none}.plyr__cues{background:currentColor;display:block;height:5px;height:var(--plyr-range-track-height,5px);left:0;margin:-var(--plyr-range-track-height,5px)/2 0 0;opacity:.8;position:absolute;top:50%;width:3px;z-index:3}.plyr__preview-thumb{background-color:hsla(0,0%,100%,.9);background-color:var(--plyr-tooltip-background,hsla(0,0%,100%,.9));border-radius:3px;bottom:100%;box-shadow:0 1px 2px rgba(0,0,0,.15);box-shadow:var(--plyr-tooltip-shadow,0 1px 2px rgba(0,0,0,.15));margin-bottom:10px;margin-bottom:calc(var(--plyr-control-spacing,10px)/2*2);opacity:0;padding:3px;padding:var(--plyr-tooltip-radius,3px);pointer-events:none;position:absolute;transform:translateY(10px) scale(.8);transform-origin:50% 100%;transition:transform .2s ease .1s,opacity .2s ease .1s;z-index:2}.plyr__preview-thumb--is-shown{opacity:1;transform:translate(0) scale(1)}.plyr__preview-thumb:before{border-left:4px solid transparent;border-left:var(--plyr-tooltip-arrow-size,4px) solid transparent;border-right:4px solid transparent;border-right:var(--plyr-tooltip-arrow-size,4px) solid transparent;border-top:4px solid hsla(0,0%,100%,.9);border-top:var(--plyr-tooltip-arrow-size,4px) solid var(--plyr-tooltip-background,hsla(0,0%,100%,.9));bottom:-4px;bottom:calc(var(--plyr-tooltip-arrow-size,4px)*-1);content:"";height:0;left:50%;position:absolute;transform:translateX(-50%);width:0;z-index:2}.plyr__preview-thumb__image-container{background:#c1c8d1;border-radius:2px;border-radius:calc(var(--plyr-tooltip-radius,3px) - 1px);overflow:hidden;position:relative;z-index:0}.plyr__preview-thumb__image-container img{height:100%;left:0;max-height:none;max-width:none;position:absolute;top:0;width:100%}.plyr__preview-thumb__time-container{bottom:6px;left:0;position:absolute;right:0;white-space:nowrap;z-index:3}.plyr__preview-thumb__time-container span{background-color:rgba(0,0,0,.55);border-radius:2px;border-radius:calc(var(--plyr-tooltip-radius,3px) - 1px);color:#fff;font-size:13px;font-size:var(--plyr-font-size-time,var(--plyr-font-size-small,13px));padding:3px 6px}.plyr__preview-scrubbing{bottom:0;filter:blur(1px);height:100%;left:0;margin:auto;opacity:0;overflow:hidden;pointer-events:none;position:absolute;right:0;top:0;transition:opacity .3s ease;width:100%;z-index:1}.plyr__preview-scrubbing--is-shown{opacity:1}.plyr__preview-scrubbing img{height:100%;left:0;max-height:none;max-width:none;-o-object-fit:contain;object-fit:contain;position:absolute;top:0;width:100%}.plyr--no-transition{transition:none!important}.plyr__sr-only{clip:rect(1px,1px,1px,1px);border:0!important;height:1px!important;overflow:hidden;padding:0!important;position:absolute!important;width:1px!important}.plyr [hidden]{display:none!important}</style><style type="text/css">.article-author[data-v-2abb8503]{border-top:8px solid var(--color-mexican-red);grid-column:2;-ms-grid-column:2;-ms-grid-row:4;padding:20px 0 0}.heading[data-v-2abb8503]{font-size:2.2rem;font-weight:600;line-height:2.7rem;margin-bottom:.8rem}.content[data-v-2abb8503]{margin-top:10px}</style><style type="text/css">.pull-quotes[data-v-1b7c527d]{color:var(--color-mexican-red);font-family:var(--font-georgia);font-size:2rem;font-weight:600}span.pull-quotes.front[data-v-1b7c527d]{margin-right:-4px}span.pull-quotes.end[data-v-1b7c527d]{margin-left:-4px}.quote-text[data-v-1b7c527d]{font-size:15pt;font-weight:700;font-weight:600;line-height:20pt}.attribution[data-v-1b7c527d],.quote-text[data-v-1b7c527d]{color:#a31f34;font-family:var(--font-avenir)}.attribution[data-v-1b7c527d]{font-size:12pt;margin-top:7px}</style><style type="text/css">.article-book[data-v-29099be6]{grid-column-gap:30px;grid-row-gap:0;-ms-grid-columns:1fr 250px;-ms-grid-rows:auto 1fr;display:grid;display:-ms-grid;grid-template-columns:1fr var(--column-two-width);grid-template-rows:auto 1fr;padding-bottom:23px}.image[data-v-29099be6]{grid-column:2;-ms-grid-column:2;grid-row:1;-ms-grid-row:1;margin-top:30px;max-width:60%;-o-object-fit:contain;object-fit:contain}.details[data-v-29099be6]{-ms-grid-row-span:2;grid-column:1;-ms-grid-column:1;grid-row:1/span 2;-ms-grid-row:1}.title[data-v-29099be6]{display:block;font-size:1.8rem;font-weight:600;line-height:2.4rem}.list[data-v-29099be6]{margin-bottom:1.5em;margin-top:5px}.list-item[data-v-29099be6]{display:inline-flex;font-size:1.6rem;font-weight:300}.list-item[data-v-29099be6]:before{content:"\025CB\00a0\00a0"}.list-item+.list-item[data-v-29099be6]{margin-left:12px}.ratings[data-v-29099be6]{padding:30px 0}.rating[data-v-29099be6],.ratings[data-v-29099be6]{align-items:center;display:flex}.rating[data-v-29099be6]{font-size:1.5rem}.rating+.rating[data-v-29099be6]{margin-left:40px}.store[data-v-29099be6]{font-size:1.6rem;margin-right:5px}.alternatives[data-v-29099be6],.description[data-v-29099be6],.prerequisites[data-v-29099be6]{font-family:var(--font-charter);font-size:1.7rem;line-height:3rem}.alternatives[data-v-29099be6],.prerequisites[data-v-29099be6]{margin-top:15px}@media screen and (min-width:768px){.list-item[data-v-29099be6]:first-of-type:before{content:none}}@media screen and (max-width:768px){.article-book[data-v-29099be6]{grid-column-gap:0;grid-row-gap:30px;grid-template-columns:auto}.image[data-v-29099be6]{grid-column:1;grid-row:1;max-width:180px}.details[data-v-29099be6]{grid-column:1;grid-row:2}.list-item[data-v-29099be6]{display:block}.list-item+.list-item[data-v-29099be6]{margin-left:0}}</style><style type="text/css">.article-callout[data-v-3caf512f]{background-color:var(--color-seashell);padding:25px}.content[data-v-3caf512f]{color:var(--color-black);font-family:var(--font-charter);font-size:1.7rem;line-height:3rem}.source[data-v-3caf512f]{color:var(--color-corduroy);font-size:1.4rem;line-height:2.1rem;margin-top:30px}.sidebar[data-v-3caf512f]{float:right;width:220px}@media screen and (max-width:960px){.sidebar[data-v-3caf512f]{float:none;width:100%}}</style><style type="text/css">.article-conference[data-v-39a5f66e]{grid-column-gap:30px;grid-row-gap:0;-ms-grid-columns:1fr 250px;-ms-grid-rows:auto 1fr;display:grid;display:-ms-grid;grid-template-columns:1fr var(--column-two-width);grid-template-rows:auto 1fr;padding-bottom:23px}.image[data-v-39a5f66e]{grid-column:2;-ms-grid-column:2;grid-row:1;-ms-grid-row:1;margin-top:30px;max-width:80%;-o-object-fit:contain;object-fit:contain}.details[data-v-39a5f66e]{-ms-grid-row-span:2;grid-column:1;-ms-grid-column:1;grid-row:1/span 2;-ms-grid-row:1}:root .details[data-v-39a5f66e],_[data-v-39a5f66e]:-ms-fullscreen{margin-right:30px}.title[data-v-39a5f66e]{font-size:1.8rem;font-weight:600;line-height:2.4rem}.list[data-v-39a5f66e]{margin-top:5px}.list-item[data-v-39a5f66e]{display:inline-flex;font-size:1.6rem;font-weight:300}.list-item[data-v-39a5f66e]:nth-child(1n-2):before{content:"\025CB\00a0\00a0"}.list-item+.list-item[data-v-39a5f66e]{margin-left:12px}.audience[data-v-39a5f66e],.description[data-v-39a5f66e],.price[data-v-39a5f66e]{font-family:var(--font-charter);font-size:1.7rem;line-height:3rem;margin-top:20px}@media screen and (min-width:768px){.list-item[data-v-39a5f66e]:first-of-type:before{content:none}}@media screen and (max-width:768px){.article-conference[data-v-39a5f66e]{grid-column-gap:0;grid-row-gap:30px;grid-template-columns:auto}.image[data-v-39a5f66e]{grid-column:1;grid-row:1;max-width:180px}.details[data-v-39a5f66e]{grid-column:1;grid-row:2}.list-item[data-v-39a5f66e]{display:block}.list-item+.list-item[data-v-39a5f66e]{margin-left:0}}</style><style type="text/css">.article-course[data-v-8a567726]{grid-column-gap:30px;-ms-grid-columns:1fr 250px;-ms-grid-rows:auto 1fr;display:grid;display:-ms-grid;grid-template-columns:1fr var(--column-two-width);grid-template-rows:auto 1fr;padding-bottom:23px}.article-course+.article-course[data-v-8a567726]{margin-top:60px}.image[data-v-8a567726]{grid-column:2;-ms-grid-column:2;grid-row:1;-ms-grid-row:1;max-width:80%;-o-object-fit:contain;object-fit:contain}.details[data-v-8a567726]{-ms-grid-row-span:2;grid-column:1;-ms-grid-column:1;grid-row:1/span 2;-ms-grid-row:1}.title[data-v-8a567726]{font-size:1.8rem;font-weight:600;line-height:2.4rem}.list[data-v-8a567726]{margin-top:5px}.list-item[data-v-8a567726]{display:inline-flex;font-size:1.6rem;font-weight:300}.list-item[data-v-8a567726]:before{content:"\025CB\00a0\00a0"}.list-item+.list-item[data-v-8a567726]{margin-left:12px}.paragraph[data-v-8a567726]{color:var(--color-black);font-size:1.7rem;line-height:3rem;margin-top:20px}.paragraph[data-v-8a567726],.price[data-v-8a567726]{font-family:var(--font-charter)}.price[data-v-8a567726]{font-weight:600}@media screen and (min-width:768px){.list-item[data-v-8a567726]:first-of-type:before{content:none}}@media screen and (max-width:768px){.article-course[data-v-8a567726]{grid-column-gap:0;grid-row-gap:30px;grid-template-columns:auto}.image[data-v-8a567726]{grid-column:1;grid-row:1;max-width:180px}.details[data-v-8a567726]{grid-column:1;grid-row:2}.list-item[data-v-8a567726]{display:block}.list-item+.list-item[data-v-8a567726]{margin-left:0}}</style><style type="text/css">.article-divider[data-v-159904c7]{background:var(--color-swiss-coffee);border:0;height:1px}</style><style type="text/css">.article-term[data-v-69ae9bda]{position:relative}.name[data-v-69ae9bda]{font-size:2.2rem;font-weight:600;line-height:2.7rem;margin-bottom:.8rem;margin-top:24px}.definition[data-v-69ae9bda],.name[data-v-69ae9bda]{color:var(--color-black);font-family:var(--font-charter)}.definition[data-v-69ae9bda]{font-size:1.7rem;line-height:3rem}.target[data-v-69ae9bda]{position:absolute;top:-80px}</style><style type="text/css">.article-person[data-v-5f663b95]{grid-column-gap:30px;grid-row-gap:30px;-ms-grid-columns:1fr 240px;-ms-grid-rows:auto 1fr;display:grid;display:-ms-grid;grid-template-columns:1fr 240px;grid-template-rows:auto 1fr}.image[data-v-5f663b95]{grid-column:2;-ms-grid-column:2;grid-row:1;-ms-grid-row:1;max-width:100%;-o-object-fit:contain;object-fit:contain}.details[data-v-5f663b95]{-ms-grid-row-span:2;grid-column:1;-ms-grid-column:1;grid-row:1/span 2;-ms-grid-row:1}:root .details[data-v-5f663b95],_[data-v-5f663b95]:-ms-fullscreen{padding-right:30px}.position[data-v-5f663b95]{display:block;font-size:2.2rem;font-weight:600;line-height:2.7rem;margin-bottom:20px}.list-item[data-v-5f663b95]{display:inline-flex;font-family:var(--font-charter);font-size:1.7rem;font-weight:300;line-height:3rem}.list-item[data-v-5f663b95]:before{content:"\025CB\00a0\00a0"}.list-item+.list-item[data-v-5f663b95]{margin-left:12px}.nationality[data-v-5f663b95]{font-family:inherit;font-weight:700;margin-right:5px}.achievements[data-v-5f663b95],.bio[data-v-5f663b95],.focus[data-v-5f663b95],.learn-more[data-v-5f663b95]{font-family:var(--font-charter);font-size:1.7rem;line-height:3rem;margin-top:20px}.learn-more[data-v-5f663b95]{display:inline-block}@media screen and (min-width:768px){.list-item[data-v-5f663b95]:first-of-type:before{content:none}}@media screen and (max-width:768px){.article-person[data-v-5f663b95]{grid-column-gap:0;grid-row-gap:30px;grid-template-columns:auto}.image[data-v-5f663b95]{grid-column:1;grid-row:1;max-width:180px}.details[data-v-5f663b95]{grid-column:1;grid-row:2}.list-item[data-v-5f663b95]{display:block}.list-item+.list-item[data-v-5f663b95]{margin-left:0}}</style><style type="text/css">.c1-video-fallback-container[data-v-2ab9b70e]{align-items:center;background-color:#151515;color:#fff;display:flex;font-size:large;height:381px;justify-content:center;width:680px}</style><style type="text/css">.c1-video-container[data-v-cc92505a]{height:381px;width:680px}</style><style type="text/css">.c1-video-container[data-v-5202c2ab]{height:381px;width:680px}</style><style type="text/css">.article-video.full[data-v-0c0e1483]{width:100%}.aspect-ratio[data-v-0c0e1483]{overflow:hidden;position:relative}.iframe[data-v-0c0e1483]{border:0;height:100%;left:0;position:absolute;top:0;width:100%}.caption[data-v-0c0e1483]{color:var(--color-corduroy);line-height:2.1rem;margin-top:5px;width:100%}.caption[data-v-0c0e1483],.caption[data-v-0c0e1483] a{font-family:var(--font-charter);font-size:1.4rem}.caption[data-v-0c0e1483] a{color:var(--color-mexican-red);font-weight:500;-webkit-text-decoration:underline;text-decoration:underline;transition:all .3s ease}.caption[data-v-0c0e1483] a:active,.caption[data-v-0c0e1483] a:focus{opacity:.4}.caption[data-v-0c0e1483] a:hover{-webkit-text-decoration:underline;text-decoration:underline}.caption[data-v-0c0e1483] em{font-family:var(--font-charter)}</style><style type="text/css">.article-web+.article-web[data-v-34dfd01d]{margin-top:60px}.title[data-v-34dfd01d]{font-size:1.8rem;font-weight:600;line-height:2.4rem}.metadata[data-v-34dfd01d]{margin-top:5px}.description[data-v-34dfd01d]{color:var(--color-black);font-family:var(--font-charter);font-size:1.7rem;line-height:3rem;margin-top:20px}.article-web[data-v-34dfd01d] .metadata ul li{display:inline-flex;font-family:var(--font-avenir);font-size:1.6rem;font-weight:300}.article-web[data-v-34dfd01d] .metadata ul li:before{content:"\025CB\00a0\00a0"}.article-web[data-v-34dfd01d] .metadata ul li+li{margin-left:12px}@media screen and (min-width:768px){.article-web[data-v-34dfd01d] .metadata ul li:first-of-type:before{content:none}}@media screen and (max-width:768px){.article-web[data-v-34dfd01d] .metadata ul li{display:block}.article-web[data-v-34dfd01d] .metadata ul li+li{margin-left:0}}</style><style type="text/css">.question-container .answer-container a{color:var(--color-mexican-red)}.question-container .parag p{font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem;line-height:3rem;margin-bottom:10px}</style><style type="text/css">.question-container[data-v-88735b06]{padding-left:10px}.radio-label[data-v-88735b06]{cursor:pointer;display:block;font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem;line-height:3rem;margin-left:15px;padding-left:25px;position:relative;-webkit-user-select:none;-moz-user-select:none;user-select:none}.radio-label[data-v-88735b06]:not(:last-child){margin-bottom:12px}.radio-label[disabled][data-v-88735b06]{cursor:not-allowed}.radio-label input[data-v-88735b06]{cursor:pointer;height:0;opacity:0;position:absolute;width:0}.radio-circle[data-v-88735b06]{border:2px solid #d2cecc;border:1px solid var(--color-corduroy);border-radius:100px;height:15px;left:0;position:absolute;top:8px;width:15px}.radio-circle[data-v-88735b06]:after{content:"";display:none;position:absolute}.radio-label input:checked~.radio-circle[data-v-88735b06]:after{display:block}.radio-label .radio-circle[data-v-88735b06]:after{background-color:var(--color-corduroy);border-width:1px;border-bottom:1px solid var(--color-corduroy);border-left:0 solid var(--color-corduroy);border-radius:100px;border-right:1px solid var(--color-corduroy);border-top:0 solid var(--color-corduroy);height:15px;left:-1px;top:-1px;width:15px}.radio-label .radio-circle.green[data-v-88735b06],.radio-label .radio-circle.green[data-v-88735b06]:after{background-color:#119d11;border:1px solid #119d11}.radio-label .radio-circle.red[data-v-88735b06],.radio-label .radio-circle.red[data-v-88735b06]:after{background-color:var(--color-mexican-red);border:1px solid var(--color-mexican-red)}.question-icon[data-v-88735b06]{left:25px;position:absolute;top:23px}.answer-container[data-v-88735b06]{margin-left:15px;padding:20px 40px 20px 65px;position:relative}.answer-container>div[data-v-88735b06]{align-items:center;display:flex}.correct-answer[data-v-88735b06]{background-color:#dce6dd}.incorrect-answer[data-v-88735b06]{background-color:#eee3e4}.parag[data-v-88735b06]{font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem;line-height:3rem}.parag a[data-v-88735b06]{color:red}.parag.small[data-v-88735b06]{font-size:1.5rem;font-weight:600}.knowledge-check-answers[data-v-88735b06]{align-self:flex-end;display:flex;flex-direction:column;max-width:248px}.knowledge-check-answer-wrapper[data-v-88735b06]{border:1px solid var(--color-black);display:flex;font-family:var(--font-avenir);font-size:13px;font-style:normal;font-weight:400;line-height:18px;margin-bottom:16px;padding:10px;position:relative}.knowledge-check-answer-wrapper-current[data-v-88735b06]{border:2px solid var(--color-black);font-weight:600}.knowledge-check-answer-wrapper-error[data-v-88735b06]{border:2px solid var(--color-mexican-red);color:var(--color-mexican-red)}.knowledge-check-answer-wrapper-success[data-v-88735b06]{background-color:var(--color-mexican-red);border:2px solid var(--color-mexican-red);color:var(--color-white)}.knowledge-check-answer-wrapper input[data-v-88735b06]{cursor:pointer;height:0;opacity:0;position:absolute;width:0}.knowledge-check-answer-wrapper img[data-v-88735b06]{padding-left:15px}.knowledge-sections-tiles[data-v-88735b06]{display:flex}.knowledge-sections-tile[data-v-88735b06]{background-color:var(--color-tiles);height:10px;margin-right:3px;width:25px}.knowledge-sections-tile[data-v-88735b06]:first-child{border-radius:90px 0 0 90px}.knowledge-sections-tile[data-v-88735b06]:last-child{border-radius:0 90px 90px 0}.knowledge-sections-tile-active[data-v-88735b06]{background-color:var(--color-black)}@media screen and (max-width:960px){.knowledge-check[data-v-88735b06]{max-width:none}.knowledge-check-question-wrapper[data-v-88735b06]{flex-direction:column;justify-items:flex-start}.knowledge-check-question[data-v-88735b06]{margin-bottom:20px}.knowledge-check-answers[data-v-88735b06]{align-self:flex-start;margin-bottom:20px}}.knowledge-section-actions[data-v-88735b06]{display:flex;flex-wrap:wrap;justify-content:space-between}.knowledge-sections-indicators[data-v-88735b06]{align-items:center;display:flex}.knowledge-sections-count[data-v-88735b06]{margin-right:8px}.break-column[data-v-88735b06]{flex-basis:100%;width:0}</style><style type="text/css">.horizon-button{align-items:center;display:inline-flex;height:34px;justify-content:center;padding:0 16px}.horizon-button>.label{color:var(--color-white);font-size:1.3rem}.horizon-button>.icon{fill:var(--color-white);margin:1px 0 1px 8px;width:8px}.horizon-button.black{background:var(--color-black)}.horizon-button.mexican-red{background:var(--color-mexican-red)}.horizon-button.inverted{background:transparent;border:2px solid var(--color-mexican-red);font-weight:700}.horizon-button>img{display:flex;margin-right:8px}.horizon-button.inverted>.label{color:var(--color-mexican-red)}.hintIsActive{background:rgba(163,31,52,.15)}.horizon-button.gray{background:var(--color-swiss-coffee)}.horizon-button.gray,.horizon-button.gray>.label{color:var(--color-black)}.horizon-button.gray>.icon{fill:var(--color-black)}</style><style type="text/css">.questions-container button[data-v-6fd68fb8]:disabled{background-color:var(--color-corduroy);cursor:not-allowed}.questions-container .clear-answers-btn[data-v-6fd68fb8]{background-color:transparent;color:var(--color-mexican-red);font-size:1.6rem;margin-left:20px}.question-content[data-v-6fd68fb8]:not(:last-child){margin-bottom:30px}.parag[data-v-6fd68fb8]{display:inline-block;font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem;line-height:3rem;margin-bottom:15px;white-space:pre-wrap}.parag[data-v-6fd68fb8] span{border-bottom:1px solid;display:inline-block;margin:0 3px;width:60px}.quiz-score[data-v-6fd68fb8]{align-items:center;background-color:var(--color-seashell);display:flex;justify-content:center;margin-bottom:30px;padding:10px}.quiz-score .parag[data-v-6fd68fb8]{margin-bottom:0}.knowledge-check-question-wrapper-outer-wrapper[data-v-6fd68fb8]{display:flex;flex-direction:column}.knowledge-check-hint[data-v-6fd68fb8]{font-family:var(--font-avenir);font-size:13px;font-style:normal;font-weight:400;line-height:21px;margin-bottom:16px}.knowledge-section-actions[data-v-6fd68fb8]{display:flex;justify-content:space-between}.knowledge-sections-indicators[data-v-6fd68fb8]{align-items:center;display:flex}.knowledge-sections-count[data-v-6fd68fb8]{margin-right:8px}.knowledge-check-success-message[data-v-6fd68fb8]{align-items:center;color:var(--color-mexican-red);display:flex;font-family:var(--font-avenir);font-size:16px;font-style:normal;font-weight:700;height:200px;justify-content:center;line-height:21px;margin-bottom:16px}</style><style type="text/css">.questions-container button[data-v-0aeb7401]:disabled{background-color:var(--color-corduroy);cursor:not-allowed}.questions-container .clear-answers-btn[data-v-0aeb7401]{background-color:transparent;color:var(--color-mexican-red);font-size:1.6rem;margin-left:20px}.question-content[data-v-0aeb7401]:not(:last-child){margin-bottom:30px}.parag[data-v-0aeb7401]{display:inline-block;font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem;line-height:3rem;margin-bottom:15px;white-space:pre-wrap}.parag[data-v-0aeb7401] span{border-bottom:1px solid;display:inline-block;margin:0 3px;width:60px}.quiz-score[data-v-0aeb7401]{align-items:center;background-color:var(--color-seashell);display:flex;justify-content:center;margin-bottom:30px;padding:10px}.quiz-score .parag[data-v-0aeb7401]{margin-bottom:0}</style><style type="text/css">.article-lead[data-v-39ae4110]{align-self:center;color:var(--color-black);font-size:1.6rem;font-weight:400}</style><style type="text/css">.article-logo[data-v-693c91f8]{display:block;grid-column:1;-ms-grid-column:1;-ms-grid-row:2;padding-right:20px}.image[data-v-693c91f8]{width:100%}</style><style type="text/css">.article-references[data-v-db53b738]{border-top:8px solid var(--color-mexican-red);grid-column:2;-ms-grid-column:2;-ms-grid-row:5;padding:20px 0 0}.heading[data-v-db53b738]{font-size:2.2rem;font-weight:600;line-height:2.7rem;margin-bottom:.8rem}.category[data-v-db53b738]{margin-top:14px}.title[data-v-db53b738]{font-size:1.7rem;font-weight:600}.reference[data-v-db53b738]{font-family:var(--font-avenir);font-size:1.7rem;font-weight:400;line-height:3rem;margin-bottom:.8rem}</style><style type="text/css">.pdf-link[data-v-1e2748c7]{box-sizing:border-box;display:block;margin-bottom:15px;padding:0}</style><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/0afde4b.js.download"></script><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/6845b95.js.download"></script><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/d78661f.js.download"></script><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/194b6df.js.download"></script><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/ed7c89e.js.download"></script><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/5e4c887.js.download"></script><script charset="utf-8" src="./Common Misconceptions _ MIT Horizon_files/5110c23.js.download"></script><style type="text/css">.content[data-v-c1fe815a]{margin-top:60px}.message[data-v-c1fe815a]{font-size:2rem;margin-top:15px}</style><style type="text/css">.podcast .description a{color:var(--color-mexican-red)}</style><style type="text/css">.podcast[data-v-2dfe6856]{background-color:#fff;display:flex;margin-top:20px;padding:25px}.image[data-v-2dfe6856]{flex-shrink:0;height:130px;max-height:130px;max-width:130px;-o-object-fit:cover;object-fit:cover;width:130px}.content[data-v-2dfe6856]{flex-basis:100%;flex-grow:1;margin-left:40px}.title[data-v-2dfe6856]{display:block;font-size:2rem;font-weight:400}.datetime[data-v-2dfe6856]{color:var(--color-corduroy);display:inline-block;font-size:1.4rem;font-weight:400;height:2rem;line-height:2rem;margin-top:5px}.icon-clock[data-v-2dfe6856]{fill:var(--color-corduroy);margin-bottom:-2px}.length[data-v-2dfe6856]{margin-left:7px}.description[data-v-2dfe6856]{color:var(--color-black);font-family:var(--font-charter);font-size:1.5rem;line-height:2.4rem;margin-top:25px}.description a[data-v-2dfe6856]{color:var(--color-mexican-red);font-size:5rem}.no-date[data-v-2dfe6856]{margin-left:7px}@media screen and (max-width:768px){.podcast[data-v-2dfe6856]{flex-direction:column}.content[data-v-2dfe6856]{margin:25px 0 0}}</style><style type="text/css">#main[data-v-04154445]{-ms-grid-columns:1fr minmax(330px,860px) 1fr;-ms-grid-rows:100px 1fr;background-color:var(--color-seashell);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:minmax(0,1fr) minmax(330px,860px) minmax(0,1fr);grid-template-rows:100px 1fr}:root #main[data-v-04154445],_[data-v-04154445]:-ms-fullscreen{background-color:#f1f1f1!important}.header[data-v-04154445]{-ms-grid-column-span:3;align-items:center;background-color:var(--color-black);display:flex;grid-column:1/span 3;-ms-grid-column:1;grid-row:1;-ms-grid-row:1;height:100px;justify-content:center;width:100vw}:root .header[data-v-04154445],_[data-v-04154445]:-ms-fullscreen{background-color:#000!important}.title[data-v-04154445]{color:var(--color-white);font-size:3.7rem;font-weight:500;line-height:4.3rem}:root .title[data-v-04154445],_[data-v-04154445]:-ms-fullscreen{color:#fff!important}.podcasts[data-v-04154445]{grid-column:2;-ms-grid-column:2;margin:30px 0;padding:0 30px}.podcasts.upcoming[data-v-04154445]{grid-row:2;-ms-grid-row:2}.podcasts.past[data-v-04154445]{grid-row:3;-ms-grid-row:3;margin-bottom:60px}.section-title[data-v-04154445]{border-bottom:6px solid #a31f34;color:var(--color-black);display:inline-block;font-size:3rem;font-weight:600;line-height:4.5rem;padding-right:20px}@media screen and (max-width:670px){.title[data-v-04154445]{font-size:2.8rem}}</style><style type="text/css">.parag[data-v-044415c6]{color:var(--color-mono);font-size:1.4rem;font-weight:400;margin-bottom:1rem}.book-cover[data-v-044415c6]{height:210px;margin-bottom:1rem;width:auto}.ebook-item[data-v-044415c6]{margin-bottom:5rem;margin-left:8%;width:140px}.horizon-nuxt-link[data-v-044415c6]{margin-bottom:.5rem}.book-title[data-v-044415c6]{font-size:1.6rem;font-weight:600}@media screen and (max-width:1200px){.ebook-item[data-v-044415c6]{margin-left:6%}}@media screen and (max-width:960px){.ebook-item[data-v-044415c6]{margin-left:0}}@media screen and (max-width:500px){.ebook-item[data-v-044415c6]{width:90%}.book-cover[data-v-044415c6]{height:240px}}@media screen and (max-width:400px){.ebook-list[data-v-044415c6]{grid-column-gap:0}.book-cover[data-v-044415c6]{height:210px}}</style><style type="text/css">#main[data-v-4c44468d]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;-ms-grid-rows:120px auto;background-color:var(--color-seashell);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:1fr minmax(330px,1170px) 1fr;grid-template-rows:120px auto}:root #main[data-v-4c44468d],_[data-v-4c44468d]:-ms-fullscreen{background-color:#fff!important}.header[data-v-4c44468d]{-ms-grid-column-span:3;align-items:center;background-color:#000;background-color:var(--color-black);display:flex;flex-direction:column;grid-column:1/span 3;-ms-grid-column:1;grid-row:1;-ms-grid-row:1;height:100px;justify-content:center;padding:20px;width:100%}:root .header[data-v-4c44468d],_[data-v-4c44468d]:-ms-fullscreen{background-color:#000!important}.title[data-v-4c44468d]{font-size:3.7rem;font-weight:500;line-height:4.3rem}.subtitle[data-v-4c44468d],.title[data-v-4c44468d]{color:var(--color-white)}.subtitle[data-v-4c44468d]{font-size:2rem;font-weight:300}:root .subtitle[data-v-4c44468d],:root .title[data-v-4c44468d],_[data-v-4c44468d]:-ms-fullscreen{color:#fff!important}.ebooks-content[data-v-4c44468d]{grid-column-gap:20px;grid-row-gap:30px;-ms-grid-columns:250px 1fr 250px;display:grid;display:-ms-grid;grid-column:2;-ms-grid-column:2;grid-row:2;-ms-grid-row:2;grid-template-columns:220px 1fr;padding:30px;position:relative}.ebook-topics[data-v-4c44468d]{grid-column:1;-ms-grid-column:1;grid-row:3;-ms-grid-row:3;position:relative;transition:all .3s;width:250px}.ebook-topics.hide[data-v-4c44468d]{opacity:0;z-index:-1}.ebook-list[data-v-4c44468d]{-ms-grid-column-span:2;display:flex;flex:1;flex-wrap:wrap;grid-column:2/span 2;-ms-grid-column:2;grid-row:3;-ms-grid-row:3}.ebook-list h2[data-v-4c44468d]{font-size:2.5rem}.ebook-no-results[data-v-4c44468d]{margin-top:40px;text-align:center;width:100%}.topics-button-container[data-v-4c44468d]{display:none;margin-bottom:2rem}.topics-button-container .chevron-icon[data-v-4c44468d]{fill:var(--color-mexican-red);margin:1px 0 1px 8px;transform:rotate(270deg);transition:all .3s;width:10px}.topics-button-container .chevron-icon.rotate[data-v-4c44468d]{transform:rotate(90deg)}.topics-button-container button[data-v-4c44468d]{color:var(--color-mexican-red);display:flex;font-size:1.4rem;height:20px}.checkbox-label[data-v-4c44468d]{color:var(--color-mexican-red);cursor:pointer;display:block;font-size:1.6rem;padding-left:25px;position:relative;-webkit-user-select:none;-moz-user-select:none;user-select:none}.checkbox-label[data-v-4c44468d]:not(:last-child){margin-bottom:12px}.checkbox-label.disabled[data-v-4c44468d]{color:var(--color-mono);cursor:default}.checkbox-label.disabled .checkmark[data-v-4c44468d]:after{border-color:grey}.checkbox-label input[data-v-4c44468d]{cursor:pointer;height:0;opacity:0;position:absolute;width:0}.checkmark[data-v-4c44468d]{border:2px solid var(--color-swiss-coffee);height:15px;left:0;position:absolute;top:2px;width:15px}.all-topic .checkmark[data-v-4c44468d]{border:2px solid var(--color-mono)}.checkmark[data-v-4c44468d]:after{content:"";display:none;position:absolute}.checkbox-label input:checked~.checkmark[data-v-4c44468d]:after{display:block}.checkbox-label .checkmark[data-v-4c44468d]:after{border:solid var(--color-mexican-red);border-width:0 1px 1px 0;height:9px;left:3px;top:0;transform:rotate(40deg);width:5px}.all-topic[data-v-4c44468d]{color:var(--color-mono)}.checkbox-label[data-v-4c44468d]:focus-within{font-weight:bolder}@media screen and (max-width:960px){.ebooks-content[data-v-4c44468d]{display:block}.ebook-topics[data-v-4c44468d]{background-color:var(--color-white);box-shadow:0 0 3px 1px #d2cecc;margin:0 auto 30px;padding:1rem;width:93%;z-index:10}.ebook-list[data-v-4c44468d]{grid-gap:auto;grid-column-gap:3%;align-self:baseline;display:grid;grid-template-columns:1fr 1fr 1fr 1fr;justify-items:center}.topics-button-container[data-v-4c44468d]{align-items:center;display:flex;justify-content:center}}@media screen and (max-width:768px){.ebook-list[data-v-4c44468d]{grid-template-columns:1fr 1fr 1fr}.title[data-v-4c44468d]{font-size:2.8rem}.subtitle[data-v-4c44468d]{font-size:1.8rem;font-weight:300}}@media screen and (max-width:650px){.ebooks-content[data-v-4c44468d]{padding:20px}}@media screen and (max-width:500px){.ebook-list[data-v-4c44468d]{grid-template-columns:1fr 1fr}}</style><style type="text/css">.collection-title[data-v-8072bcbe]{color:var(--color-mexican-red);color:#a31f34;display:block;font-size:2rem;font-weight:500;margin-bottom:5px}.collection-info-container[data-v-8072bcbe]{height:100%;overflow:hidden}.video-collection-icon[data-v-8072bcbe]{height:90px;margin:0 30px 0 0;width:75px}.parag[data-v-8072bcbe]{font-family:var(--font-charter);font-family:Lora,serif;font-size:1.5rem;line-height:2.4rem}@media screen and (max-width:768px){.colletion-info-container[data-v-8072bcbe]{flex-direction:column}.video-collection-icon[data-v-8072bcbe]{margin-bottom:20px}}</style><style type="text/css">.video-collection-section[data-v-9bb44ad4]{background-color:var(--color-white);background-color:#fff;height:347px;margin-bottom:30px;margin-right:30px;padding:25px;text-align:left;width:347px}.video-collection-section[data-v-9bb44ad4]:hover{cursor:pointer}.list-item[data-v-9bb44ad4]{background-color:var(--color-white);padding-bottom:20px;position:relative}.list-item-title[data-v-9bb44ad4]{color:var(--color-mexican-red);display:inline-block;font-size:2rem;font-weight:400;margin-bottom:5px}.parag[data-v-9bb44ad4]{font-family:var(--font-charter);font-family:Lora,serif;font-size:1.7rem;line-height:3rem}.video-list[data-v-9bb44ad4]{margin-top:30px;padding:0 30px 0 60px}@media screen and (min-width:1200px){.video-collection-section[data-v-9bb44ad4]:nth-child(12),.video-collection-section[data-v-9bb44ad4]:nth-child(15),.video-collection-section[data-v-9bb44ad4]:nth-child(3),.video-collection-section[data-v-9bb44ad4]:nth-child(6),.video-collection-section[data-v-9bb44ad4]:nth-child(9){margin-right:0}}@media screen and (max-width:1200px){.video-collection-section[data-v-9bb44ad4]{height:auto;width:48%}.video-collection-section[data-v-9bb44ad4]:nth-child(10),.video-collection-section[data-v-9bb44ad4]:nth-child(2),.video-collection-section[data-v-9bb44ad4]:nth-child(4),.video-collection-section[data-v-9bb44ad4]:nth-child(6),.video-collection-section[data-v-9bb44ad4]:nth-child(8){margin-right:0}}@media screen and (max-width:820px){.video-collection-section[data-v-9bb44ad4]{margin-right:0!important;width:100%}}@media screen and (max-width:768px){.video-collection-section[data-v-9bb44ad4]{padding:20px}.video-list[data-v-9bb44ad4]{padding:0 30px 0 35px}}</style><style type="text/css">#main[data-v-2bc7bff8]{-ms-grid-columns:minmax(0,1fr) minmax(330px,1170px) minmax(0,1fr);background-color:var(--color-seashell);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:minmax(0,1fr) minmax(330px,1170px) minmax(0,1fr)}.header[data-v-2bc7bff8]{-ms-grid-column-span:3;align-items:center;background-color:#000;background-color:var(--color-black);display:flex;flex-direction:column;grid-column:1/span 3;-ms-grid-column:1;grid-row:1;-ms-grid-row:1;height:100px;justify-content:center;padding:20px;width:100%}:root .header[data-v-2bc7bff8],_[data-v-2bc7bff8]:-ms-fullscreen{background-color:#000!important}.title[data-v-2bc7bff8]{font-size:3.7rem;font-weight:500;line-height:4.3rem}.subtitle[data-v-2bc7bff8],.title[data-v-2bc7bff8]{color:var(--color-white);color:#fff}.subtitle[data-v-2bc7bff8]{font-size:2rem;font-weight:300}.video-collections-container[data-v-2bc7bff8]{display:flex;flex-wrap:wrap;grid-column:2;-ms-grid-column:2;grid-row:2;-ms-grid-row:2;margin:30px}@media screen and (max-width:768px){.title[data-v-2bc7bff8]{font-size:2.8rem}}</style><style type="text/css">img.small-image{display:none;height:0;visibility:hidden;width:0}#carousel{background:#8f8f8f}#carousel,.agile,.agile__list,.agile__track,.agile_slides{height:100%}.agile__nav-button{background:transparent;border:none;color:hsla(0,0%,100%,.639);cursor:pointer;font-size:17em;height:100%;position:absolute;top:0;transition-duration:.3s;width:80px}.agile__nav-button--prev{left:0}.agile__nav-button--next{right:0}.agile_slides:before{content:"";float:left;height:0;margin-left:-1px;padding-bottom:56.15%;width:1px}.agile_slides:after{clear:both;content:"";display:table}.slide-image{height:auto;position:relative;top:-10px;transform-origin:left top;width:100%}.agile__actions{background:#6f7271}.agile__dots{grid-gap:5px;bottom:-20px;display:grid!important;grid-template-columns:auto;grid-template-rows:20px;left:0;position:absolute;width:100%}.agile__dot{background:#8e8e8e;grid-row:1;height:100%;width:100%}.agile__dot button{cursor:pointer;height:100%;width:100%}.agile__dot button:hover,.agile__dot--current{background:var(--color-mexican-red)}@media screen and (max-width:1125px){.slide-image{height:100%;top:0;width:auto}}@media screen and (max-width:990px){.slide-image{top:0}}@media screen and (max-width:700px){.full-image{display:none;visibility:hidden}img.small-image{display:block;height:100%;visibility:visible;width:auto}.slide-image{height:100%;transform-origin:left bottom}.agile__nav-button{visibility:hidden}}@media screen and (max-width:645px){.full-image{display:none;visibility:hidden}img.small-image{display:block;height:100%;visibility:visible;width:100%}.slide-image{height:100%;transform-origin:left bottom}.agile__nav-button{visibility:hidden}}</style><style type="text/css">.heading[data-v-6fbc1d44]{border-bottom:6px solid var(--color-mexican-red);color:var(--color-black);display:inline-block;font-size:3rem;font-weight:700;font-weight:600;letter-spacing:0;padding-right:20px}*+.heading[data-v-6fbc1d44]{margin-top:30px}</style><style type="text/css">.list[data-v-773fa6d2]{background-color:var(--color-white);margin-top:30px;padding:25px}</style><style type="text/css">.list-item[data-v-489bcfad]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-489bcfad]:not(:first-child){padding-top:24px}.list-item[data-v-489bcfad]:last-child{border-bottom:none;padding-bottom:0}.icon[data-v-489bcfad]{flex-shrink:0;height:50px;-o-object-fit:scale-down;object-fit:scale-down;width:50px}.icon.gray[data-v-489bcfad]{filter:opacity(65%) grayscale(100%)}.content[data-v-489bcfad]{flex-grow:1;margin-left:18px}.content>.horizon-nuxt-link[data-v-489bcfad]{-webkit-user-select:text;-moz-user-select:text;user-select:text}.link[data-v-489bcfad],.title[data-v-489bcfad]{font-size:1.6rem;font-weight:500}.subtitle[data-v-489bcfad],.title[data-v-489bcfad]{color:var(--color-corduroy)}.subtitle[data-v-489bcfad]{font-size:1.4rem;font-weight:400;margin-top:4px}.under-development>p[data-v-489bcfad]{background-color:var(--color-swiss-coffee);font-weight:500;padding:1px 5px;position:relative;top:-2px}.under-development.full[data-v-489bcfad]{display:inline-block;margin-top:4px}.under-development.full>p[data-v-489bcfad]{display:inline}.under-development.mobile[data-v-489bcfad]{display:none}@media screen and (max-width:960px){.under-development.full[data-v-489bcfad]{display:none}.under-development.mobile[data-v-489bcfad]{display:block;margin-bottom:5px}.under-development.mobile>p[data-v-489bcfad]{display:inline}}</style><style type="text/css">.container[data-v-4b449b74]{background-color:var(--color-white);margin-top:30px;padding:20px 25px}</style><style type="text/css">.list-item[data-v-d33b0436]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-d33b0436]:not(:first-child){padding-top:24px}.list-item[data-v-d33b0436]:last-child{border-bottom:none;padding-bottom:0}.image[data-v-d33b0436]{border-radius:50%;filter:grayscale(100%);flex-shrink:0;height:75px;-o-object-fit:cover;object-fit:cover;width:75px}.content[data-v-d33b0436]{flex-basis:100%;margin-left:0}.title[data-v-d33b0436]{display:inline-block;font-size:1.6rem;font-weight:500;line-height:2rem}.article-info[data-v-d33b0436]{color:var(--color-corduroy);font-size:1.4rem;font-weight:400;height:2rem;line-height:2rem;margin-top:6px}</style><style type="text/css">.container[data-v-d31478de]{background-color:var(--color-white);margin-top:30px;padding:20px 25px}</style><style type="text/css">.message[data-v-f0c1df8c]{color:var(--color-corduroy);font-size:1.4rem}</style><style type="text/css">.list-item[data-v-4318e410]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-4318e410]:not(:first-child){padding-top:24px}.list-item[data-v-4318e410]:last-child{border-bottom:none;padding-bottom:0}.image[data-v-4318e410]{border-radius:50%;filter:grayscale(100%);flex-shrink:0;height:75px;-o-object-fit:cover;object-fit:cover;width:75px}.content[data-v-4318e410]{flex-basis:100%;margin-left:18px}.title[data-v-4318e410]{font-size:1.6rem;font-weight:500}.datetime[data-v-4318e410],.title[data-v-4318e410]{display:inline-block;line-height:2rem}.datetime[data-v-4318e410]{align-items:center;color:var(--color-corduroy);font-size:1.4rem;font-weight:400;height:2rem;justify-content:flex-start;margin-top:6px}.icon-calendar[data-v-4318e410]{fill:var(--color-corduroy);margin-top:-2px;width:12px}.date[data-v-4318e410]{margin-left:7px}.date[data-v-4318e410],.duration[data-v-4318e410],.time[data-v-4318e410]{color:var(--color-corduroy)}.duration[data-v-4318e410]{white-space:nowrap}.no-date[data-v-4318e410]{color:var(--color-corduroy);margin-left:7px}</style><style type="text/css">*+.title[data-v-395e4d50]{margin-top:30px}.title[data-v-395e4d50]{font-size:3rem;font-weight:600;letter-spacing:0;margin-top:40px;width:100%}.subtitle[data-v-395e4d50],.title[data-v-395e4d50]{color:var(--color-black);display:inline-block;padding-right:20px}.subtitle[data-v-395e4d50]{border-bottom:6px solid var(--color-mexican-red);font-size:1.3rem;font-weight:400;padding-bottom:2px}</style><style type="text/css">.list[data-v-0991dae4]{margin:0;padding:0}.container[data-v-0991dae4],.list[data-v-0991dae4]{background-color:var(--color-white)}.container[data-v-0991dae4]{margin-top:30px;padding:20px 25px}</style><style type="text/css">.list-item[data-v-1752cb36]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-1752cb36]:not(:first-child){padding-top:24px}.list-item[data-v-1752cb36]:last-child{border-bottom:none;padding-bottom:0}.icon[data-v-1752cb36]{flex-shrink:0;height:50px;-o-object-fit:scale-down;object-fit:scale-down;width:50px}.icon.gray[data-v-1752cb36]{filter:opacity(65%) grayscale(100%)}.content[data-v-1752cb36]{flex-basis:100%;flex-grow:1;margin-left:0}.content>.horizon-nuxt-link[data-v-1752cb36]{-webkit-user-select:text;-moz-user-select:text;user-select:text}.link[data-v-1752cb36],.title[data-v-1752cb36]{font-size:1.6rem;font-weight:500}.subtitle[data-v-1752cb36],.title[data-v-1752cb36]{color:var(--color-corduroy)}.subtitle[data-v-1752cb36]{font-size:1.4rem;font-weight:400;margin-top:4px}.under-development>p[data-v-1752cb36]{background-color:var(--color-swiss-coffee);font-weight:500;padding:1px 5px;position:relative;top:-2px}.under-development.full[data-v-1752cb36]{display:inline-block;margin-top:4px}.under-development.full>p[data-v-1752cb36]{display:inline}.under-development.mobile[data-v-1752cb36]{display:none}@media screen and (max-width:960px){.under-development.full[data-v-1752cb36]{display:none}.under-development.mobile[data-v-1752cb36]{display:block;margin-bottom:5px}.under-development.mobile>p[data-v-1752cb36]{display:inline}}</style><style type="text/css">.container[data-v-02c5ebd3]{background-color:var(--color-white);margin-top:30px;padding:20px 25px}</style><style type="text/css">.list-item[data-v-21e07c1a]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-21e07c1a]:not(:first-child){padding-top:24px}.list-item[data-v-21e07c1a]:last-child{border-bottom:none;padding-bottom:0}.image[data-v-21e07c1a]{border-radius:50%;filter:grayscale(100%);flex-shrink:0;height:75px;-o-object-fit:cover;object-fit:cover;width:75px}.content[data-v-21e07c1a]{flex-basis:100%;margin-bottom:0;margin-left:18px}.title[data-v-21e07c1a]{display:inline-block;font-size:1.6rem;font-weight:500;line-height:2rem}.date[data-v-21e07c1a]{margin-left:7px}.article-info[data-v-21e07c1a],.date[data-v-21e07c1a]{color:var(--color-corduroy)}.article-info[data-v-21e07c1a]{font-size:1.4rem;font-weight:400;height:2rem;line-height:2rem;margin-top:6px}</style><style type="text/css">#main[data-v-140c3f42]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;-ms-grid-rows:17% auto;background-color:var(--color-seashell);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:1fr minmax(330px,1170px) 1fr;grid-template-rows:17% auto}:root #main[data-v-140c3f42],_[data-v-140c3f42]:-ms-fullscreen{background-color:#f1f1f1!important}.hero[data-v-140c3f42]{-ms-grid-column-span:3;background-color:var(--color-black);background:#8f8f8f;grid-column:1/span 3;-ms-grid-column:1;grid-row:1;-ms-grid-row:1;margin:0 auto;position:relative;width:100%}:root .hero[data-v-140c3f42],_[data-v-140c3f42]:-ms-fullscreen{background-color:#000!important}.content[data-v-140c3f42]{display:flex;flex-wrap:wrap;grid-column:2;-ms-grid-column:2;grid-row:2;-ms-grid-row:2}.column[data-v-140c3f42]{flex-basis:50%;max-width:50%;padding:30px 30px 40px}.events[data-v-140c3f42],.topics[data-v-140c3f42]{margin-top:50px}.button[data-v-140c3f42]{margin-top:35px}.recently-published[data-v-140c3f42]{margin-top:40px}@media screen and (max-width:960px){#main[data-v-140c3f42]{grid-template-rows:450px 1fr}.content[data-v-140c3f42]{display:block}.events[data-v-140c3f42],.topics[data-v-140c3f42]{align-self:center;flex-basis:0;width:100%}.events[data-v-140c3f42]{margin-top:-30px}.column[data-v-140c3f42]{max-width:100%}}@media screen and (max-width:750px){#main[data-v-140c3f42]{grid-template-rows:450px 1fr}}@media screen and (max-width:700px){#main[data-v-140c3f42]{grid-template-rows:700px 1fr}}@media screen and (max-width:600px){#main[data-v-140c3f42]{grid-template-rows:545px 1fr}}@media screen and (max-width:465px){#main[data-v-140c3f42]{grid-template-rows:405px 1fr}}</style><style type="text/css">.content[data-v-1db38d26]{margin-top:60px}.message[data-v-1db38d26]{font-size:2rem;margin-top:15px}</style><style type="text/css">.event[data-v-3096f027]{background-color:var(--color-white);display:flex;margin-top:20px;padding:25px}.image[data-v-3096f027]{border-radius:50%;filter:grayscale(100%);flex-shrink:0;height:130px;max-height:130px;max-width:130px;-o-object-fit:cover;object-fit:cover;width:130px}.content[data-v-3096f027]{flex-basis:100%;flex-grow:1;margin-left:40px}.title[data-v-3096f027]{display:block;font-size:2rem;font-weight:400}.datetime[data-v-3096f027]{color:var(--color-corduroy);display:inline-block;font-size:1.4rem;font-weight:400;height:2rem;line-height:2rem;margin-top:5px}.icon-calendar[data-v-3096f027]{fill:var(--color-corduroy);width:12px}.date[data-v-3096f027]{margin-left:7px}.description[data-v-3096f027]{color:var(--color-black);font-family:var(--font-charter);font-size:1.5rem;line-height:2.4rem;margin-top:25px}.no-date[data-v-3096f027]{margin-left:7px}@media screen and (max-width:768px){.event[data-v-3096f027]{flex-direction:column}.content[data-v-3096f027]{margin:25px 0 0}}</style><style type="text/css">.content[data-v-794a845e]{margin-top:60px}.message[data-v-794a845e]{font-size:2rem;margin-top:15px}</style><style type="text/css">.events .event .description a{color:var(--color-mexican-red)}</style><style type="text/css">#main[data-v-df97dfba]{-ms-grid-columns:1fr minmax(330px,860px) 1fr;-ms-grid-rows:100px 1fr;background-color:var(--color-seashell);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:minmax(0,1fr) minmax(330px,860px) minmax(0,1fr);grid-template-rows:100px 1fr}:root #main[data-v-df97dfba],_[data-v-df97dfba]:-ms-fullscreen{background-color:#f1f1f1!important}.header[data-v-df97dfba]{-ms-grid-column-span:3;align-items:center;background-color:var(--color-black);display:flex;grid-column:1/span 3;-ms-grid-column:1;grid-row:1;-ms-grid-row:1;height:100px;justify-content:center;width:100vw}:root .header[data-v-df97dfba],_[data-v-df97dfba]:-ms-fullscreen{background-color:#000!important}.title[data-v-df97dfba]{color:var(--color-white);font-size:3.7rem;font-weight:500;line-height:4.3rem}:root .title[data-v-df97dfba],_[data-v-df97dfba]:-ms-fullscreen{color:#fff!important}.events[data-v-df97dfba]{grid-column:2;-ms-grid-column:2;margin:30px 0;padding:0 30px}.events.upcoming[data-v-df97dfba]{grid-row:2;-ms-grid-row:2}.events.past[data-v-df97dfba]{grid-row:3;-ms-grid-row:3;margin-bottom:60px}.section-title[data-v-df97dfba]{border-bottom:6px solid #a31f34;color:var(--color-black);display:inline-block;font-size:3rem;font-weight:600;line-height:4.5rem;padding-right:20px}@media screen and (max-width:670px){.title[data-v-df97dfba]{font-size:2.8rem}}</style><style type="text/css">.list[data-v-2d8a1d8d]{list-style-position:inside;list-style-type:none}.list-item[data-v-2d8a1d8d]{border-bottom:1px solid var(--color-swiss-coffee);line-height:2.4rem;padding:0 0 10px}.list-item+.list-item[data-v-2d8a1d8d]{margin-top:10px}.list-item[data-v-2d8a1d8d]:last-child{border-bottom:none;padding:0}.link[data-v-2d8a1d8d]{display:inline-flex;font-size:1.6rem}.status-tag[data-v-2d8a1d8d]{background-color:var(--color-swiss-coffee);font-weight:500;margin-left:10px;padding:1px 5px;position:relative;top:-2px}.title[data-v-2d8a1d8d]{display:block;font-size:1.6rem;font-weight:500}.owner[data-v-2d8a1d8d],.title[data-v-2d8a1d8d]{color:var(--color-corduroy)}.owner[data-v-2d8a1d8d]{font-size:1.4rem;font-weight:400}</style><style type="text/css">.breadcrumbs[data-v-1a3b9ffc]{display:flex;grid-column:2;-ms-grid-column:2;grid-row:3;-ms-grid-row:3;height:60px;padding:30px 30px 0}.link[data-v-1a3b9ffc]{font-size:1.6rem;margin-right:5px}@media screen and (max-width:960px){.breadcrumbs[data-v-1a3b9ffc]{flex-direction:column;height:100%}.link+.link[data-v-1a3b9ffc]{margin-top:8px}}</style><style type="text/css">.list[data-v-0759312e]{list-style-position:inside;list-style-type:none}.list-item[data-v-0759312e]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-0759312e]:not(:first-child){padding-top:24px}.list-item[data-v-0759312e]:last-child{border-bottom:none;padding-bottom:0}.image[data-v-0759312e]{border-radius:50%;filter:grayscale(100%);flex-shrink:0;height:75px;-o-object-fit:cover;object-fit:cover;width:75px}.content[data-v-0759312e]{flex-basis:100%;margin-left:18px}.title[data-v-0759312e]{font-size:1.6rem;font-weight:500}.datetime[data-v-0759312e],.title[data-v-0759312e]{display:inline-block;line-height:2rem}.datetime[data-v-0759312e]{color:var(--color-corduroy);font-size:1.4rem;font-weight:400;height:2rem;margin-top:8px}.icon-calendar[data-v-0759312e]{fill:var(--color-corduroy);width:12px}.date[data-v-0759312e],.no-date[data-v-0759312e]{margin-left:7px}</style><style type="text/css">.list[data-v-296c4ad4]{list-style-position:inside;list-style-type:none}.list-item[data-v-296c4ad4]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-296c4ad4]:not(:first-child){padding-top:24px}.list-item[data-v-296c4ad4]:last-child{border-bottom:none;padding-bottom:0}.image[data-v-296c4ad4]{border-radius:50%;filter:grayscale(100%);flex-shrink:0;height:75px;-o-object-fit:cover;object-fit:cover;width:75px}.content[data-v-296c4ad4]{flex-basis:100%;margin-left:18px}.title[data-v-296c4ad4]{font-size:1.6rem;font-weight:500}.datetime[data-v-296c4ad4],.title[data-v-296c4ad4]{display:inline-block;line-height:2rem}.datetime[data-v-296c4ad4]{color:var(--color-corduroy);font-size:1.4rem;font-weight:400;height:2rem;margin-top:8px}.icon-calendar[data-v-296c4ad4]{fill:var(--color-corduroy);width:12px}.date[data-v-296c4ad4],.no-date[data-v-296c4ad4]{margin-left:7px}</style><style type="text/css">.hero[data-v-4c4a83e6]{-ms-grid-column-span:3;background-color:var(--color-black);display:flex;grid-column:1/span 3;-ms-grid-column:1;grid-row:1;-ms-grid-row:1;position:relative}.hero[data-v-4c4a83e6],.image[data-v-4c4a83e6]{height:150px;width:100%}.image[data-v-4c4a83e6]{filter:grayscale(100%) opacity(40%);-o-object-fit:cover;object-fit:cover}:root .image[data-v-4c4a83e6],_[data-v-4c4a83e6]:-ms-fullscreen{display:none}.content[data-v-4c4a83e6]{position:absolute;top:50%;transform:translateY(-50%);width:100%}.title[data-v-4c4a83e6]{font-size:3.8rem;font-weight:300;font-weight:600;text-align:center}.description[data-v-4c4a83e6],.title[data-v-4c4a83e6]{align-items:center;color:var(--color-white);display:flex;justify-content:center;width:100%}.description[data-v-4c4a83e6]{font-size:2rem;font-weight:300}@media screen and (max-width:960px){.title[data-v-4c4a83e6]{font-size:3.2rem}}@media screen and (max-width:670px){.title[data-v-4c4a83e6]{font-size:2.1rem}.description[data-v-4c4a83e6]{font-size:1.8rem}}</style><style type="text/css">.list[data-v-142cf3ec]{list-style-position:inside;list-style-type:none}.list-item[data-v-142cf3ec]{border-bottom:1px solid var(--color-swiss-coffee);display:flex;padding:0 0 20px}.list-item[data-v-142cf3ec]:not(:first-child){padding-top:24px}.list-item[data-v-142cf3ec]:last-child{border-bottom:none;padding-bottom:0}.image[data-v-142cf3ec]{flex-shrink:0;height:75px;-o-object-fit:cover;object-fit:cover;width:75px}.content[data-v-142cf3ec]{flex-basis:100%;margin-left:18px}.title[data-v-142cf3ec]{display:block;font-size:1.6rem;font-weight:500;line-height:2rem}.datetime[data-v-142cf3ec]{color:var(--color-corduroy);display:inline-block;font-size:1.4rem;font-weight:400;height:2rem;line-height:2rem;margin-top:8px}.icon-clock[data-v-142cf3ec]{fill:var(--color-corduroy);margin-bottom:-2px}.length[data-v-142cf3ec]{margin-left:2px}.no-date[data-v-142cf3ec]{margin-left:7px}</style><style type="text/css">.section+.section[data-v-25521044]{margin-top:30px}.title[data-v-25521044]{border-bottom:6px solid var(--color-mexican-red)}.title[data-v-25521044],.title-w-sub[data-v-25521044]{color:var(--color-black);display:inline-block;font-size:3rem;font-weight:600;letter-spacing:0;padding-right:20px}.title-w-sub[data-v-25521044]{border-bottom:none;width:100%}.subtitle[data-v-25521044]{color:var(--color-black);display:inline-block;font-size:1.4rem;font-style:italic;font-weight:400;padding-bottom:2px;padding-right:20px}.content[data-v-25521044]{background-color:var(--color-white);margin-top:20px;padding:20px 25px}</style><style type="text/css">.section+.section[data-v-ee8468a2]{margin-top:30px}.title[data-v-ee8468a2]{border-bottom:6px solid var(--color-mexican-red);color:var(--color-black);display:inline-block;font-size:3rem;letter-spacing:0;margin-bottom:10px;padding-right:20px}.subtitle[data-v-ee8468a2]{font-size:1.4rem;font-style:italic;font-weight:300;margin-bottom:15px}.content[data-v-ee8468a2]{background-color:var(--color-white);margin-top:20px;padding:20px 25px;position:relative}.list[data-v-ee8468a2]{list-style-position:inside;list-style-type:none}.list-item[data-v-ee8468a2]{border-bottom:1px solid var(--color-swiss-coffee);line-height:2.4rem;padding:0 0 10px}.list-item+.list-item[data-v-ee8468a2]{margin-top:10px}.list-item[data-v-ee8468a2]:last-child{border-bottom:none;padding:0}.link[data-v-ee8468a2]{display:inline-flex;font-size:1.6rem}.more-link[data-v-ee8468a2]{background-color:var(--color-swiss-coffee);color:var(--color-black);display:block;height:30px;margin-left:auto;margin-top:10px;padding:5px 15px;width:87px}</style><style type="text/css">.more-link .more-icon{fill:var(--color-black);stroke:var(--color-mexican-red);margin-left:6px;width:.9rem}</style><style type="text/css">.section+.section[data-v-310ae7a7]{margin-top:30px}[data-v-310ae7a7]:root{--color-light-grey:#e3e3e3;--color-wolf-grey:#666868}.title[data-v-310ae7a7]{border-bottom:6px solid var(--color-mexican-red);font-size:3rem;font-weight:600;letter-spacing:0}.subtitle[data-v-310ae7a7],.title[data-v-310ae7a7]{color:var(--color-black);display:inline-block;padding-right:20px}.subtitle[data-v-310ae7a7]{font-size:1.4rem;font-style:italic;font-weight:400;margin-bottom:15px;padding-top:6px}.info-tiles[data-v-310ae7a7]{background-color:var(--color-white);justify-content:space-between;padding:0 1.6rem 2.8rem}.info-tile[data-v-310ae7a7],.info-tiles[data-v-310ae7a7]{align-items:center;display:flex}.info-tile[data-v-310ae7a7]{--border-width:0.2rem;border-color:#eee;border-style:solid;border-width:0 .2rem;border-width:0 var(--border-width) 0 var(--border-width);flex-direction:column;padding:0 1.6rem;text-align:center}.info-tile[data-v-310ae7a7]:first-child{border:none;padding-left:0}.info-tile[data-v-310ae7a7]:last-child{border:none;padding-right:0}.tile-icon[data-v-310ae7a7]{--size:3.2rem;height:3.2rem;height:var(--size);width:3.2rem;width:var(--size)}.tile-text[data-v-310ae7a7]{color:#666868;color:var(--color-wolf-grey);font-size:1.2rem;font-weight:700;margin-top:1rem}.video-modules[data-v-310ae7a7]{background-color:#e3e3e3;background-color:var(--color-light-grey);padding:2rem 2rem 1rem}.link-container[data-v-310ae7a7]{border-bottom:1px solid var(--color-swiss-coffee);margin-left:34px;position:relative}.link[data-v-310ae7a7]{line-height:2.4rem;padding:10px 0 8px}.link-container:first-child .link[data-v-310ae7a7]{padding-top:0}.link-container[data-v-310ae7a7]:last-child{border:none}.list-line[data-v-310ae7a7]{left:-2.5rem}.link-container:first-child:not(:last-child) .list-line[data-v-310ae7a7]{height:150%;top:8px}.link-container:last-child .list-line[data-v-310ae7a7]{height:0}.content-container[data-v-310ae7a7]{background-color:var(--color-white);display:flex;flex-direction:column;margin-top:10px;padding:10px 25px;position:relative}.pro-title[data-v-310ae7a7]{font-size:1.6rem;padding-bottom:.4rem}.slogan[data-v-310ae7a7]{color:#666868;color:var(--color-wolf-grey);font-size:1.4rem;font-weight:500;margin-bottom:1rem}</style><style type="text/css">.list[data-v-03241e1c]{list-style-position:inside;list-style-type:none}.list-item[data-v-03241e1c]{border-bottom:1px solid var(--color-swiss-coffee);line-height:2.4rem;padding:0 0 10px}.list-item+.list-item[data-v-03241e1c]{margin-top:10px}.list-item[data-v-03241e1c]:last-child{border-bottom:none;padding:0}.link[data-v-03241e1c]{display:inline-flex;font-size:1.6rem}.status-tag[data-v-03241e1c]{background-color:var(--color-swiss-coffee);font-weight:500;margin-left:10px;padding:1px 5px;position:relative;top:-2px}.title[data-v-03241e1c]{display:block;font-size:1.6rem;font-weight:500}.owner[data-v-03241e1c],.title[data-v-03241e1c]{color:var(--color-corduroy)}.owner[data-v-03241e1c]{font-size:1.4rem;font-weight:400}</style><style type="text/css">#main[data-v-6ff6f672]{-ms-grid-columns:1fr minmax(330px,1170px) 1fr;-ms-grid-rows:150px auto auto 1fr;background-color:var(--color-seashell);display:grid;display:-ms-grid;flex-grow:1;grid-template-columns:1fr minmax(330px,1170px) 1fr;grid-template-rows:150px auto auto 1fr}:root #main[data-v-6ff6f672],_[data-v-6ff6f672]:-ms-fullscreen{background-color:#f1f1f1!important}.content[data-v-6ff6f672]{display:flex;grid-column:2;-ms-grid-column:2;grid-row:4;-ms-grid-row:4;padding:30px 30px 40px}.column[data-v-6ff6f672]{flex-basis:50%;margin-bottom:50px}.column+.column[data-v-6ff6f672]{margin-left:50px}.button[data-v-6ff6f672]{margin-top:25px}.in-development[data-v-6ff6f672]{-ms-grid-column-span:3;background-color:#b6dce1;grid-column:1/span 3;-ms-grid-column:1;grid-row:2;-ms-grid-row:2;text-align:center}.in-development>p[data-v-6ff6f672]{font-size:1.6rem;font-weight:500;padding:10px 0}.topic-newsletter[data-v-6ff6f672]{margin:1px;padding:0}@media screen and (max-width:960px){.content[data-v-6ff6f672]{flex-direction:column}.column[data-v-6ff6f672]{align-self:center;flex-basis:0;width:100%}.column+.column[data-v-6ff6f672]{margin-left:0}}</style></head>
  <body data-new-gr-c-s-check-loaded="14.1143.0" data-gr-ext-installed="" style="overflow: hidden;">
    <div id="__nuxt"><!----><div id="__layout"><div id="layout-default" data-v-2efbf2a4=""><a href="https://learn.cce.af.mil/article/common-misconceptions-bda#main-content" class="sr-only" data-v-2efbf2a4="">Skip to main content</a> <header id="header" data-v-572edd00="" data-v-2efbf2a4=""><section class="content" data-v-572edd00=""><button id="header-menu" aria-label="open the menu" tabindex="0" name="menu" data-v-f1571146="" data-v-572edd00=""><svg viewBox="0 0 16 13" class="icon" data-v-f1571146=""><path d="M0 0h16v2.5H0zM0 5h16v2.5H0zM0 10h16v2.5H0z"></path></svg></button> <header data-v-572edd00=""><a href="https://learn.cce.af.mil/" id="header-logo" aria-label="MIT Horizon" class="nuxt-link-active" data-v-cc0202ae="" data-v-572edd00=""><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 296.9 26.2" xml:space="preserve" aria-label="MIT Horizon logo" alt="" role="img" class="icon" style="enable-background:new 0 0 296.9 26.2;" data-v-c9a8a4ae="" data-v-cc0202ae=""><g id="divider" data-v-c9a8a4ae=""></g> <g id="OfficeName_x5F_Off" class="st0" data-v-c9a8a4ae=""></g> <g id="MITLogo_x5F_Off" data-v-c9a8a4ae=""></g> <g id="Layer_4" data-v-c9a8a4ae=""><g data-v-c9a8a4ae=""><g data-v-c9a8a4ae=""><path d="M80.1,18.8H80L70.8,5.3v20.4H68V0.5h2.9l9.2,13.8l9.2-13.8h2.9v25.2h-2.8V5.3L80.1,18.8z" class="st5" data-v-c9a8a4ae=""></path> <path d="M99.2,0.5v25.2h-2.8V0.5H99.2z" class="st5" data-v-c9a8a4ae=""></path> <path d="M114.3,25.7h-2.9V3.1H103V0.5h19.8v2.6h-8.5V25.7z" class="st5" data-v-c9a8a4ae=""></path> <path d="M141.3,14.4v11.3h-2.8V0.5h2.8v11.2h14.5V0.5h2.8v25.2h-2.8V14.4H141.3z" class="st5" data-v-c9a8a4ae=""></path> <path d="M174.8,26.1c-7.7,0-12.8-6-12.8-13c0-7,5.2-13.1,12.9-13.1s12.8,6,12.8,13C187.7,20.1,182.5,26.1,174.8,26.1
  				z M174.8,2.7c-5.8,0-9.9,4.6-9.9,10.4s4.2,10.4,9.9,10.4s9.9-4.6,9.9-10.4S180.6,2.7,174.8,2.7z" class="st5" data-v-c9a8a4ae=""></path> <path d="M208.4,25.7l-7.3-9.8h-7.2v9.8H191V0.5h10.8c5.5,0,9.1,3,9.1,7.5c0,4.2-2.9,6.7-6.8,7.4l7.7,10.3H208.4z
  				 M201.6,3.1h-7.8v10.2h7.7c3.8,0,6.5-1.9,6.5-5.2C208.1,5,205.7,3.1,201.6,3.1z" class="st5" data-v-c9a8a4ae=""></path> <path d="M219.3,0.5v25.2h-2.8V0.5H219.3z" class="st5" data-v-c9a8a4ae=""></path> <path d="M225,3.1V0.5h19.7v1.9l-16.3,20.7h16.3v2.6h-20.2v-1.9l16.3-20.7H225z" class="st5" data-v-c9a8a4ae=""></path> <path d="M258.3,26.1c-7.7,0-12.8-6-12.8-13c0-7,5.2-13.1,12.9-13.1s12.8,6,12.8,13C271.2,20.1,265.9,26.1,258.3,26.1
  				z M258.3,2.7c-5.8,0-9.9,4.6-9.9,10.4s4.2,10.4,9.9,10.4c5.8,0,9.9-4.6,9.9-10.4S264,2.7,258.3,2.7z" class="st5" data-v-c9a8a4ae=""></path> <path d="M293,0.5h2.8v25.2h-2.3L277.2,5v20.7h-2.8V0.5h2.7L293,20.7V0.5z" class="st5" data-v-c9a8a4ae=""></path></g> <path d="M44.2,12.5c-0.4-0.8-0.8-1.5-1.3-2.2l-14.3,8.2l8.2-14.3c-0.7-0.5-1.4-0.9-2.2-1.3l-8.2,14.3V0.8
  			c-0.4,0-0.8,0-1.3,0c-0.4,0-0.9,0-1.3,0v16.5L15.6,3c-0.8,0.4-1.5,0.8-2.2,1.3l8.2,14.3L7.4,10.3c-0.5,0.7-0.9,1.4-1.3,2.2
  			l11.7,6.8H0v3.2h19.6c1.8,0,3.4,1.1,4.1,2.7c0.2,0.6,0.8,1,1.5,1s1.2-0.4,1.5-1c0.7-1.6,2.3-2.7,4.1-2.7h19.6v-3.2H32.5L44.2,12.5
  			z" class="st5" data-v-c9a8a4ae=""></path></g></g></svg></a></header> <nav id="header-navigation" aria-label="Main Navigation Links" data-v-39b4582a="" data-v-572edd00=""><a href="https://learn.cce.af.mil/" class="link" data-v-39b4582a="">
    Topics
  </a> <a href="https://learn.cce.af.mil/events" class="link" data-v-39b4582a="">
    Online Events
  </a> <a href="https://learn.cce.af.mil/podcasts" class="link" data-v-39b4582a="">
    Podcasts
  </a> <a href="https://learn.cce.af.mil/ebooks" class="link" data-v-39b4582a="">
    eBooks
  </a> <a href="https://learn.cce.af.mil/video-collections" class="link" data-v-39b4582a="">
    Horizon PRO
  </a></nav> <form id="header-search" data-v-2af08a1e="" data-v-572edd00=""><label aria-label="Search" for="search" class="label" data-v-2af08a1e=""><svg aria-hidden="true" alt="Search Icon" class="icon" data-v-2af08a1e=""><path d="M5.90906302 9.72727273c-2.10871529 0-3.81816013-1.70945455-3.81816013-3.81818182s1.70944484-3.81818182 3.81816013-3.81818182c2.10885165 0 3.81816012 1.70945455 3.81816012 3.81818182S8.01791467 9.72727273 5.90906302 9.72727273m6.93118788 2.34163637L9.74467759 8.97331818c.67185982-.83986363 1.07344844-1.90513636 1.07344844-3.06422727C10.81812603 3.19790909 8.62036579 1 5.90906302 1 3.1978966 1 1 3.19790909 1 5.90909091c0 2.71118182 2.1978966 4.90909091 4.90906302 4.90909091 1.15908432 0 2.22435099-.40172727 3.06420986-1.07359091l3.09557332 3.09559089c.1064994.1065.246135.1598182.3856342.1598182.1396355 0 .2792711-.0533182.3857705-.1598182.2129988-.2128636.2129988-.5584091 0-.7712727" stroke-width=".675"></path></svg>
    Search
    <div id="searchDesc" hidden="hidden" data-v-2af08a1e="">
      Search for Articles, Events, Ebooks, Podcasts, and more.
    </div></label> <input id="search" name="search" placeholder="Search" type="text" aria-describedby="searchDesc" value="" data-v-2af08a1e=""></form> <button id="header-account" name="account" data-v-c651f032="" data-v-572edd00="">
  Log Out
  <svg viewBox="0 0 512 512" aria-hidden="true" alt="Log Out Icon" class="icon" data-v-c651f032=""><path d="M255.15 468.625H63.787c-11.737 0-21.262-9.526-21.262-21.262V64.638c0-11.737 9.526-21.262 21.262-21.262H255.15c11.758 0 21.262-9.504 21.262-21.262S266.908.85 255.15.85H63.787C28.619.85 0 29.47 0 64.638v382.724c0 35.168 28.619 63.787 63.787 63.787H255.15c11.758 0 21.262-9.504 21.262-21.262 0-11.758-9.504-21.262-21.262-21.262z"></path> <path d="M505.664 240.861L376.388 113.286c-8.335-8.25-21.815-8.143-30.065.213s-8.165 21.815.213 30.065l92.385 91.173H191.362c-11.758 0-21.262 9.504-21.262 21.262 0 11.758 9.504 21.263 21.262 21.263h247.559l-92.385 91.173c-8.377 8.25-8.441 21.709-.213 30.065 4.167 4.21 9.653 6.336 15.139 6.336 5.401 0 10.801-2.041 14.926-6.124l129.276-127.575c4.04-3.997 6.336-9.441 6.336-15.139 0-5.696-2.275-11.118-6.336-15.137z"></path></svg></button> <a href="http://www.mit.edu/" rel="noopener" class="mit-logo" data-v-572edd00=""><svg id="Layer_1" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="40px" height="18px" viewBox="0 0 72 38" enable-background="new 0 0 72 38" xml:space="preserve" role="img" aria-labelledby="title" data-v-572edd00=""><title id="title">MIT Logo</title> <g><rect x="52" fill="#8B8B8C" width="20" height="8"></rect> <rect x="13" fill="#8B8B8C" width="8" height="26"></rect> <rect x="26" fill="#8B8B8C" width="8" height="38"></rect> <rect fill="#8B8B8C" width="8" height="38"></rect> <rect x="52" y="13" fill="#8B8B8C" width="8" height="25"></rect> <rect x="39" fill="#8B8B8C" width="8" height="8"></rect> <rect x="39" y="13" fill="#C3C0BF" width="8" height="25"></rect></g></svg></a></section></header> <!----> <!----> <aside id="sidebar" data-v-04c72ac8="" data-v-2efbf2a4=""><div class="mask" data-v-04c72ac8=""></div> <!----></aside>  <main id="main" data-v-4aa1c1da="" data-v-2efbf2a4=""><article class="article" data-v-4aa1c1da=""><nav class="article-breadcrumbs" data-v-6e2f4a60="" data-v-4aa1c1da=""><a href="https://learn.cce.af.mil/" class="horizon-nuxt-link link nuxt-link-active" data-v-6e2f4a60="">
  All Topics

  <svg viewBox="0 0 238.003 238.003" class="icon"><path d="M181.776,107.719L78.705,4.648c-6.198-6.198-16.273-6.198-22.47,0s-6.198,16.273,0,22.47
			l91.883,91.883l-91.883,91.883c-6.198,6.198-6.198,16.273,0,22.47s16.273,6.198,22.47,0l103.071-103.039
			c3.146-3.146,4.672-7.246,4.64-11.283C186.416,114.902,184.89,110.833,181.776,107.719z"></path></svg></a> <a href="https://learn.cce.af.mil/topic/big-data-analytics" class="horizon-nuxt-link link " data-v-6e2f4a60="">
  Big Data Analytics

  <svg viewBox="0 0 238.003 238.003" class="icon"><path d="M181.776,107.719L78.705,4.648c-6.198-6.198-16.273-6.198-22.47,0s-6.198,16.273,0,22.47
			l91.883,91.883l-91.883,91.883c-6.198,6.198-6.198,16.273,0,22.47s16.273,6.198,22.47,0l103.071-103.039
			c3.146-3.146,4.672-7.246,4.64-11.283C186.416,114.902,184.89,110.833,181.776,107.719z"></path></svg></a> <a href="https://learn.cce.af.mil/article/common-misconceptions-bda" aria-current="page" class="horizon-nuxt-link link nuxt-link-exact-active nuxt-link-active" data-v-6e2f4a60="">
  Common Misconceptions

  <!----></a></nav> <!----> <header id="main-content" class="article-header" data-v-04cf394c="" data-v-4aa1c1da=""><h1 class="article-title" data-v-de2ee628="" data-v-4aa1c1da="" data-v-04cf394c="">
  Common Misconceptions
</h1> <!----> <p class="article-byline" data-v-0c35bf98="" data-v-4aa1c1da="" data-v-04cf394c=""><!----> <!----> <span class="reading-time" data-v-0c35bf98="">
    6 min read
  </span></p> <!----></header> <aside aria-label="Article Navigation" class="article-navigation" data-v-7e9f4ce0="" data-v-4aa1c1da=""><nav aria-label="Table of Contents, skip to section" class="navigation" data-v-7e9f4ce0=""><h2 class="heading" data-v-7e9f4ce0="">Sections:</h2> <a target="_self" href="https://learn.cce.af.mil/article/common-misconceptions-bda#data-analytics-can-answer-every-question" class="horizon-link link" data-v-7e9f4ce0="">
      Data analytics can answer every question
    </a><a target="_self" href="https://learn.cce.af.mil/article/common-misconceptions-bda#the-more-data-the-better" class="horizon-link link" data-v-7e9f4ce0="">
      The more data the better
    </a><a target="_self" href="https://learn.cce.af.mil/article/common-misconceptions-bda#analyzing-big-data-requires-building-custom-tools-in-house" class="horizon-link link" data-v-7e9f4ce0="">
      Analyzing big data requires building custom tools in-house
    </a><a target="_self" href="https://learn.cce.af.mil/article/common-misconceptions-bda#big-data-analytics-is-only-for-large-organizations" class="horizon-link link" data-v-7e9f4ce0="">
      Big data analytics is only for large organizations
    </a></nav></aside> <section class="article-content" data-v-576aadda="" data-v-4aa1c1da=""><div classname="c1-player" topic="" data-v-576aadda=""><span id="listen" class="listen">Listen to this article</span> <div class="plyr-simplified" data-v-0a1aeb67=""><div tabindex="0" class="plyr plyr--full-ui plyr--audio plyr--html5 plyr--paused plyr--stopped"><div class="plyr__controls"><button class="plyr__controls__item plyr__control" type="button" data-plyr="restart"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-restart"></use></svg><span class="plyr__sr-only">Restart</span></button><button class="plyr__controls__item plyr__control" type="button" data-plyr="rewind"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-rewind"></use></svg><span class="plyr__sr-only">Rewind 10s</span></button><button class="plyr__controls__item plyr__control" type="button" data-plyr="play" aria-label="Play"><svg class="icon--pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-pause"></use></svg><svg class="icon--not-pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-play"></use></svg><span class="label--pressed plyr__sr-only">Pause</span><span class="label--not-pressed plyr__sr-only">Play</span></button><button class="plyr__controls__item plyr__control" type="button" data-plyr="fast-forward"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-fast-forward"></use></svg><span class="plyr__sr-only">Forward 10s</span></button><div class="plyr__controls__item plyr__progress__container"><div class="plyr__progress"><input data-plyr="seek" type="range" min="0" max="100" step="0.01" value="0" autocomplete="off" role="slider" aria-label="Seek" aria-valuemin="0" aria-valuemax="547.720089" aria-valuenow="0" id="plyr-seek-3601" aria-valuetext="00:00 of 00:00" style="--value: 0%;"><progress class="plyr__progress__buffer" min="0" max="100" value="0" role="progressbar" aria-hidden="true">% buffered</progress><span class="plyr__tooltip">00:00</span></div></div><div class="plyr__controls__item plyr__time--current plyr__time" aria-label="Current time">00:00</div><div class="plyr__controls__item plyr__time--duration plyr__time" aria-label="Duration">09:07</div><div class="plyr__controls__item plyr__volume"><button type="button" class="plyr__control" data-plyr="mute"><svg class="icon--pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-muted"></use></svg><svg class="icon--not-pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-volume"></use></svg><span class="label--pressed plyr__sr-only">Unmute</span><span class="label--not-pressed plyr__sr-only">Mute</span></button><input data-plyr="volume" type="range" min="0" max="1" step="0.05" value="1" autocomplete="off" role="slider" aria-label="Volume" aria-valuemin="0" aria-valuemax="100" aria-valuenow="100" id="plyr-volume-3601" aria-valuetext="100.0%" style="--value: 100%;"></div><button class="plyr__controls__item plyr__control" type="button" data-plyr="captions"><svg class="icon--pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-captions-on"></use></svg><svg class="icon--not-pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-captions-off"></use></svg><span class="label--pressed plyr__sr-only">Disable captions</span><span class="label--not-pressed plyr__sr-only">Enable captions</span></button><div class="plyr__controls__item plyr__menu"><button aria-haspopup="true" aria-controls="plyr-settings-3601" aria-expanded="false" type="button" class="plyr__control" data-plyr="settings"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-settings"></use></svg><span class="plyr__sr-only">Settings</span></button><div class="plyr__menu__container" id="plyr-settings-3601" hidden=""><div><div id="plyr-settings-3601-home"><div role="menu"><button data-plyr="settings" type="button" class="plyr__control plyr__control--forward" role="menuitem" aria-haspopup="true" hidden=""><span>Captions<span class="plyr__menu__value">Disabled</span></span></button><button data-plyr="settings" type="button" class="plyr__control plyr__control--forward" role="menuitem" aria-haspopup="true" hidden=""><span>Quality<span class="plyr__menu__value">undefined</span></span></button><button data-plyr="settings" type="button" class="plyr__control plyr__control--forward" role="menuitem" aria-haspopup="true"><span>Speed<span class="plyr__menu__value">Normal</span></span></button></div></div><div id="plyr-settings-3601-captions" hidden=""><button type="button" class="plyr__control plyr__control--back"><span aria-hidden="true">Captions</span><span class="plyr__sr-only">Go back to previous menu</span></button><div role="menu"></div></div><div id="plyr-settings-3601-quality" hidden=""><button type="button" class="plyr__control plyr__control--back"><span aria-hidden="true">Quality</span><span class="plyr__sr-only">Go back to previous menu</span></button><div role="menu"></div></div><div id="plyr-settings-3601-speed" hidden=""><button type="button" class="plyr__control plyr__control--back"><span aria-hidden="true">Speed</span><span class="plyr__sr-only">Go back to previous menu</span></button><div role="menu"><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="0.5"><span>0.5</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="0.75"><span>0.75</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="true" value="1"><span>Normal</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="1.25"><span>1.25</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="1.5"><span>1.5</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="1.75"><span>1.75</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="2"><span>2</span></button><button data-plyr="speed" type="button" role="menuitemradio" class="plyr__control" aria-checked="false" value="4"><span>4</span></button></div></div></div></div></div><button class="plyr__controls__item plyr__control" type="button" data-plyr="pip"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-pip"></use></svg><span class="plyr__sr-only">PIP</span></button><a class="plyr__controls__item plyr__control" href="https://learn.cce.af.mil/_media/audio/MITHorizonAudioCommonmisconceptionsaboutBigDataAnalytics.mp3" target="_blank" download="" data-plyr="download"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-download"></use></svg><span class="plyr__sr-only">Download</span></a><button class="plyr__controls__item plyr__control" type="button" data-plyr="fullscreen"><svg class="icon--pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-exit-fullscreen"></use></svg><svg class="icon--not-pressed" aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-enter-fullscreen"></use></svg><span class="label--pressed plyr__sr-only">Exit fullscreen</span><span class="label--not-pressed plyr__sr-only">Enter fullscreen</span></button></div><audio data-v-0a1aeb67="" playsinline=""><source data-v-0a1aeb67="" src="https://learn.cce.af.mil/_media/audio/MITHorizonAudioCommonmisconceptionsaboutBigDataAnalytics.mp3" type="audio/mp3"></audio><button type="button" class="plyr__control plyr__control--overlaid" data-plyr="play" aria-label="Play"><svg aria-hidden="true" focusable="false"><use href="/_nuxt/img/plyr.3a727a9.svg#plyr-play"></use></svg><span class="plyr__sr-only">Play</span></button></div></div></div><div topic="" class="article-heading" data-v-2d715583="" data-v-576aadda=""><h2 class="heading" data-v-2d715583="">
    Data analytics can answer every question
  </h2> <a id="data-analytics-can-answer-every-question" class="target" data-v-2d715583=""></a></div><div topic="" class="article-paragraph markdown" data-v-eec403c2="" data-v-576aadda=""><p><em>The reality: Data analytics can help in answering narrow, specific questions. It is not as helpful for broad or abstract questions.</em></p>
<p>Data analytics can provide answers to well-defined questions, such as Where do customers shop? It isnt as useful for broader or vaguer questions, such as How can we increase sales? Data analytics <em>can</em> still help an organization answer a question like thatbut incrementally, if it can be broken into smaller, more manageable queries, such as where do customers shop, what do they buy, and when do they buy it. Combining the answers to those narrow questions can allow an organization to begin addressing the larger one.</p>
<p>Even when a large or abstract question can be divided into narrow, specific queries, however, data analytics may still be unable to provide useful answers. Some real-world circumstances include too many variables, in relationships too complex or mysterious, for experts to quantify. For example, in a 2019 <a href="https://www.pnas.org/content/117/15/8398#sec-1">study</a> published by PNAS, the official journal of the National Academy of Sciences, 160 teams of scientists were invited to answer a very big question: How would the lives of a cohort of children turn out? To help the scientists predict outcomes such as grade-point average and family evictions, they were provided with massive data sets collected over 15 years, including information about each childs health and development, education, family income, parental discipline, and sibling relationships; and transcripts of interviews with parents and caregivers. Scientists were invited to use any analytical techniques on that data, ranging from statistical analysis to artificial intelligence. Nonetheless, the accuracy of their predictions proved low. Despite having access to a rich data set, and despite using modern <span class="glossary-term">machine learning<span class="glossary-definition">A sophisticated type of artificial intelligence. For more, see our Artificial Intelligence topic, specifically "How AI Works."</span></span> methods that are optimized for prediction, none of the 160 teams were able to make very accurate predictions, Matthew Salganik, a Princeton professor and one of the studys authors, tells MIT Horizon. The most accurate models were generally not much better than naive guessing.</p>
</div><figure topic="" class="article-image body" data-v-4b0c7309="" data-v-576aadda=""><div aria-modal="true" data-v-632b4e38="" data-v-4b0c7309=""><a href="https://learn.cce.af.mil/article/[object%20Object]" target="_blank" class="lightbox__thumbnail" data-v-632b4e38=""><img src="./Common Misconceptions _ MIT Horizon_files/bd_103_01_specific_queries_1f6f78cc88_a2c219730e.svg" alt="" class="image" data-v-632b4e38=""></a> <div data-v-632b4e38="" class="lightbox"><div data-v-632b4e38="" class="lightbox__close"><svg data-v-632b4e38="" fill="#424242" viewBox="0 0 17 17" xmlns="http://www.w3.org/2000/svg" aria-label="close the image" alt="Dismiss Banner" id="close-image" tabindex="0" role="button"><g transform="scale(.17, .17)"><path d="m60.285 48.281l37.027-37.02c2.5898-2.5938 2.5742-6.7461 0-9.3242-2.5781-2.5898-6.7305-2.5586-9.3086 0l-37.02 37.023-37.023-37.023c-2.5898-2.5898-6.7422-2.5742-9.3047 0-2.5938 2.5977-2.5781 6.7422 0 9.3242l37.023 37.02-37.023 37.008c-2.5938 2.5898-2.5781 6.7422 0 9.3164 2.5781 2.5977 6.7305 2.582 9.3047 0l37.023-37.02 37.02 37.02c2.5938 2.5977 6.7461 2.582 9.3086 0 2.5898-2.5898 2.5742-6.7422 0-9.3164z" fill-rule="evenodd"></path></g></svg></div> <div data-v-632b4e38="" class="lightbox__element"><!----> <div data-v-632b4e38="" class="lightbox__image"> <img data-v-4b0c7309="" data-v-632b4e38="" src="./Common Misconceptions _ MIT Horizon_files/bd_103_01_specific_queries_1f6f78cc88_a2c219730e(1).svg" class="svg"> <div data-v-4b0c7309="" data-v-632b4e38="" class="lightbox-caption-container"><figcaption data-v-4b0c7309="" data-v-632b4e38="" class="caption">A large and complicated question such as "How can we increase sales?" can be broken down into simpler queries for which the data is measurable and specific.</figcaption></div></div> <!----></div></div></div> <figcaption class="caption" data-v-4b0c7309="">A large and complicated question such as "How can we increase sales?" can be broken down into simpler queries for which the data is measurable and specific.</figcaption></figure><div topic="" class="article-heading" data-v-2d715583="" data-v-576aadda=""><h2 class="heading" data-v-2d715583="">
    The more data the better
  </h2> <a id="the-more-data-the-better" class="target" data-v-2d715583=""></a></div><div topic="" class="article-paragraph markdown" data-v-eec403c2="" data-v-576aadda=""><p><em>The reality: A small amount of good data, paired with an effective process, can yield better insights than a lot of data managed badly.</em></p>
<p>A larger data set does not necessarily translate into better insights. In fact, many of the problems that organizations use big data analytics to solve can also be solved with smaller data sets and practical thinking. Focusing on <em>quantity</em> over <em>quality</em> is therefore a common mistake. Poor-quality data is a huge problem, Bruce Rogers, then chief insights officer at Forbes Media, <a href="https://www.forbes.com/sites/forbespr/2017/05/31/poor-quality-data-imposes-costs-and-risks-on-businesses-says-new-forbes-insights-report/?sh=560c5e7c452b">said</a> in 2017. It leaves many companies trying to navigate the information age in the equivalent of a horse and buggy.</p>
<p>Poor-quality data includes typos, inaccuracies, and missing entries. (Such data may have been improperly <em>cleaned</em>. For more on data cleaning, see <a href="https://learn.cce.af.mil/article/limitations-bd">Limitations</a>.) But even if data is clean and accurate, it may simply be irrelevant to the question an organization is trying to answer. An organization hoping to better engage its customers doesnt need the customers every biographical detail before it can reach them. It needs only the details that correlate highly with their engagement.</p>
<p>Even when data is clean, accurate, and relevant, lots of it may still prove too cumbersome (or resource intensive) for an organization to manage. Rather than race to collect more data, therefore, an organization should carefully consider the goal of its analytics project, what question it is trying to answer, and what information is necessary to answer it. Translating the real-world problem into the exact right engineering problem should be 80% of the effort, David Scheinker, a Stanford professor who works with hospitals to implement lessons from data, tells MIT Horizon. Using fancy mathematical tools should be 5%, and the remaining 15% should be iterating.</p>
</div><div topic="" class="article-heading" data-v-2d715583="" data-v-576aadda=""><h2 class="heading" data-v-2d715583="">
    Analyzing big data requires building custom tools in-house
  </h2> <a id="analyzing-big-data-requires-building-custom-tools-in-house" class="target" data-v-2d715583=""></a></div><div topic="" class="article-paragraph markdown" data-v-eec403c2="" data-v-576aadda=""><p><em>The reality: Many forms of data analytics can be done with off-the-shelf products.</em></p>
<p>Organizations embarking on big data analytics projects often assume they must build all the tools theyll use in-house. Doing so is expensive and laborious and can be daunting for a company that doesnt specialize in tech. But in many circumstances, its unnecessary. The most common mistake I see is people still running their own infrastructure, Abel Sanchez, an MIT research scientist who specializes in helping companies use big data analytics, tells MIT Horizon.</p>
<p>A variety of off-the-shelf products are available for storing and analyzing data. Common examples come from Amazon, Google, and Microsoft. These providers and others typically deliver their services using remote servers, rather than depending on the equipment on the customers premises. Remotely accessed resources like this are often called in the cloud; for more, see <a href="https://learn.cce.af.mil/topic/cloud-computing">Cloud Computing</a>, and for an overview specifically of major providers and their services, see <a href="https://learn.cce.af.mil/article/major-cloud-computing-service-providers">Major Cloud Providers</a>.</p>
<p>One available service, for example, is <em>data warehousing</em>collecting and storing data in one secure place. Most organizations can outsource their data warehousing to a professional provider for $20 80 per <span class="glossary-term">terabyte<span class="glossary-definition">A terabyte stores roughly 250,000 photos or 6.5 million documents.</span></span> per month, depending on their needs. Those rates are significantly cheaper than what it would cost an organization to purchase, install, and maintain its own servers. (These costs are more difficult to average, since they include up-front equipment purchases and do not scale linearly in proportion to data. However, experts estimate a typical cost per terabyte in the $50120 range.)</p>
<p>In addition to saving on storage, organizations using off-the-shelf tools can also save on personnel costs. Data scientists typically command salaries of $100,000 to $200,000. For an organization that cannot afford its own expert staff, the same providers that offer data warehousing offer data analytics platforms that automatically perform some of the tasks a data scientist or other personnel would. All told, Sanchez, of MIT, estimates that by using off-the-shelf products instead of building tools in-house, organizations can save as much as 30%.</p>
</div><div topic="" class="article-heading" data-v-2d715583="" data-v-576aadda=""><h2 class="heading" data-v-2d715583="">
    Big data analytics is only for large organizations
  </h2> <a id="big-data-analytics-is-only-for-large-organizations" class="target" data-v-2d715583=""></a></div><div topic="" class="article-paragraph markdown" data-v-eec403c2="" data-v-576aadda=""><p><em>The reality: Organizations of all sizes can usually access enough data for analytics, and learn from it.</em></p>
<p>A small or medium-sized organization can almost always learn from more data than it alone generates. It can do so by pairing the data it collects with data supplied by others. This allows small entities to combine their knowledge and learn from others in similar situations.</p>
<p>But even small organizations can often generate enough data to learn from through analytics. For example, Twiddy &amp; Company, a family-owned business in North Carolina that employs about 130 full-time staff, is a longtime user of big data analytics. Twiddy &amp; Company rents vacation homes on the Outer Banks; beginning in 2008, it worked with SAS, a data analytics platform, to help determine what price to rent homes at as well as when, where, and what to advertise. Eventually, the company hired its own internal data analytics team. (To learn more about Twiddy &amp; Companys journey, see MIT Horizons archived <a href="https://learn.cce.af.mil/event/how-big-data-can-benefit-small-businesses-with-clark-twiddy">event</a> with company president Clark Twiddy.)</p>
</div><blockquote topic="" class="article-blockquote markdown" data-v-7368c93e="" data-v-576aadda=""><span class="quotes" data-v-7368c93e=""></span> <div class="content" data-v-7368c93e=""><p>At our own small company   like big companies, weve used data at scale to make a measurably valuable impact to our customer experience. <a href="https://horizonapp.mit.edu/event/how-big-data-can-benefit-small-businesses-with-clark-twiddy">Clark Twiddy to MIT Horizon</a></p>
</div></blockquote><div topic="" class="article-paragraph markdown" data-v-eec403c2="" data-v-576aadda=""><p>Twiddy &amp; Company <a href="https://hospitalitytech.com/how-vacation-rental-company-used-data-analytics-decrease-vacancies-twice-rate-its-competitors">credits its data analytics strategy</a> with helping the company survive the COVID-19 pandemic, when the bridge linking the Outer Banks to the mainland closed to visitors, cutting off renters and revenue. While competitors (including national brands like Arbnb and Vrbo) reduced their advertising during the bridge closure, Twiddy used data from Google Analytics and other tools to continue advertising, but in a targeted way. If a potential customer searching for rentals had previously stayed at Twiddy property in a particular North Carolina town, Twiddy showed them ads for rentals in that town specifically. According to the company, efforts like this one quadrupled the value it got from its advertising budget. During the eight weeks the bridge was closed in spring 2020, Twiddy calculates it booked twice as many rentals as some competitors did, and was better positioned for both demand and price when the bridge reopened. Clark Twiddy says that data analytics helped the company make profitable decisions despite uncertain circumstances. We were all dealing with more stress, more anxiety than we ever had, he <a href="https://smallbiztrends.com/2020/11/clark-twiddy-twiddy-co-interview.html">said</a> in November 2020. But we trusted our ability to make data-driven decisions.</p>
</div><h3 topic="" class="article-subheading" data-v-7ac49cc3="" data-v-576aadda="">
  Next up
</h3><div topic="" class="article-list" data-v-0e21aa74="" data-v-576aadda=""><ul>
<li><a href="https://learn.cce.af.mil/article/recent-developments-bd" target="_blank">Recent developments in big data analytics<span class="sr-only">Link opens in new tab</span></a></li>
</ul>
</div> <!----></section> <!----> <!----></article></main> <!----> <footer id="footer" data-v-44437d65="" data-v-2efbf2a4=""><div class="content" data-v-44437d65=""><div id="footer-about" data-v-65e247b0="" data-v-44437d65=""><svg viewBox="0 0 38 21" aria-hidden="true" alt="" role="img" aria-label="MIT Logo" class="logo" data-v-65e247b0=""><path d="M.62 20.0589312V.84412416h4.05098496V20.0589312H.62zm6.59902464-6.134953V.84288h4.05098496v13.0810982H7.21902464zm6.59778046 6.1361972V.84412416h4.050985V20.0601754h-4.050985zm6.5990247-15.39648004V.84412416h4.0509849v3.8195712h-4.0509849zm6.7134873 15.39648004V6.97907712h4.050985V20.0601754h-4.050985zm-6.7134873 0V6.97907712h4.0509849V20.0601754h-4.0509849zM27.128073 4.66369536V.84412416h10.6500096v3.8195712H27.128073z"></path></svg> <p class="copyright" data-v-65e247b0="">
     2023 All rights reserved. MIT Horizon<br data-v-65e247b0="">
    Massachusetts Institute of Technology<br data-v-65e247b0="">
    Cambridge, MA 02139
  </p></div> <nav id="footer-navigation" aria-label="Footer navagation links" data-v-458174f5="" data-v-44437d65=""><a href="https://learn.cce.af.mil/article/faq" class="link" data-v-458174f5="">
    FAQ
  </a> <a href="https://digitalu.af.mil/app/support" rel="noopener" target="_blank" class="link" data-v-458174f5=""><span class="sr-only" data-v-458174f5="">Opens support page in new window</span>
    Contact Us
  </a> <!----> <a href="https://learn.cce.af.mil/public/privacy-policy" class="link" data-v-458174f5="">
    Privacy Policy
  </a> <a href="https://learn.cce.af.mil/public/terms-of-service" class="link" data-v-458174f5="">
    Terms of Service
  </a> <a rel="noopener" href="https://accessibility.mit.edu/" class="link" data-v-458174f5="">
    Accessibility
  </a></nav></div></footer></div></div></div><script>window.__NUXT__=(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,aA,aB,aC,aD,aE,aF,aG,aH,aI,aJ,aK,aL,aM,aN,aO,aP,aQ,aR,aS,aT,aU,aV,aW,aX,aY,aZ,a_,a$,ba,bb,bc,bd,be,bf,bg,bh,bi,bj,bk,bl,bm,bn,bo,bp,bq,br,bs,bt,bu,bv,bw,bx,by,bz,bA,bB,bC,bD,bE,bF,bG,bH,bI,bJ,bK,bL,bM,bN,bO,bP,bQ,bR,bS,bT,bU,bV,bW,bX,bY,bZ,b_,b$,ca,cb,cc,cd,ce,cf,cg,ch,ci,cj,ck,cl,cm,cn,co,cp,cq,cr,cs,ct,cu,cv,cw,cx,cy,cz,cA,cB,cC,cD,cE,cF,cG,cH,cI,cJ,cK,cL,cM,cN,cO,cP,cQ,cR,cS,cT,cU,cV,cW,cX,cY,cZ,c_,c$,da,db,dc,dd,de,df,dg,dh,di,dj,dk,dl,dm,dn,do0,dp,dq,dr,ds,dt,du,dv,dw,dx,dy,dz,dA,dB,dC,dD,dE,dF,dG,dH,dI,dJ,dK,dL,dM,dN,dO,dP,dQ,dR,dS,dT,dU,dV,dW,dX,dY,dZ,d_,d$,ea,eb,ec,ed,ee,ef,eg,eh,ei,ej,ek,el,em,en,eo,ep,eq,er,es,et,eu,ev,ew,ex,ey,ez,eA,eB,eC,eD,eE,eF,eG,eH,eI,eJ,eK,eL,eM,eN,eO,eP,eQ,eR,eS,eT,eU,eV,eW,eX,eY,eZ,e_,e$,fa,fb,fc,fd,fe,ff,fg,fh,fi,fj,fk,fl,fm,fn,fo,fp,fq,fr,fs,ft,fu,fv,fw,fx,fy,fz,fA,fB,fC,fD,fE,fF,fG,fH,fI,fJ,fK,fL,fM,fN,fO,fP,fQ,fR,fS,fT,fU,fV,fW,fX,fY,fZ,f_,f$,ga,gb,gc,gd,ge,gf,gg,gh,gi,gj,gk,gl,gm,gn,go,gp,gq,gr,gs,gt,gu,gv,gw,gx,gy,gz,gA,gB,gC,gD,gE,gF,gG,gH,gI,gJ,gK,gL,gM,gN,gO,gP,gQ,gR,gS,gT,gU,gV,gW,gX,gY,gZ,g_,g$,ha,hb,hc,hd,he,hf,hg,hh,hi,hj,hk,hl,hm,hn,ho,hp,hq,hr,hs,ht,hu,hv,hw,hx,hy,hz,hA,hB,hC,hD,hE,hF,hG,hH,hI,hJ,hK,hL,hM,hN,hO,hP,hQ,hR,hS,hT,hU,hV,hW,hX,hY,hZ,h_,h$,ia,ib,ic,id,ie,if0,ig,ih,ii,ij,ik,il,im,in0,io,ip,iq,ir,is,it,iu,iv,iw,ix,iy,iz,iA,iB,iC,iD,iE,iF,iG,iH,iI,iJ,iK,iL,iM,iN,iO,iP,iQ,iR,iS,iT,iU,iV,iW,iX,iY,iZ,i_,i$,ja,jb,jc,jd,je,jf,jg,jh,ji,jj,jk,jl,jm,jn,jo,jp,jq,jr,js,jt,ju,jv,jw,jx,jy,jz,jA,jB,jC,jD,jE,jF,jG,jH,jI,jJ,jK,jL,jM,jN,jO,jP,jQ,jR,jS,jT,jU,jV,jW,jX,jY,jZ,j_,j$,ka,kb,kc,kd,ke,kf,kg,kh,ki,kj,kk,kl,km,kn,ko,kp,kq,kr,ks,kt,ku,kv,kw,kx,ky,kz,kA,kB,kC,kD,kE,kF,kG,kH,kI,kJ,kK,kL,kM,kN,kO,kP,kQ,kR,kS,kT,kU,kV,kW,kX,kY,kZ,k_,k$,la,lb,lc,ld,le,lf,lg,lh,li,lj,lk,ll,lm,ln,lo,lp,lq,lr,ls,lt,lu,lv,lw,lx,ly,lz,lA,lB,lC,lD,lE,lF,lG,lH,lI,lJ,lK,lL,lM,lN,lO,lP,lQ,lR,lS,lT,lU,lV,lW,lX,lY,lZ,l_,l$,ma,mb,mc,md,me,mf,mg,mh,mi,mj,mk,ml,mm,mn,mo,mp,mq,mr,ms,mt,mu,mv,mw,mx,my,mz,mA,mB,mC,mD,mE,mF,mG,mH,mI,mJ,mK,mL,mM,mN,mO,mP,mQ,mR,mS,mT,mU,mV,mW,mX,mY,mZ,m_,m$,na,nb,nc,nd,ne,nf,ng,nh,ni){iA.contentType={sys:{id:"audio"}};iA.id=aP;iB.contentType={sys:{id:g$}};iB.id=g_;iC.contentType={sys:{id:gZ}};iC.id=ha;iD.data={id:he,attributes:{name:hN,alternativeText:a,caption:a,width:H,height:aQ,formats:a,hash:hO,ext:r,mime:s,size:hP,url:hf,previewUrl:a,provider:j,provider_metadata:a,createdAt:gU,updatedAt:gU}};iD.fields={file:{url:hf},title:iE,description:iE};iF.contentType={sys:{id:"image"}};iF.id=hb;iG.contentType={sys:{id:g$}};iG.id=hg;iH.contentType={sys:{id:gZ}};iH.id=hh;iI.contentType={sys:{id:g$}};iI.id=hi;iJ.contentType={sys:{id:gZ}};iJ.id=hj;iK.contentType={sys:{id:g$}};iK.id=hk;iL.contentType={sys:{id:gZ}};iL.id=hl;iM.contentType={sys:{id:"blockquote"}};iM.id=hm;iN.contentType={sys:{id:gZ}};iN.id=ho;iO.contentType={sys:{id:"subheading"}};iO.id=hp;iP.contentType={sys:{id:"list"}};iP.id=hq;return {layout:"default",data:[{article:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e,pdf:a,logo:a,contentBlocks:[{id:aP,__component:bB,audioUrl:hJ,sys:iA,fields:{id:aP,__component:bB,audioUrl:hJ,sys:iA}},{id:g_,__component:f,text:hK,slug:hL,sys:iB,fields:{id:g_,__component:f,text:hK,slug:hL,sys:iB}},{id:ha,__component:d,text:hM,sys:iC,fields:{id:ha,__component:d,text:hM,sys:iC}},{id:hb,__component:w,title:hc,type:x,caption:hd,image:iD,sys:iF,fields:{id:hb,__component:w,title:hc,type:x,caption:hd,image:iD,sys:iF}},{id:hg,__component:f,text:hQ,slug:hR,sys:iG,fields:{id:hg,__component:f,text:hQ,slug:hR,sys:iG}},{id:hh,__component:d,text:hS,sys:iH,fields:{id:hh,__component:d,text:hS,sys:iH}},{id:hi,__component:f,text:hT,slug:hU,sys:iI,fields:{id:hi,__component:f,text:hT,slug:hU,sys:iI}},{id:hj,__component:d,text:hV,sys:iJ,fields:{id:hj,__component:d,text:hV,sys:iJ}},{id:hk,__component:f,text:hW,slug:hX,sys:iK,fields:{id:hk,__component:f,text:hW,slug:hX,sys:iK}},{id:hl,__component:d,text:hY,sys:iL,fields:{id:hl,__component:d,text:hY,sys:iL}},{id:hm,__component:hn,text:hZ,sys:iM,fields:{id:hm,__component:hn,text:hZ,sys:iM}},{id:ho,__component:d,text:h_,sys:iN,fields:{id:ho,__component:d,text:h_,sys:iN}},{id:hp,__component:v,text:gL,sys:iO,fields:{id:hp,__component:v,text:gL,sys:iO}},{id:hq,__component:aM,items:h$,sys:iP,fields:{id:hq,__component:aM,items:h$,sys:iP}}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm,image:{data:{id:2058,attributes:{name:"662_818e7d7f9c.png",alternativeText:a,caption:a,width:aN,height:ia,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_662_818e7d7f9c_4b7d82b259.png",hash:"thumbnail_662_818e7d7f9c_4b7d82b259",mime:p,name:"thumbnail_662_818e7d7f9c.png",path:a,size:32.99,width:hr,height:z}},hash:"662_818e7d7f9c_4b7d82b259",ext:o,mime:p,size:31.62,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F662_818e7d7f9c_4b7d82b259.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iQ,updatedAt:iQ}}}}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu,image:{data:{id:2071,attributes:{name:"675_bd144c49bb.png",alternativeText:a,caption:a,width:aN,height:ia,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_675_bd144c49bb_71db72a601.png",hash:"thumbnail_675_bd144c49bb_71db72a601",mime:p,name:"thumbnail_675_bd144c49bb.png",path:a,size:35.35,width:hr,height:z}},hash:"675_bd144c49bb_71db72a601",ext:o,mime:p,size:32.82,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F675_bd144c49bb_71db72a601.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iR,updatedAt:iR}}}}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD,image:{data:{id:2089,attributes:{name:"693_8cf3f1911b.png",alternativeText:a,caption:a,width:aN,height:ia,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_693_8cf3f1911b_5b4af6c3c7.png",hash:"thumbnail_693_8cf3f1911b_5b4af6c3c7",mime:p,name:"thumbnail_693_8cf3f1911b.png",path:a,size:28.36,width:hr,height:z}},hash:"693_8cf3f1911b_5b4af6c3c7",ext:o,mime:p,size:26.58,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F693_8cf3f1911b_5b4af6c3c7.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iS,updatedAt:iS}}}}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM,image:{data:{id:2091,attributes:{name:"695_55e28261b7.png",alternativeText:a,caption:a,width:aN,height:aN,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_695_55e28261b7_206dfffb7f.png",hash:"thumbnail_695_55e28261b7_206dfffb7f",mime:p,name:"thumbnail_695_55e28261b7.png",path:a,size:32.48,width:z,height:z}},hash:"695_55e28261b7_206dfffb7f",ext:o,mime:p,size:26.13,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F695_55e28261b7_206dfffb7f.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iT,updatedAt:iT}}}}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV,image:{data:{id:2094,attributes:{name:"698_2c1da65fa2.png",alternativeText:a,caption:a,width:aN,height:aN,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_698_2c1da65fa2_e0ddbbff7f.png",hash:"thumbnail_698_2c1da65fa2_e0ddbbff7f",mime:p,name:"thumbnail_698_2c1da65fa2.png",path:a,size:33.63,width:z,height:z}},hash:"698_2c1da65fa2_e0ddbbff7f",ext:o,mime:p,size:30.68,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F698_2c1da65fa2_e0ddbbff7f.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iU,updatedAt:iU}}}}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc,image:{data:{id:2098,attributes:{name:"702_e491658039.png",alternativeText:a,caption:a,width:aN,height:aN,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_702_e491658039_dafa97e529.png",hash:"thumbnail_702_e491658039_dafa97e529",mime:p,name:"thumbnail_702_e491658039.png",path:a,size:31.11,width:z,height:z}},hash:"702_e491658039_dafa97e529",ext:o,mime:p,size:25.18,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F702_e491658039_dafa97e529.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iV,updatedAt:iV}}}}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl,image:{data:{id:2100,attributes:{name:"704_ef2d26f9a7.png",alternativeText:a,caption:a,width:aN,height:aN,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_704_ef2d26f9a7_84788c3340.png",hash:"thumbnail_704_ef2d26f9a7_84788c3340",mime:p,name:"thumbnail_704_ef2d26f9a7.png",path:a,size:29.84,width:z,height:z}},hash:"704_ef2d26f9a7_84788c3340",ext:o,mime:p,size:34.56,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F704_ef2d26f9a7_84788c3340.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iW,updatedAt:iW}}}}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du,image:{data:{id:2123,attributes:{name:"727_ec3d7f4e44.png",alternativeText:a,caption:a,width:aN,height:aN,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_727_ec3d7f4e44_bac0368ba3.png",hash:"thumbnail_727_ec3d7f4e44_bac0368ba3",mime:p,name:"thumbnail_727_ec3d7f4e44.png",path:a,size:34.38,width:z,height:z}},hash:"727_ec3d7f4e44_bac0368ba3",ext:o,mime:p,size:30.16,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F727_ec3d7f4e44_bac0368ba3.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iX,updatedAt:iX}}}}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC,image:{data:{id:2154,attributes:{name:"Pablo_1_c34b067964.png",alternativeText:a,caption:a,width:iY,height:iY,formats:{thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Pablo_1_c34b067964_45d1d19195.png",hash:"thumbnail_Pablo_1_c34b067964_45d1d19195",mime:p,name:"thumbnail_Pablo_1_c34b067964.png",path:a,size:39.17,width:z,height:z}},hash:"Pablo_1_c34b067964_45d1d19195",ext:o,mime:p,size:25.59,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FPablo_1_c34b067964_45d1d19195.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:iZ,updatedAt:iZ}}}}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:134,__component:bB,audioUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioBigDataAnalyticsExecutiveSummary.mp3"},{id:5671,__component:d,text:"Big data analytics is the process of analyzing large amounts of data to gain insights and make better decisions. The term *big data* refers to data sets too large to fit on a single computer. However, the term has implications beyond size; usually it also refers to data gathered continuously, from multiple sources. Analyzing big data thus requires software that can handle large data sets distributed among multiple computers, and an organized, professional workflow."},{id:1442,__component:w,title:i_,type:x,caption:a,image:{data:{id:ib,attributes:{name:i$,alternativeText:a,caption:a,width:H,height:ja,formats:a,hash:jb,ext:r,mime:s,size:jc,url:jd,previewUrl:a,provider:j,provider_metadata:a,createdAt:hs,updatedAt:hs}}}},{id:2991,__component:f,text:"How it works",slug:"how-it-works"},{id:5676,__component:d,text:"Any data science team must understand its organizations needs and have a clear goal in mind for a given project. Data can be analyzed in a few different ways, and which is preferable depends on the objective. \n"},{id:910,__component:aM,items:"- *Descriptive analytics* summarizes data in a way that makes it easier to understand.\n- *Predictive analytics* analyzes data to predict future trends.\n- *Prescriptive analytics* automatically suggests the optimum course of action based on data."},{id:5674,__component:d,text:"\nThere are many kinds of data that a team can collect and analyze."},{id:911,__component:aM,items:"- *Structured* data is information that can be put into the columns and rows of a table, such as a list of usernames and passwords.\n- *Unstructured* data is information that cant be easily stored in a table, such as text and videos. \n- *Semi-structured* data has some structured and some unstructured parts, such as emails, which all have a date, sender, and recipient but can contain many kinds of files."},{id:1443,__component:w,title:"BDA 111 Article Image Data Types",type:x,caption:a,image:{data:{id:je,attributes:{name:"BD_111_collectig_data_1b39dae5db.svg",alternativeText:a,caption:a,width:H,height:381,formats:a,hash:"BD_111_collectig_data_1b39dae5db_478858a09c",ext:r,mime:s,size:77.82,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_111_collectig_data_1b39dae5db_478858a09c.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jf,updatedAt:jf}}}},{id:5672,__component:d,text:"Big data sets often consist largely of unstructured and semi-structured data. Storing and analyzing these kinds of data requires newer software and methods.\n\nRegardless of an organizations goal, the kind of analysis it wishes to do, and the type of data it has, it must collect, *clean*, and *transform* that data. Cleaning consists of removing inaccurate data, duplicates, and minor issues like typos. Transforming means putting the data in a format thats relatively easy for computers to analyzefor example, by standardizing data from multiple sources or combining different types of data. These data processing steps often must be done manually, and typically take up the majority of a data analysts time.\n\nOnce the data is collected, cleaned, and transformed, it can be analyzed. Because big data sets are too large to fit on a single computer, big data analytics requires getting multiple computers to work on the same problem in parallel. Essentially, the analysis is split into separate tasks that each computer performs on its own, on a portion of the data, in such a way that all the computers results can be combined for a complete analysis of the whole data set.\n\nBig data analytics often makes use of simple statistical tests, such as identifying trend lines, identifying clusters of similarities, and testing for correlations between variables. These statistical methods can yield important insights when applied to unprecedented amounts of data. To accomplish some goals, however, big data analytics requires more complicated algorithms, which may be referred to as *artificial intelligence*.\n"},{id:2993,__component:f,text:bk,slug:"benefits"},{id:912,__component:aM,items:"- Big data analytics can help organizations better understand their customers.\n- Big data analytics can guide improvements in products and services.\n- Big data analytics can help organizations manage risk.\n- Big data analytics can help organizations operate more efficiently."},{id:2990,__component:f,text:bl,slug:"limitations"},{id:909,__component:aM,items:"- Data is difficult and expensive to organize.\n- Data is expensive to store.\n- Data alone is unhelpful.\n- Data analytics can reflect and encourage bias.\n- Data collection threatens privacy.\n- Organizations must secure the data they collect."},{id:2992,__component:f,text:"Common applications",slug:"common-applications"},{id:1549,__component:v,text:jg},{id:5675,__component:d,text:"Smartphone app Hopper helps users get the best rates on air travel and hotels by tracking the fluctuating prices in the market, predicting when rates will be at their lowest, and advising the user when to buy."},{id:1548,__component:v,text:jh},{id:5673,__component:d,text:"Amazon uses large amounts of data on shopping behavior to recommend products to users. It tracks what users have viewed, bought, rated, or reviewed to automatically suggest products that a user is likely to want to buy next."},{id:1550,__component:v,text:ji},{id:5677,__component:d,text:"Concrete construction supplies company Dayton Superior uses big data analytics to automatically suggest prices for its services around the world. The companys pricing optimization system takes into account data from global markets and from Dayton Superiors own sales history and domain expertise to set fair prices quickly. This helps make the company competitive in unfamiliar markets."},{id:1551,__component:v,text:jj},{id:5678,__component:d,text:"UPS uses big data analytics to determine routes for delivery trucks, which can make over a hundred deliveries per day. To automatically create the optimum route for each truck, its software takes into account package weights, shapes, and sizes, as well as up-to-date information on road conditions and historical trends in demand and shipping capacity."},{id:2994,__component:f,text:"Common misconceptions",slug:"common-misconceptions"},{id:5679,__component:d,text:"#### Data analytics can answer every question\n\n*The reality: Data analytics can help in answering narrow, specific questions. It is not as helpful for broad or abstract questions.*"},{id:1444,__component:w,title:hc,type:x,caption:hd,image:{data:{id:he,attributes:{name:hN,alternativeText:a,caption:a,width:H,height:aQ,formats:a,hash:hO,ext:r,mime:s,size:hP,url:hf,previewUrl:a,provider:j,provider_metadata:a,createdAt:gU,updatedAt:gU}}}},{id:5680,__component:d,text:"#### The more data the better\n\n*The reality: A small amount of good data, paired with an effective process, can yield better insights than a lot of data managed badly.*\n\n#### Analyzing big data requires building custom tools in-house\n\n*The reality: Many forms of data analytics can be done with off-the-shelf products.*\n\n#### Big data analytics is only for large organizations\n\n*The reality: Organizations of all sizes can usually access enough data for analytics, and learn from it.*\n"},{id:1552,__component:v,text:gL},{id:913,__component:aM,items:"- [How big data analytics works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-works-bd)"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:155,__component:bB,audioUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioHowBigDataAnalyticsWorks.mp3"},{id:3305,__component:f,text:"What is big data analytics?",slug:"what-is-big-data-analytics"},{id:6253,__component:d,text:"Big data analytics is the process of analyzing large amounts of data to gain insights, which an organization typically uses to make better decisions. Recent technological developments make it possible to collect information on an unprecedented scale. As use of the internet has risen, more data has been generated about how people shop, what they read, what they watch, and what they post on social media. Meanwhile, more and more sensors around the world record information about their environment, such as [tracking devices](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-the-iot-is-used-today-in-industry) on shipping containers that collect information on location, temperature, and humidity in transit. Its difficult even to estimate how much digital data there is in the world; market research firm IDC [estimates](https:\u002F\u002Fwww.idc.com\u002Fgetdoc.jsp?containerId=prUS46286020) that 59 zettabytes of data was collected in 2020, roughly enough to fill a trillion smartphones. Data collection is still increasing rapidly; IDC estimates that over the next five years, three times as much data will be gathered as in the past five."},{id:1672,__component:w,title:i_,type:x,caption:a,image:{data:{id:ib,attributes:{name:i$,alternativeText:a,caption:a,width:H,height:ja,formats:a,hash:jb,ext:r,mime:s,size:jc,url:jd,previewUrl:a,provider:j,provider_metadata:a,createdAt:hs,updatedAt:hs}}}},{id:6252,__component:d,text:"The term *big data* refers to these massive data sets collected with modern technology. Theres no official threshold for how much data a set must include for it to qualify as big. Abel Sanchez, executive director of MITs Geospatial Data Center, explains that most experts use the term to refer to a data set thats too big to fit on one computer. Big data sets are distributed across multiple computers.\n\nBut *big data* has connotations beyond size. It also implies data gathered quickly and continuously, from many sources and in many different formats. The Three Vs, a term coined by research company Gartner, names the attributes of a big data set that most experts regard as significant: "},{id:1009,__component:aM,items:"- __Volume.__ The main characteristic of big data is that theres a lot of it. A major part of big data analysis is figuring out how to store, access, and analyze data efficiently when its distributed across multiple computers.\n- __Variety.__ Typically, a big data set will come from several sources and include different types of data, such as text, images, or video files. For instance, one could perform big data analysis on large amounts of written text, scanning social media posts to see how people feel about a particular product.\n- __Velocity.__ Big data analytics tools need to keep up with the high speed at which massive quantities of data are generated in order to provide timely, accurate, and useful analysis. "},{id:6254,__component:d,text:"The popularity of this framework has led some to call additional Vs significant attributes. Analysts and vendors may discuss the importance of the *veracity* and *validity* of big data sets, or how big data analytics involves data *visualization* to maximize *value*.\n\nThe analysis of a big data set follows a series of steps. The name given to each step varies depending on the analyst and organization, but the substance and sequence of them is always similar: plan, collect, process, analyze, and communicate results.\n"},{id:3306,__component:f,text:"Planning",slug:"planning"},{id:6256,__component:d,text:"During the first phase of any big data project, a data analytics team seeks to understand its organizations goals for the project, defining the problem it is solving and determining the best method of analysis. The team must also decide what data sources it will need (and can obtain) for the project. Often, a data analytics team will include *exploratory data analysis* in their planning stage: taking a small sample of data and running quick statistical analyses on it to figure out what kind of full-scale analysis would be most appropriate.\n\nIn general, there are three types of analysis that can be run on data:\n"},{id:1008,__component:aM,items:"- __Descriptive analytics.__ Summarizing existing data to allow people to understand it. For instance, a retail company might track its sales, combining data from many stores into a single report. Decision-makers can look at descriptive analysis to better understand their organization and its environment. Descriptive analysis can also highlight opportunities for further analytics.\n- __Predictive analytics.__ Using historical data to predict future trends. For example, data scientists at a shipping company could analyze data on how past trips have been affected by fuel prices, weather conditions, and maintenance needs to predict transit times and costs.\n- __Prescriptive analytics.__ Using data to directly guide decision-making. Prescriptive analysis uses data to try to optimize results. For example, an airline could automatically adjust the price of plane tickets based on fluctuating demand, finding the price point that maximizes profits. Online shopping services use prescriptive analytics to recommend new purchases, taking into account a users shopping history and demographics, recent purchases by other users, or the date and time of day to suggest the most likely products for the user to buy next.\n"},{id:3307,__component:f,text:"Data collection",slug:"data-collection"},{id:6255,__component:d,text:"After deciding on its goals, an analytics team must collect the relevant data. There are three types of data: *structured*, *unstructured*, and *semi-structured*."},{id:1727,__component:v,text:jk},{id:6258,__component:d,text:"Structured data fits into the columns and rows of a table. For instance, a retail company might have a database of all of its products, with each one having a name, category, identification number, and price. The techniques for analyzing structured data are well established. There is a standard computer language for storing and interacting with this data, *SQL*, short for Structured Query Language, which was developed in the 1970s."},{id:1728,__component:v,text:jl},{id:6259,__component:d,text:"Unstructured data refers to information that cant easily be stored in a table. This can include some geospatial data; video and sound files; large amounts of text, such as the contents of a book; and information about the connections in a social network. Estimates vary, but experts generally say around 80% of data in the world is unstructuredmeaning that big data analytics typically involves seeking insights in large amounts of unstructured data.\n\nTo store unstructured data, data scientists use many kinds of databases, called NoSQL databases because they do not rely on the SQL language. (This originally meant non-SQL, but some vendors now use the term to mean not only SQL, because many of these databases can also be interacted with using the SQL language). There are many types of NoSQL databases: Some represent data as a library of documents, for example, others as a graph of connected points."},{id:1726,__component:v,text:"Semi-structured data"},{id:6260,__component:d,text:"Semi-structured data is data that has some structured and some unstructured parts. For instance, the contents of an email inbox are semi-structured data. Each email has a date, sender, subject line, and contents, all of which is structured data that can be stored in a table. But the contents of an email may be long, complicated, and full of links and attachments, making them unstructured data."},{id:1675,__component:w,title:"BDA 101 Article Image Types of Data",type:x,caption:a,image:{data:{id:1435,attributes:{name:"BD_101_02_collecting_data_v4_88b06e7472.svg",alternativeText:a,caption:a,width:H,height:443,formats:a,hash:"BD_101_02_collecting_data_v4_88b06e7472_d00a02ab00",ext:r,mime:s,size:118.18,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_101_02_collecting_data_v4_88b06e7472_d00a02ab00.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jm,updatedAt:jm}}}},{id:3308,__component:f,text:"Data processing",slug:"data-processing"},{id:6257,__component:d,text:"Once data is collected but before it is analyzed, it must be *cleaned*, which means removing erroneous data and duplicates and identifying and correcting minor issues like typos. Cleaning can also include removing sensitive personal information, if necessary.\n\nData must also be *transformed*, altered to make it easier for a computer program to analyze. For instance, data from multiple sources must be put in the same format, or standardized. Different kinds of data about a single phenomenon can be linked. For example, to analyze how weather affects retail shopping, weather and sales data would both have to be tagged with times and locations, so the data could be combined. Data scientists estimate that these processing steps [take up around 60%](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Fgilpress\u002F2016\u002F03\u002F23\u002Fdata-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says\u002F?sh=722785c86f63) of their time."},{id:1673,__component:w,title:"BDA 101 Article Image Data Transforming Cleaning",type:x,caption:a,image:{data:{id:1437,attributes:{name:"BD_101_03_cleaning_data_v3_da87d5ef50.gif",alternativeText:a,caption:a,width:jn,height:3644,formats:{large:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7.gif",hash:"large_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7",mime:bu,name:"large_BD_101_03_cleaning_data_v3_da87d5ef50.gif",path:a,size:83.43,width:686,height:P},small:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7.gif",hash:"small_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7",mime:bu,name:"small_BD_101_03_cleaning_data_v3_da87d5ef50.gif",path:a,size:34.76,width:343,height:y},medium:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7.gif",hash:"medium_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7",mime:bu,name:"medium_BD_101_03_cleaning_data_v3_da87d5ef50.gif",path:a,size:59.56,width:515,height:B},thumbnail:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7.gif",hash:"thumbnail_BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7",mime:bu,name:"thumbnail_BD_101_03_cleaning_data_v3_da87d5ef50.gif",path:a,size:6.9,width:107,height:z}},hash:"BD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7",ext:bt,mime:bu,size:1096.37,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_101_03_cleaning_data_v3_da87d5ef50_caeeee27f7.gif",previewUrl:a,provider:j,provider_metadata:a,createdAt:jo,updatedAt:jo}}}},{id:6261,__component:d,text:"There are two models of when to perform data processing, each requiring the same steps, but in a different order."},{id:1732,__component:v,text:"Extract, Load, and Transform (ELT)"},{id:6265,__component:d,text:"In an ELT system, data is first extracted from its source (such as a sensor, a program that monitors web traffic, or a database) and then loaded into a repository of raw, unprocessed data. Due to the size of big data sets, this repository always needs to be distributed across several computers. A catchall repository for raw data that has not yet been processed is known as a *data lake*. Analyzing the data in a data lake is a little like fishing: You look around for interesting data, pull it out, and see if your analysis yields anything of value. Only when a data scientist actually intends to analyze a set of data does it need to be cleaned and transformed. This makes ELT systems comparatively fast."},{id:1730,__component:v,text:"Extract, Transform, and Load (ETL)"},{id:6262,__component:d,text:"In an ETL system, data is extracted *and* transformed before it is loaded into an organizations repository; all data in the repository is already cleaned and organized. In this scenario, the repository is called a *data warehouse*. To analyze data from a data warehouse, a team can easily access the data it needs and start analyzing it right away.\n\nETL systems can be more reliable, creating a uniform, easy-to-analyze structure for an organizations data. They can also be better when handling sensitive data that may have legal restrictions on its use, as they ensure all data has been processed before its stored."},{id:1676,__component:w,title:"BDA 101 Article Image Data Processing ELT ETL",type:x,caption:a,image:{data:{id:1436,attributes:{name:"BD_101_04_etl_elt_v5_d91ff09323.svg",alternativeText:a,caption:a,width:H,height:jp,formats:a,hash:"BD_101_04_etl_elt_v5_d91ff09323_0179f03d0a",ext:r,mime:s,size:174.38,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_101_04_etl_elt_v5_d91ff09323_0179f03d0a.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jq,updatedAt:jq}}}},{id:3309,__component:f,text:"Data analysis",slug:"data-analysis"},{id:6263,__component:d,text:"Once data is collected and processed, it can be analyzed. This step is often an iterative process: Analysis may reveal that more data or more processing is needed, returning the analyst to earlier steps."},{id:1729,__component:v,text:"Distributed computing"},{id:6264,__component:d,text:"Because big data sets are too large to fit on a single computer, big data analytics relies on *distributed computing*: getting multiple computers to work on the same problem, in parallel. An organization can store data on its own network of computers or on remote servers in a data center operated by a professional provider. (These remote data centers are often called the cloud. To learn more, see [Cloud Computing](https:\u002F\u002Fhorizonapp.mit.edu\u002Ftopic\u002Fcloud-computing).)\n\nThe current era of big data analysis was [kick-started](https:\u002F\u002Fwww.wired.com\u002F2011\u002F10\u002Fhow-yahoo-spawned-hadoop\u002F) in 2007 with the release of [Hadoop](https:\u002F\u002Fhadoop.apache.org\u002F), a free collection of software that made distributed computing easier. Hadoop includes a program that splits large data sets into smaller chunks that can be distributed across many computers. More important, it also includes a program that can convert a statistical analysis into a smaller set of instructions for each computer in a distributed computing network. Each computer follows its instructions and analyzes only the data its currently storing. Then the computers results are pieced together to create a complete analysis of the whole data set.\n\nThe software that makes this possible is complicated. Today, other distributed computing software collections are available. Hadoop is still widely used, though in recent years [Spark](https:\u002F\u002Fspark.apache.org\u002F) has become increasingly popular. But all distributed computing relies on a simple idea: To analyze a big data set, you dont need a supercomputer capable of analyzing it all; you only need a way of dividing the data and the analysis among many computers.\n"},{id:1731,__component:v,text:"Statistical methods"},{id:6267,__component:d,text:"The statistical methods used in big data analytics arent always mathematically sophisticated. If a retail company has collected a lot of data on its users ages, locations, and other personal characteristics as well as their purchase histories, a simple *clustering* algorithm can find groups of similar people in the data. Predictions of a specific customers future purchases could then be based on what the customers most similar to them have bought. Even finding the trend line that best fits a graph of points, a basic statistical analysis method called *linear regression*, can provide valuable insights into a large data set. For instance, a shipping company could find a trend connecting the price of fuel to its total shipping costs, enabling better prediction of future costs. There are also many statistical tests that find associations between variables. When performing diagnostic analytics, contrasting correlations among different data sets can reveal if one factor is causing another."},{id:1674,__component:w,title:"BDA 101 Article Image Statistical Analysis",type:x,caption:a,image:{data:{id:1434,attributes:{name:"BD_101_05_statistical_method_v5_ddb489d97b.svg",alternativeText:a,caption:a,width:H,height:518,formats:a,hash:"BD_101_05_statistical_method_v5_ddb489d97b_f212be150f",ext:r,mime:s,size:68.89,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_101_05_statistical_method_v5_ddb489d97b_f212be150f.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jr,updatedAt:jr}}}},{id:1733,__component:v,text:"Artificial intelligence"},{id:6266,__component:d,text:"Some big data analytics does use more sophisticated algorithms. For instance, Twitter decides the order in which to put tweets on a given users timeline based on huge amounts of data about past user behavior. This is sent to an [algorithm](https:\u002F\u002Fblog.twitter.com\u002Fengineering\u002Fen_us\u002Ftopics\u002Finsights\u002F2017\u002Fusing-deep-learning-at-scale-in-twitters-timelines.html) that determines which tweets the user will find most engaging. Sophisticated cutting-edge techniques to make automated decisions or predictions like these are often called *artificial intelligence*. To learn more, see [Artificial Intelligence](https:\u002F\u002Fhorizonapp.mit.edu\u002Ftopic\u002Fartificial-intelligence).\n\nSome data analysis projects could equally well be referred to as big data analytics or artificial intelligence. Which term is used depends on whether the organization prefers to emphasize the amount of data analyzed or the sophisticated techniques used for analysis."},{id:3310,__component:f,text:"Communicating results",slug:"communicating-results"},{id:6269,__component:d,text:"A crucial, final responsibility for any data analytics team is to translate mathematical results into clear, actionable insights for its organization. Data scientists often use visualizations to convey information. These can be as simple as a scatter plot or line graph or be interactive displays made with specialized software."},{id:1677,__component:w,title:"BDA 101 Article Image Dashboard",type:x,caption:a,image:{data:{id:1433,attributes:{name:"BD_101_06_dashboard_results_v3_cefb9df560.svg",alternativeText:a,caption:a,width:H,height:415,formats:a,hash:"BD_101_06_dashboard_results_v3_cefb9df560_000b0ac025",ext:r,mime:s,size:39.78,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_101_06_dashboard_results_v3_cefb9df560_000b0ac025.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:js,updatedAt:js}}}},{id:6268,__component:d,text:"Communicating results successfully requires more than just producing reports after an analysis. The goal for most organizations is to operationalize their big data analytics: to incorporate big data analytics directly into their decision-making process. The results of a big data analytics project can directly guide decision-making or prompt testing of a hypothesis by making a change and analyzing the data the test generates.\n\nA major reason that big data analytics projects fail is not poor analysis but [a lack of integration](https:\u002F\u002Fsloanreview.mit.edu\u002Farticle\u002Fwhy-so-many-data-science-projects-fail-to-deliver\u002F) with the rest of the organization. For big data analytics to be effective, it cant just be an occasional task performed by a specialized team. Executives and decision-makers must be aware of the capabilities of big data analytics and constantly communicate their needs to their data scientists. Data scientists, in turn, must be clear, relevant, and timely when communicating their findings to executives."},{id:1734,__component:v,text:gL},{id:1010,__component:aM,items:"- [Benefits of big data analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-bd)\n"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:154,__component:bB,audioUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioBenefitsofBigDataAnalytics.mp3"},{id:3301,__component:f,text:"Big data analytics can help organizations better understand their customers",slug:"better-understand-their-customers"},{id:6246,__component:d,text:"Analyzing data about customer purchases can give companies the ability to understand their customers desires and shopping patterns, a crucial step for several business processes, including improving advertisements, recommendations, and sales. According to a 2018 [study](https:\u002F\u002Fwww.accenture.com\u002F_acnmedia\u002FPDF-77\u002FAccenture-Pulse-Survey.pdf) from the business consulting firm Accenture, 91% of customers are more likely to shop with companies that provide them with offers and recommendations they find relevant.\n\nFor example, Walmart collects data on what, where, and when its customers buy, and uses it to gain insights about their interests and behaviors and to inform its decisions, such as what products to stock or discontinue and when. In 2017, the company created a data hub hosted in Missouri for storing and analyzing the data it receives from its stores, its website, and many other sources, including social media sites such as Twitter and Facebook, weather reports and listings of local events. Although some of these data sources may seem irrelevant, they can help Walmart capitalize on trends quickly. For example, if Walmart realizes that a particular product is trending on social media, it can move quickly to stock that product on its shelves. Similarly, weather data can help Walmart understand when to stock up on flashlights because a storm is rolling in."},{id:1669,__component:w,title:"BD 102a Article Image How Much Data",type:x,caption:"Walmart collects a huge volume of data, including local weather and events,  in order to stay on top of trends that affect its locations and business. Although Walmart does not use all that data every day, collecting it means having it on hand, much like a reference library, to answer questions or influence business decisions as they arise.",image:{data:{id:1430,attributes:{name:"BD_102a_01_Walmart_infographic_rev_580c29e4b4.svg",alternativeText:a,caption:a,width:H,height:457,formats:a,hash:"BD_102a_01_Walmart_infographic_rev_580c29e4b4_1abd2e8c15",ext:r,mime:s,size:171.47,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102a_01_Walmart_infographic_rev_580c29e4b4_1abd2e8c15.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jt,updatedAt:jt}}}},{id:6249,__component:d,text:"Every hour, the company is capable of analyzing a huge amount of that combined dataabout 2.5 petabyes. By gathering it in one place, Walmart can make specific observations at a scale that would be impossible otherwise. For example, during one Halloween, analysts at Walmarts data center [reportedly](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Fbernardmarr\u002F2017\u002F01\u002F23\u002Freally-big-data-at-walmart-real-time-insights-from-their-40-petabyte-data-cloud\u002F?sh=488413946c10) saw that a certain type of cookie was selling well at all but two locations. The analysts phoned the managers of the two stores, who realized theyd forgotten to stock it on the shelves. The managers did so and soon saw sales. Without immediate access to large-scale data, such observations could take weeks or months. If you cant get insights until youve analyzed your sales for a week or a month, then youve lost sales within that time, Naveen Peddamail, a senior statistical analyst at Walmart, [said in 2017](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Fbernardmarr\u002F2017\u002F01\u002F23\u002Freally-big-data-at-walmart-real-time-insights-from-their-40-petabyte-data-cloud\u002F?sh=1c81bcb96c10). If you can cut down that time from two or three weeks to 20 or 30 minutes, then that saves a lot of money for Walmart and stopped us losing sales. In the first quarter of 2019, in-store sales grew by [3.4%](https:\u002F\u002Ftechcrunch.com\u002F2019\u002F05\u002F16\u002Fwalmart-beats-on-earnings-in-q1-with-u-s-e-commerce-up-by-37-percent\u002F), the best first quarter that Walmart has had in nine years.\n\nA year after launching its main data hub, in 2018 Walmart also redesigned its website, using insights from big data to offer customers more personalized online shopping experiences, as *TechCrunch* then [reported](https:\u002F\u002Ftechcrunch.com\u002F2018\u002F04\u002F16\u002Fwalmart-coms-big-redesign-will-roll-out-in-may\u002F). Its new site analyzes each customers past purchases and browsing history and offers a custom list of recommended products. For example, a Walmart customer searching for a dining room table might also get recommendations for a sideboard and dining room lamps. The website also offers an easy reorder feature for frequently purchased items. You want the site to make shopping faster and easier, and when youre showing those items that customers are trending toward, youre actually making a faster shopping journey for them, which is ultimately the goal, Marc Lore, CEO of Walmart eCommerce U.S., [told](https:\u002F\u002Ftechcrunch.com\u002F2018\u002F04\u002F16\u002Fwalmart-coms-big-redesign-will-roll-out-in-may\u002F) *TechCrunch* in 2018. The new site also makes product recommendations based on broader population data. For example, it collects a customers location and shows them popular (trending) products in their area. When a local sports team wins a game, the website shows team paraphernalia. A year after Walmart launched the revamped website, its online sales grew by 37%.\n"},{id:1670,__component:w,title:"BD 102a Article Image Walmart Website",type:x,caption:"Walmart's new website uses personalized recommendations from data analysis and a new design to create a better experience for shoppers.  Walmart 2018. Used with permission.",image:{data:{id:1432,attributes:{name:"BD_102a_02_Walmart_homepage_v3_2c3f54b5d4.gif",alternativeText:a,caption:a,width:ju,height:1460,formats:{large:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4.gif",hash:"large_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4",mime:bu,name:"large_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4.gif",path:a,size:189.25,width:P,height:851},small:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4.gif",hash:"small_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4",mime:bu,name:"small_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4.gif",path:a,size:52.56,width:y,height:425},medium:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4.gif",hash:"medium_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4",mime:bu,name:"medium_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4.gif",path:a,size:108.84,width:B,height:jv},thumbnail:{ext:bt,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4.gif",hash:"thumbnail_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4",mime:bu,name:"thumbnail_BD_102a_02_Walmart_homepage_v3_2c3f54b5d4.gif",path:a,size:10.23,width:183,height:z}},hash:"BD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4",ext:bt,mime:bu,size:1053.3,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102a_02_Walmart_homepage_v3_2c3f54b5d4_131b2906f4.gif",previewUrl:a,provider:j,provider_metadata:a,createdAt:jw,updatedAt:jw}}}},{id:3304,__component:f,text:"Big data analytics can guide improvements in products and services",slug:"big-data-analytics-can-help-organizations-build-better-products-and-services"},{id:6247,__component:d,text:"The potential benefits from analyzing data go beyond understanding customers to improve sales. The insights an organization gains can also allow it to understand itself better, and to identify opportunities to improve its products and services.\n\nFor example, universities can use the insights from data to reduce student dropout rates. From 2011 to 2019, Georgia State University used the insights it gained from analyzing student data to increase its graduation rate from [48%](https:\u002F\u002Fstrategic.gsu.edu\u002Faccomplishments\u002Fgoal-1-accomplishments\u002F) to [69%](https:\u002F\u002Fnews.uga.edu\u002Fuga-completion-rates-2019\u002F), almost 30 points above the [national average](https:\u002F\u002Fwww.studentclearinghouse.org\u002Fnscblog\u002Flatest-state-level-completion-rates-includes-race-and-ethnicity-for-first-time-students-at-4-year-public-institutions\u002F). Timothy M. Renick, a vice president at Georgia State who is in charge of the project, [said](https:\u002F\u002Fwww.thirdway.org\u002Finterview\u002Finterviews-with-influencers-dr-tim-renick) in March 2020: I think what makes Georgia States approach promising for other institutions is that were at the front end of a technological and data revolution that is allowing us, for low cost, to deliver daily personalized attention to students.\n\nGeorgia State used its database of over 140,000 student records and 2.5 million grades to identify [several hundred](https:\u002F\u002Fwww.politico.com\u002Fagenda\u002Fstory\u002F2019\u002F01\u002F16\u002Ftracking-student-data-graduation-000868\u002F) risk factors correlated with dropping out, such as low class attendance and not taking a course required for a major in the recommended time frame. Through its analysis, the university learned that no single variable guaranteed a student would drop out, but many minor variables could throw a student off track. Its data showed that a fifth of freshmen dropped out before they even arrived on campus. Based on these insights, it created a virtual assistant that could text students reminders such as enrollment deadlines and could also answer  incoming students questions about college life. The year after the service launched, the number of students who dropped out before arriving at school decreased by more than [20 percent](https:\u002F\u002Fwww.admithub.com\u002Fcase-study\u002Fhow-georgia-state-university-supports-every-student-with-personalized-text-messaging\u002F)."},{id:6248,__component:d,text:"Georgia State also learned that other variables correlate with a students chances of graduating once they have arrived on campus. For example, earning a C in an introductory political science course decreased the likelihood that a political science major will graduate to just [25%](https:\u002F\u002Fwww.npr.org\u002Fsections\u002Fed\u002F2016\u002F10\u002F30\u002F499200614\u002Fhow-one-university-used-big-data-to-boost-graduation-rates). To improve students likelihood of graduating, Georgia State hired more advisors, increasing its advising staff by over [fivefold](https:\u002F\u002Fwww.politico.com\u002Fagenda\u002Fstory\u002F2019\u002F01\u002F16\u002Ftracking-student-data-graduation-000868\u002F). The advisors received a dashboard that assigns students a risk level of red, yellow, or green based on data analysis. If, for example, a student does poorly on the first few quizzes of a course, the dashboard notifies advisors, who contact the student for a conversation about possible solutions such as tutoring or summer school. This approach also helps students graduate sooner, reducing tuition costs. Prior to Georgia States big data project, students took an average of [20 wasted credit hours](https:\u002F\u002Fwww.politico.com\u002Fagenda\u002Fstory\u002F2019\u002F01\u002F16\u002Ftracking-student-data-graduation-000868\u002F)time that did not count toward graduation. Afterward, students graduated a semester earlier on average.\n\nBig data analysis can also make medical organizations more effective, allowing providers to make more informed choices, which can be the difference between life and death. For example, at Lucile Packard Childrens Hospital in Palo Alto, California, lessons from big data help to reduce a common danger at many American hospitals: infections in a patients central line, the long tube that goes into a vein near or inside the heart. An estimated 248,000 central-line infections occur each year in the U.S., with mortality rates of up to a [quarter](https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fbooks\u002FNBK430891\u002F). Even when they are not fatal, central-line infections are expensive.\n\nBetween 2015 and 2018, Lucile Packard teamed up with Stanford researchers to reduce its central-line infection rate, which was higher than the national average: [2.2 compared with 1.5 per 1,000 lines](https:\u002F\u002Fjamanetwork.com\u002Fjournals\u002Fjama\u002Ffullarticle\u002F2758589). The researchers gathered departmental data on the time of day central lines were inserted and removed, the reason for doing so, and infection rates. The researchers also gathered data on the nurses and doctors adherence to sanitary recommendations such as checking if the patient is hooked to unnecessary lines and having the patient bathe in germicide. We were trying to get a better data-driven picture of the current state of the hospital, David Scheinker, the director of Systems Utilization Research for Stanford Medicine group and a lead researcher on the project, tells MIT Horizon.\n\nBy analyzing this data, researchers were able to see which departments had the highest infection rates, as well as which departments were following the sanitizing procedures and which were skipping steps. Skipping just one or two sanitizing steps was linked with a higher infection rate. We developed state-of-the-art visualizations of where there are opportunities to improve, Scheinker says, so the hospital leaders in charge of infection control could target their efforts. Hospital administrators were then able to have discussions with the relevant departments to ensure that everyone followed the sanitizing procedures. In some cases, feedback from nurses and doctors in these discussions led to the hospital creating better, more customized sanitization procedures for specific departments. By 2018, the infection rate at Lucile Packard had fallen to 1.1 per 1,000 lines, below the national average and half its initial rate.  \n"},{id:3302,__component:f,text:"Big data analytics can help organizations manage risk",slug:"big-data-analytics-can-help-companies-manage-risk"},{id:6251,__component:d,text:"Data can help an organization manage its risk, including determining whom to lend to and detecting fraud. For example, data analysis can help banks determine whether a customer is likely to repay a loan, especially in the tricky case of assessing whether to lend to someone with a limited credit history. In this scenario, if banks err on the side of risk, they open themselves up to potential defaults. If they err on the side of safety, they might miss potential business and profits, and the customers lose an opportunity.\n\nBanks struggle with small to medium businesses in particular: A 2017 global study showed that these businesses made up [74% of rejected loans](https:\u002F\u002Fwww.adb.org\u002Fsites\u002Fdefault\u002Ffiles\u002Fpublication\u002F359631\u002Fadb-briefs-83.pdf). But data analysis can help a bank decide whom it should make loans to in the absence of traditional credit history. For example, the San Francisco company Flowcast analyzes many data sources to evaluate the creditworthiness of companies that dont have a credit score. A traditional credit score considers a small businesss overall debt, available credit, and late or missed payments. Flowcast considers more data: the businesss operations, invoices, shipment data, payment history, and product information. Based on this data, in a 2018 test of its model with a major global bank and 50,000 small and medium-sized businesses across three countries, Flowcast predicted with [85%](https:\u002F\u002Fflowcast.ai\u002FFlowcast%20whitepaper%20-%20Big%20Data%20Smart%20Credit.pdf) accuracy whether a business would default on a loan. Flowcast counts Singapores DBS bank and Standard Chartered among its clients, and in 2019 partnered with the Singapore branch of ING, an international bank. Benoit Legrand, chief innovation officer at ING, [commented](https:\u002F\u002Fwww.prnewswire.com\u002Fnews-releases\u002Fflowcast-raises-usd-3-million-in-series-a-funding-to-unlock-credit-decisioning-at-scale-300923818.html) in 2019 on the importance of big data analysis for ING: This investment and partnership with Flowcast is  to empower our clients and ourselves in global trade operations.\n\nIn addition to assessing creditworthiness, big data analysis can also help companies identify and stop fraud. As recently [as 2013](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-ai-is-used-today-in-industry), credit card companies relied on customers to self-report suspicious activity on their cards. However, big data analysis has given credit card companies the ability to automatically identify a suspicious transaction. A typical credit card purchase creates a lot of data: time of purchase, location, amount, item, etc. This allows credit card companies to understand a customers typical purchases and identify something that looks suspicious. For example, Visa identifies potentially fraudulent transactions by looking at data such as the transaction amount and the time of day and comparing this with a customers usual spending habits. Visa believes its data analysis is crucial for identifying fraud and retaining customer trust. Melissa McSherry, Visas global head of Data, Risk and Identity Products and Solutions, [commented](https:\u002F\u002Fusa.visa.com\u002Fabout-visa\u002Fnewsroom\u002Fpress-releases.releaseId.16421.html) in 2019, Consumers identified Visa as the most trusted company  and we believe it is due to Visas unrelenting focus on eliminating fraud and protecting the payment ecosystem. Visa doesnt disclose the raw numbers, but it estimated its data practices prevented [$25 billion](https:\u002F\u002Fusa.visa.com\u002Fabout-visa\u002Fnewsroom\u002Fpress-releases.releaseId.16421.html) worth of fraud from April 2018 to April 2019. \n"},{id:1671,__component:w,title:"BD 102a Article Image Chase",type:x,caption:"Chase Bank is one of several banks that use data analysis to spot a potentially fraudulent transaction. When Chase spots one, it sends the customer a text to ask if the activity is theirs. If not, Chase shuts down the card and sends a replacement.",image:{data:{id:1431,attributes:{name:"BD_102a_05_Chase_fraud_v8_4378542d43.png",alternativeText:a,caption:a,width:1600,height:ht,formats:{large:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4.png",hash:"large_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4",mime:p,name:"large_BD_102a_05_Chase_fraud_v8_4378542d43.png",path:a,size:174,width:P,height:B},small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4.png",hash:"small_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4",mime:p,name:"small_BD_102a_05_Chase_fraud_v8_4378542d43.png",path:a,size:59.21,width:y,height:375},medium:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4.png",hash:"medium_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4",mime:p,name:"medium_BD_102a_05_Chase_fraud_v8_4378542d43.png",path:a,size:112.41,width:B,height:jx},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4.png",hash:"thumbnail_BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4",mime:p,name:"thumbnail_BD_102a_05_Chase_fraud_v8_4378542d43.png",path:a,size:13.4,width:208,height:z}},hash:"BD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4",ext:o,mime:p,size:78.07,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102a_05_Chase_fraud_v8_4378542d43_3aa0aa53d4.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:jy,updatedAt:jy}}}},{id:3303,__component:f,text:"Big data analytics can help organizations operate more efficiently",slug:"big-data-analytics-can-help-organizations-operate-more-efficiently"},{id:6250,__component:d,text:"By analyzing data about the resources they use and business processes they rely on, organizations can reveal opportunities to become more efficient. One opportunity is in reducing waste, especially across an organization with multiple locations. \n\nFor example, the energy management company ENGIE Insight works with clients to reduce their energy use and pare down their utility bills by gathering data on how the clients are using energy and then analyzing it to make recommendations. One of its clients is the furniture retailer Havertys, which has over 100 showrooms and millions of square feet of floor space, [according](https:\u002F\u002Fengieservices.us\u002Fsuccess-stories\u002Fsuccess-story-havertys\u002F) to the two companies. When ENGIE Insights started working with Havertys in 2009, Havertys had little information about how much energy it was using because some of its buildings had no live meters measuring energy consumption. ENGIE Insights installed meters to record energy consumption every 15 minutes across Havertys locations, allowing ENGIE Insights to understand how much energy was being consumed at each location and overall.  Based on this data, ENGIE Insights was able to make informed recommendations across all Havertys locations (for example, to install LED lights) as well as for particular locations (for example, to retrofit an HVAC system). Four years after the inception of the project, Havertys lowered its energy consumption by [29%](https:\u002F\u002Fwww.havertys.com\u002Ffurniture\u002Fhavertys-furniture-environmental-responsibility). Since 2016, the company has continued to reduce its energy consumption by over [40%](https:\u002F\u002Fwww.havertys.com\u002Ffurniture\u002Fhavertys-furniture-environmental-responsibility) each year, thanks to continued data analysis. Weve focused on making our stores leaner and more productive, Havertys website [says](https:\u002F\u002Fwww.havertys.com\u002Ffurniture\u002Fhavertys-furniture-environmental-responsibility).\n\nBig data analysis can also improve efficiency by helping an organization target when to make repairs, instead of relying on routine inspections. General Electric, which was [recognized](https:\u002F\u002Fwww.ge.com\u002Fnews\u002Fpress-releases\u002Fge-renewable-energy-secures-more-2-gw-us-onshore-wind-orders-through-may-2019) in 2018 as the top manufacturer of wind turbines in the U.S., supplying 40% of the nations onshore wind power, uses data to improve its wind turbines power production. As part of a big data initiative [begun](https:\u002F\u002Fwww.ge.com\u002Fnews\u002Fpress-releases\u002Fge-launches-next-evolution-wind-energy-making-renewables-more-efficient-economic) in 2015, General Electric equipped its turbines with sensors to collect data on temperature, vibration, how much power the turbine is producing, and turbine misalignments ...  that can affect performance. The turbines send these data points to GEs big data platform. There, analysts can monitor the performance of entire wind farms or individual turbines. They can see if a turbine is producing unusually low amounts of power and send a repair person in as soon as theres a problem, which minimizes downtime. In addition, because GE creates a custom maintenance schedule to fix turbines only when needed, instead of conducting routine inspections, GE saves money. Ganesh Bell, GE Powers former chief digital officer, [commented](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Fbernardmarr\u002F2017\u002F03\u002F28\u002Fthe-amazing-way-ge-is-combining-big-data-and-electrons-to-create-the-internet-of-energy\u002F#1f4309a11806) in 2017, We have seen results like reducing unplanned downtime by 5% \\[and] reducing operations and maintenance costs by 25%and these start adding up to meaningful value."},{id:1725,__component:v,text:gL},{id:1007,__component:aM,items:"- [Limitations of big data analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Flimitations-bd)\n"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:153,__component:bB,audioUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioLimitationsofBigDataAnalytics.mp3"},{id:3300,__component:f,text:"Data is difficult and expensive to organize",slug:"data-is-difficult-and-expensive-to-organize"},{id:6236,__component:d,text:"Before computers can analyze information, it must be collected and organized in a format that computers can access and interpret, known as *machine readable*. The more data an organization has, the more difficult and expensive it can be to organize it this way.\n\nFor example, one of the simplest ways that organizations collect data is having their customers or employees fill out a form and submit it. But even this is not so simple. To become machine readable, the information on the form must often be *cleaned*. This means correcting misspellings and typos, removing or supplementing items that are incomplete (for example are those times a.m. or p.m.?), and ensuring that all numbers appear in the same format (two am and two pm will become 0200 and 1400), among other things. It also means identifying and consolidating any duplicate fields across multiple forms. "},{id:1664,__component:w,title:"Five Steps for Cleaning Data",type:x,caption:"Data cleaning is a multistep processoften expensive and time intensivethat must take place before analysis can even begin. Without data cleaning it is difficult to produce accurate insights. [Source](https:\u002F\u002Fwww.tableau.com\u002Flearn\u002Farticles\u002Fwhat-is-data-cleaning \"Data cleaning: The benefits and steps to creating and using clean data\")",image:{data:{id:1429,attributes:{name:"BD_102b_01_data_cleaning_steps_v3_e09415ddc5.svg",alternativeText:a,caption:a,width:H,height:566,formats:a,hash:"BD_102b_01_data_cleaning_steps_v3_e09415ddc5_f62a10633a",ext:r,mime:s,size:377.87,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102b_01_data_cleaning_steps_v3_e09415ddc5_f62a10633a.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jz,updatedAt:jz}}}},{id:6243,__component:d,text:"Another common way for organizations to collect data is to track shoppers purchases at checkout, especially if shoppers scan a member card to identify themselves. Over time, the shoppers profile can grow to include not just name and phone number but preferred store locations and shopping times, favored brands, and how often certain products are purchased together. (Based on such information, a store might stock those items on the same aisle.) But checkout scans from many shoppers, across many store locations, will almost certainly yield errors and inconsistencies.\n\nEvery method of data collection requires some cleaningwhether the data was collected directly or from a third party, whether it was collected manually or automatically. Working data scientists make their daily bread and butter through data collection and data cleaning, [writes data scientist](https:\u002F\u002Fhbr.org\u002F2018\u002F08\u002Fwhat-data-scientists-really-do-according-to-35-data-scientists \"What Data Scientists Really Do, According to 35 Data Scientists\") Hugo Bowne-Anderson. This work is time intensive, meaning it is costly in labor hours and machine resources, and must be done before analysis can begin.\n\nIn some circumstances, a preceding step is necessary even before data can be cleaned. When a person fills out a form or a shopper makes a purchase, the relevant information is already words or numbers, which is easy for a machine to analyze. If organizations want to collect image or sound data they may first have humans or machines attach *labels* to this information. While it is possible to analyze data without adding labels, combining those labels with other information about the data, or *metadata*, can help the company catalog and organize the data. Like cleaning, it can be laborious and costly, and also introduces new avenues for error."},{id:3295,__component:f,text:"Data is expensive to store outside the cloud",slug:"data-is-expensive-to-store-outside-the-cloud"},{id:6239,__component:d,text:"The simplest way to store data is on the memory of a single computer, but most big datasets are too large to be contained on one computer. Big data is what you cannot put in an Excel spreadsheet, [says](https:\u002F\u002Fhorizonapp.mit.edu\u002Fvideo-module\u002Fwireless-technologies-and-smart-cities\u002Fchapter\u002Fbig-data) MIT professor and director of the Institutes SENSEable City Lab Carlo Ratti. When you [have] big data, then you also need new tools. For example, in October 2020, the National Health Service [discovered](https:\u002F\u002Fwww.bbc.com\u002Fnews\u002Fuk-54422505 \"Covid: Test error 'should never have happened' - Health Secretary Hancock\") that the [master Microsoft Excel file](https:\u002F\u002Fwww.nytimes.com\u002F2020\u002F10\u002F05\u002Fworld\u002Feurope\u002Fuk-testing-johnson-hancock.html \"In U.K.s Test and Trace: Now You See em, Now You Dont\") it was using to keep track of COVID cases had reached its maximum size a week earlier. As a result, 15,841 recent cases were left out of the daily records. The week-long error led to a lag in contact tracing, which is ideally done within 48 hours of a positive test. "},{id:1668,__component:w,title:"COVID Cases in the UK Left Out of Daily Numbers",type:x,caption:"The National Health Services error in tracking COVID cases demonstrates the consequences of using inadequate tools for collecting big data. Source [[1]](https:\u002F\u002Fwww.gov.uk\u002Fgovernment\u002Fnews\u002Fphe-statement-on-delayed-reporting-of-covid-19-cases \"PHE statement on delayed reporting of COVID-19 cases\") and [[2]](https:\u002F\u002Fcoronavirus.data.gov.uk\u002Fdetails\u002Fcases \"Cases by date reported\")",image:{data:{id:1426,attributes:{name:"BD_102b_02_missing_covid_cases_v4_e523bc3376.svg",alternativeText:a,caption:a,width:H,height:420,formats:a,hash:"BD_102b_02_missing_covid_cases_v4_e523bc3376_940f4b705d",ext:r,mime:s,size:82.21,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102b_02_missing_covid_cases_v4_e523bc3376_940f4b705d.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jA,updatedAt:jA}}}},{id:6237,__component:d,text:"Organizations in need of more space can store data on multiple computers on premises or remotely via the internet. (Data that is stored remotely is often called *in the cloud*. For more on cloud computing, see [Cloud Computing](https:\u002F\u002Fhorizonapp.mit.edu\u002Ftopic\u002Fcloud-computing).) Either method presents costs, challenges, and tradeoffs.\n\nTo store data on-site, an organization must purchase high-powered computers, called *servers*, that share data or programs with other computers over a network. Servers require maintenance, upkeep, and floor space. They must be kept at consistent temperatures, well ventilated, and free from moisture. They also need a consistent, reliable power supply, so they can always be on and their data accessible. Finally, servers require security, both digital (such as firewalls) and physical (such as guards and locks).  The costs of these requirements add up.\n\nStoring data remotely requires the use of servers owned and managed by a third party. This comes with different costs for storage and access, as detailed by [Cloud Computing](https:\u002F\u002Fmitpress.mit.edu\u002Fbooks\u002Fcloud-computing \"Cloud Computing by Nayan Ruparelia, published by MIT Press\"), by Nayan Ruparelia, and requires trusting the third-party providers security. Some cloud storage companies offer low rates to keep data but higher rates to access or add data on a frequent basis."},{id:3297,__component:f,text:"Data alone is unhelpful",slug:"data-alone-is-unhelpful"},{id:6238,__component:d,text:"Data is simply a collection of informationof numbers, words, or facts expressed otherwise. Without analysis and action, companies that collect data get no benefit from it. Or errors may occur in the analysis or the company may lack a strategy to integrate insights gained from the data into a business plan. A [2021 survey of Fortune 1000](https:\u002F\u002Fc6abb8db-514c-4f5b-b5a1-fc710f1e464e.filesusr.com\u002Fugd\u002Fe5361a_76709448ddc6490981f0cbea42d51508.pdf \"New Vantage Partners 2021 Survey of Fortune 1000 Companies\") executives by NewVantage Partners found that while 95% of respondents found data management technology easy to adopt, 92% of companies still see cultural barriers, like people and processes, as the main challenge to becoming data driven. "},{id:1665,__component:w,title:"Companies Still Struggle With Using Big Data Effectively",type:x,caption:"While major companies have universally and continually invested in big data in recent years, they still face challenges around how to integrate the data they collect. For most companies, the greatest obstacle in establishing a data culture are people and processes that are slow to change. [Source](https:\u002F\u002Fc6abb8db-514c-4f5b-b5a1-fc710f1e464e.filesusr.com\u002Fugd\u002Fe5361a_76709448ddc6490981f0cbea42d51508.pdf \"New Vantage Partners Big Data and AI Executive Survey 2021\")",image:{data:{id:1427,attributes:{name:"BD_102b_03_big_data_corporate_obstacles_v3_97def11971.svg",alternativeText:a,caption:a,width:H,height:ic,formats:a,hash:"BD_102b_03_big_data_corporate_obstacles_v3_97def11971_8ffc9290f4",ext:r,mime:s,size:94.78,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102b_03_big_data_corporate_obstacles_v3_97def11971_8ffc9290f4.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jB,updatedAt:jB}}}},{id:6240,__component:d,text:"Abel Sanchez, executive director of MITs Geospatial Data Center, who specializes in helping companies navigate digital transformations, tells MIT Horizon that a successful big data analytics project depends on more than just having the right technology. Ive seen billion-dollar transformations go bust, Sanchez says. Its not just techits people, its processes, [and bringing] all of those together.\n\nRetail chain Sears struggled to combine big data analytics with effective business processes illustrating Sanchezs point. Since as long ago as 2009, Sears has collected data from customers in stores and online as part of a Shop Your Way customer loyalty program. This provides discounts to customers in exchange for personal information (such as age, gender, email and mailing addresses, and phone numbers) and data about shopping habits. Shop Your Way enabled Sears to also collect third-party information, for example from social media or web browsing, that would link a customer to their interests, hobbies, lifestyle choices, groups with which they are affiliated, and products the groups members want, like, or own.\n\nIn exchange for all this information, customers in the Shop Your Way program received points for each dollar they spent at a Sears, Kmart, or partner store. Once a customer collects enough points, they become cash back.\n\nHowever, from the start, the project stumbled. First, it slowed down Sears checkout lines: Cashiers were required to meet a monthly quota for enrollments, so signing up customers meant cashiers went from scanning [18 items a minute to five](https:\u002F\u002Fwww.businessinsider.com\u002Fsears-failing-stores-closing-edward-lampert-bankruptcy-chances-2017-1 \"Inside Sears' death spiral: How an iconic American brand has been driven to the edge of bankruptcy\"), causing frustrated customers to abandon their carts. Meanwhile, the promotions linked to the membership program cut into profit margins on every purchase. The perks, offered as an incentive to collect customer data, were so generous that repeat customers would regularly leave with all or almost all of their purchase paid for by points, as the [Wall Street Journal](https:\u002F\u002Fwww.wsj.com\u002Farticles\u002Fsears-is-in-bankruptcybut-its-lavish-rewards-program-wont-quit-11554217047 \"Sears Went Into Bankruptcybut Its Lavish Rewards Program Wont Quit\") reported.\n\nMeanwhile, the collected data was held by a division of Sears that did analytics. Other parts of the company, like marketing and retail management, had to buy [analytics internally in order to access it](https:\u002F\u002Fdigital.hbs.edu\u002Fplatform-digit\u002Fsubmission\u002Fcan-data-save-us-all-lessons-from-attempting-to-transform-sears-and-kmart-through-data-analytics\u002F \"Can Data Save Us All? Lessons from Attempting to Transform Sears and Kmart through Data & Analytics\"). This hurt relationships between the analytics division and other departments, and made it hard for company-wide collaboration and thus for the entire company to profit from the analytics.\n\nShop Your Way failed even though it did collect data largely because Sears was unable to integrate the data into an effective business process. In October 2018, just months after Sears announced a massive expansion of the Shop Your Way programs discounts, Sears declared bankruptcy.\n"},{id:3296,__component:f,text:"Data analytics can reflect and encourage bias",slug:"data-analytics-can-reflect-and-encourage-bias"},{id:6241,__component:d,text:"Data can reflect biases, and analysis can replicate themleading organizations to take discriminatory actions, even if inadvertently.\n\nBias can be introduced in data projects  by using a data set that replicates biases already found in society. For example, in 2017 Princeton researchers [revealed](https:\u002F\u002Fscience.sciencemag.org\u002Fcontent\u002F356\u002F6334\u002F183.full \"Semantics Derived Automatically from Language Corpora Contain Human-like Biases\") biases in Google Translate. The Turkish language contains a gender-neutral pronoun that covers *he*, *she*, and *it*. However, when Turkish sentences using that pronoun were dropped into Google Translate in association with various professions, Google Translate demonstrated gender bias: o bir doktor was translated as he is a doctor. *President*, *hard-working*, and *writer* were also translated as *he*. *Nurse*, *teacher*, and *lazy* were all translated as *she*. Google Translate is trained on data sets of sentences that people translated previously: If most Turkish-to-English translations pair *doctor* with males, Google Translate will carry that bias forward.\n\nWhen biased data is used in analysis, it can lead people to replicate that bias in their decision-making. In 2017, Amazon abandoned an analysis tool it built to automatically find the most talented job applicants. The tool filtered through stacks of resumes for the purpose of selecting the most qualified applicants for human review. Amazon trained the tool on resumes submitted over the past decade, noting which of those applicants it hired. However, the successful past applicants were already a biased data set. The tool routinely recommended that Amazon hire male candidates over female candidates. Though not intentionally trained to look for male resumes, the tool learned a preference for male resumes, because those belonged to the candidates that Amazon had hired historically. This bias was in the data. For example, Amazons tool learned that successful resumes did not include items like womens chess club captain or the names of womens colleges.\n\nBias in data leading to real-world harms is also seen in the world of criminal justice. For example, Florida used a data analytics tool to predict the likelihood a criminal would commit another crime. The tool was twice as likely to falsely flag black defendants as future criminals.\n\nAs [ProPublica](https:\u002F\u002Fwww.propublica.org\u002Farticle\u002Fmachine-bias-risk-assessments-in-criminal-sentencing \"Machine Bias Risk Assessments in Criminal Sentencing\") found, this tool was likely to falsely flag black defendants as future criminals at almost twice the rate as white defendants. Attorney General Eric Holder, speaking against the use of such risk assessment tools, [commented](https:\u002F\u002Fwww.propublica.org\u002Farticle\u002Fmachine-bias-risk-assessments-in-criminal-sentencing \"Machine Bias Risk Assessments in Criminal Sentencing\"): Although these measures were crafted with the best of intentions, I am concerned they inadvertently undermine our efforts to ensure individualized and equal justice."},{id:3298,__component:f,text:"Data collection threatens privacy",slug:"data-collection-threatens-privacy"},{id:6244,__component:d,text:"Many pieces of data collected about a person reveal to strangers things once kept private. That informationabout everything from online searches to purchases made to their age or number of siblings can be sensitive or  intimate. Asked by Pew in 2019 to [define privacy](https:\u002F\u002Fwww.pewresearch.org\u002Finternet\u002F2019\u002F11\u002F15\u002Fhow-americans-think-about-privacy-and-the-vulnerability-of-their-personal-data\u002F \"How Americans Think About Privacy and the Vulnerability of Their Personal Data\"), a woman said, No one knows my credit card numbers, address info, where I have been, my banking info, my health info, etc. People dont know anything about me I do not intend to share.\n\nMany people are thus uncomfortable with data collection to begin with, and are especially concerned about how both government and companies will use that data. In the same Pew survey, roughly three-quarters of adults said they benefit very little or not at all from the data that companies or the government collect about them.\n"},{id:1666,__component:w,title:"Do the Potential Risks of Data Collection Outweigh the Potential Benefits?",type:x,caption:"A 2019 Pew study of more than 4,200 Americans found that most respondents believed data collection to be riskier than it is beneficial. [Source](https:\u002F\u002Fwww.pewresearch.org\u002Finternet\u002Fwp-content\u002Fuploads\u002Fsites\u002F9\u002F2019\u002F11\u002FPew-Research-Center_PI_2019.11.15_Privacy_FINAL.pdf \"Americans and Privacy: Concerned, Confused, and Feeling Lack of Control Over Their Personal Information\")",image:{data:{id:1425,attributes:{name:"BD_102b_04_pew_data_privacy_research_v2_f2dbfccef6.svg",alternativeText:a,caption:a,width:H,height:id,formats:a,hash:"BD_102b_04_pew_data_privacy_research_v2_f2dbfccef6_d7ed9d5aba",ext:r,mime:s,size:43,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102b_04_pew_data_privacy_research_v2_f2dbfccef6_d7ed9d5aba.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jC,updatedAt:jC}}}},{id:6245,__component:d,text:"To protect users, some organizations limit the amount of data they keep. For example, the social media app Snapchat was built on the premise that all posts are automatically deleted once theyve been read, or after a certain amount of time.  However, even that doesnt guarantee users that their information remains private. According to a 2019 [news report](https:\u002F\u002Fwww.vice.com\u002Fen\u002Farticle\u002Fxwnva7\u002Fsnapchat-employees-abused-data-access-spy-on-users-snaplion \"Snapchat Employees Abused Data Access to Spy on Users\"), employees inside Snapchat still used their access to company data to spy on Snapchat users.\n\nTypically, much of the data that an organization collects is stripped of personal identifiers, such as names. However, the more data an organization collects, the more easily it can infer details it does not explicitly collect, compromising subjects privacy. For example, Google and other tech companies can collect information on where people are and where they go during the day using voluntary check-ins, IP addresses, and [location data shared from users phones](https:\u002F\u002Fapnews.com\u002Farticle\u002F828aefab64d4411bac257a07c1af0ecb \"AP Exclusive: Google Tracks Your Movements, Like It or Not\"). Google and peer companies dont explicitly collect race details. But the companies can look at the neighborhood where a person lives and the places they go and can combine that with other information to infer the race of that individual. As one regulator [said](https:\u002F\u002Fwww.cnil.fr\u002Fen\u002Fcnils-restricted-committee-imposes-financial-penalty-50-million-euros-against-google-llc \"The CNILs restricted committee imposes a financial penalty of 50 Million euros against GOOGLE LLC\") in January 2019, Googles data analytics can reveal important parts of their private life since they are based on a huge amount of data, a wide variety of services and almost unlimited possible combinations.\n\nWith enough information, an organization can even determine a persons identity from anonymized data sets. Combining just 15 traits, such as zip code, birthday, and number of children, offers enough information to identify 99.98% of people in Massachusetts, according to a 2019 paper in [*Nature Communications*](https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fs41467-019-10933-3 \"Estimating the success of re-identifications in incomplete datasets using generative models\")."},{id:1667,__component:w,title:"How Big Data Can Identify Specific Individuals",type:x,caption:"Knowledge of just 4 attributeszip code, date of birth, gender, number of childrenis enough to identify nearly 80% of people in Massachusetts. If all 15 of these attributes are known, it becomes possible to identify virtually anyone in the state, according to a 2019 paper in *Nature Communications*. [Source](https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fs41467-019-10933-3#Sec6 \"Estimating the success of re-identifications in incomplete datasets using generative models\")",image:{data:{id:1428,attributes:{name:"BD_102b_05_identification_fifteen_traits_v1_a3d0b3f85d.svg",alternativeText:a,caption:a,width:H,height:jD,formats:a,hash:"BD_102b_05_identification_fifteen_traits_v1_a3d0b3f85d_c5bcfc944c",ext:r,mime:s,size:80.17,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_102b_05_identification_fifteen_traits_v1_a3d0b3f85d_c5bcfc944c.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jE,updatedAt:jE}}}},{id:3299,__component:f,text:"Organizations must secure the data they collect",slug:"organizations-must-secure-the-data-they-collect"},{id:6242,__component:d,text:"Exposed data can put consumers at risk for identity theft, fraud, and other crimes. Therefore, an organization that collects data must secure it. If an organization is hacked or otherwise loses control of data, individuals can find their privacy compromised.\n\nRecent news offers many examples. One is the September 2020 announcement by the Department of Veterans Affairs that the personal information, including Social Security numbers, of roughly 46,000 veterans was exposed in a cyberattack against a health-care payment system.\n\nTo protect consumers from exposure or harm, governments have taken action by enacting laws that mandate penalties for companies that lose control of collected data. For example, in May 2018 the European Union introduced the General Data Protection Regulation, which dictates how organizations can use data they collect, how long they may store certain kinds of data, and who can access it. Also under the GDPR, people subject to data collection can request that some collected data be deleted. Organizations that violate the GDPR can face fines: either 20 million (~USD $23.7 million) or 4% of annual revenues, whichever is greater. These rules apply to any organization that stores data physically within the EU or that handles data from EU residents, even if it physically stores it elsewhere. Laws like the GDPR influence both data collection and analysis. They also make data leaks and breaches expensive for companies, in the form of lost business, reputational damage, and fines. In January 2019, Frances National Data Protection Commission fined Google [$57 million](https:\u002F\u002Fwww.nytimes.com\u002F2019\u002F01\u002F21\u002Ftechnology\u002Fgoogle-europe-gdpr-fine.html \"Google Is Fined $57 Million Under Europes Data Privacy Law\") under the GDPR, on the grounds that Google failed to get adequate user consent for the data it collected. In reaction to this, Anurag Kahol, CTO and Founder of the cloud security company Bitglass, [commented](https:\u002F\u002Fwww.helpnetsecurity.com\u002F2019\u002F01\u002F22\u002Fgoogle-gdpr-fine\u002F \"Industry reactions to Googles 50 million GDPR violation fine\"): data protection authorities have been incredibly patient with companies. However, it seems this grace period is more or less passing.\n\nIn the U.S., the [California Consumer Privacy Act](https:\u002F\u002Fleginfo.legislature.ca.gov\u002Ffaces\u002FbillTextClient.xhtml?bill_id=201720180AB375 \"California Legislative Assembly Bill No. 375, Chapter 55\") of 2018 is similar to the GDPR in how it legally tasks organizations to be responsible stewards of the information they collect or control. Like the GDPR, it applies not only to organizations based in California but to organizations that handle data from California residents.\n\nAs of January 2021, the online stationery and crafts marketplace Minted, which is based in California, faces an [ongoing class-action lawsuit](https:\u002F\u002Fnews.bloomberglaw.com\u002Fclass-action\u002Fminted-sued-over-data-breach-under-new-california-privacy-law \"Minted Sued Over Data Breach Under California Law\"), brought in part under the CCPA. According to the complaint, Minted failed to adequately protect customer names, email addresses, passwords, and possibly other information. The lawsuit followed a cyberattack against Minted and other companies in May 2020, after which hackers attempted to sell the stolen information online. "},{id:1724,__component:v,text:gL},{id:1006,__component:aM,items:"- [Common misconceptions about big data analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fcommon-misconceptions-bda)"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:aP,__component:bB,audioUrl:hJ},{id:g_,__component:f,text:hK,slug:hL},{id:ha,__component:d,text:hM},{id:hb,__component:w,title:hc,type:x,caption:hd,image:{data:{id:he,attributes:{name:hN,alternativeText:a,caption:a,width:H,height:aQ,formats:a,hash:hO,ext:r,mime:s,size:hP,url:hf,previewUrl:a,provider:j,provider_metadata:a,createdAt:gU,updatedAt:gU}}}},{id:hg,__component:f,text:hQ,slug:hR},{id:hh,__component:d,text:hS},{id:hi,__component:f,text:hT,slug:hU},{id:hj,__component:d,text:hV},{id:hk,__component:f,text:hW,slug:hX},{id:hl,__component:d,text:hY},{id:hm,__component:hn,text:hZ},{id:ho,__component:d,text:h_},{id:hp,__component:v,text:gL},{id:hq,__component:aM,items:h$}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:jF,__component:bB,audioUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioRecentdevelopmentsinBigDataAnalytics.mp3"},{id:3291,__component:f,text:"A data warehousing company set a record for the largest software company IPO",slug:"a-data-warehousing-company-set-a-record-for-the-largest-software-company-ipo"},{id:1715,__component:v,text:hu},{id:6228,__component:d,text:"Snowflake Inc., which makes storing and analyzing big data easier, set a record for the largest ever initial public offering (IPO) for a software company, raising $3.4 billion. Snowflake software makes it easier for companies to store and analyze large amounts of data in the cloud. Big data analysis often relies on a *data warehouse*, a system that stores and manages a companys data, organized in a way that makes it easy to search and analyze. But setting up data warehouses can be an onerous task. Before Snowflake, companies would have to figure out how much data storage they expect to use, how much computational power their data analysis will take, and how to acquire and connect the computing resources they will need. Snowflakes software makes it so companies can skip those steps by automatically setting up data warehouses in the cloud. Its customers include Instacart and Capital One.\n"},{id:1663,__component:w,title:"How Does Snowflake Work?",type:x,caption:"Unlike other services, Snowflakes platform separates the acts of storing and processing data, and scales each automatically, allowing customers to pay for as much or as little as they actually use. It also handles all of the tedious tasks of database management, freeing up time and resources for customers.",image:{data:{id:1421,attributes:{name:"BD_104_01_snowflake_01a63cfa8b.svg",alternativeText:a,caption:a,width:H,height:jD,formats:a,hash:"BD_104_01_snowflake_01a63cfa8b_00ad3ba879",ext:r,mime:s,size:65.17,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_104_01_snowflake_01a63cfa8b_00ad3ba879.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jG,updatedAt:jG}}}},{id:6225,__component:d,text:"The companys stock price continued to rise after the September 16, 2020, IPO. By December, Snowflakes market valuation was around $110 billion, higher than AMD or IBM."},{id:1714,__component:v,text:hv},{id:6226,__component:d,text:"Snowflake is targeting companies that want to do big data analytics but dont have the internal expertise to set up a data warehouse on their own. Its success is a sign of a larger trend: the growing demand for businesses that make big data analysis easier for companies that lack experts in the field. Stock prices are not the only measure of a companys quality, and theres no guarantee of Snowflakes long-term success. But its initial high valuation does show that investors think that making big data analytics accessible is a vital service.\n\nA few Snowflake features make it an appealing tool for storing and analyzing big data. The amount of storage and computing power that companies are billed for flexibly scales based on how much they use; a company can store a lot of data but only pay for computing power at the times its doing big data analysis. This cuts analysis costs to at least a [third of the standard price](https:\u002F\u002Fruthvenblog.wordpress.com\u002F2019\u002F07\u002F17\u002Fthe-cost-of-not-using-snowflake\u002F). Data stored using Snowflake can also be analyzed with {%SQL%}|short for Structured Query Language|, a computer language frequently used with databases of all sizes, making it user-friendly for software engineers who dont specialize in big data.\n\nHowever, because of the demand for easier big data analytics solutions, Snowflake faces competition from major cloud computing platforms like those offered by Google, Amazon, or Microsoft. In July 2020, Google released BigQuery Omni, which allows users to perform SQL database queries on big data hosted in a Google, Amazon, and\u002For Microsoft cloud. In December 2020, Microsoft launched Azure Synapse, which makes big data analysis in the Microsoft cloud simpler.\n"},{id:3292,__component:f,text:"Nvidia released a new chip for data analysis",slug:"nvidia-released-a-new-chip-for-data-analysis"},{id:ju,__component:v,text:hu},{id:6227,__component:d,text:"Computer hardware company Nvidia released a new chip in May 2020 designed specifically for data analytics. Called the [A100](https:\u002F\u002Fwww.nvidia.com\u002Fen-us\u002Fdata-center\u002Fa100\u002F), the chip is much faster than previous state-of-the-art hardware. Data analysis on the A100 is up to twice as fast as on previous chips, and up to 20 times faster for processing neural networks, complex data processing systems inspired by the human brain. (For more on neural networks, see [How AI Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-ai-works).) Nvidia CEO Jensen Huang says the A100 is at nearly the theoretical limits of whats possible in semiconductor manufacturing today."},{id:1660,__component:w,title:"Nvidia A100 Chip",type:x,caption:"Each Nvidia A100 processing unit is 27 cm by 11 cm. The chip inside is only a few centimeters across, and contains 54 billion transistors. The previous state-of-the-art chip contained fewer than half as many transistors in a similar size. Nvidia 2020. Used with permission.",image:{data:{id:1423,attributes:{name:"BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769.jpg",alternativeText:a,caption:a,width:2750,height:1334,formats:{large:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1.jpg",hash:"large_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1",mime:h,name:"large_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769.jpg",path:a,size:52,width:P,height:485},small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1.jpg",hash:"small_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1",mime:h,name:"small_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769.jpg",path:a,size:17.15,width:y,height:242},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1.jpg",hash:"medium_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1",mime:h,name:"medium_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769.jpg",path:a,size:32.66,width:B,height:364},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1.jpg",hash:"thumbnail_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1",mime:h,name:"thumbnail_BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769.jpg",path:a,size:5.26,width:aI,height:119}},hash:"BD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1",ext:l,mime:h,size:259.24,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_104_02_NVIDIA_A100_GPU_resize_e8a2509769_4b20f8f9f1.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jH,updatedAt:jH}}}},{id:6229,__component:d,text:"The chips first customers are major companies Alibaba, Amazon, Baidu, Google, and Microsoft, which will use the chip in their servers that support cloud computing, crucial infrastructure for big data analytics. Nvidia also sells a bank of eight A100 processing units joined together for extremely demanding data analysis tasks. The first customer for this bank is the U.S. Energy Departments Argonne National Laboratory, which incorporated the chips into its supercomputer, currently being used for COVID-19 vaccine research."},{id:1717,__component:v,text:hv},{id:6230,__component:d,text:"One of the major limiting factors in big data analytics is the hardware used. The more data, the longer it takes for a computer to process it. Major hardware advances like this one make data analysis faster and cheaper. They also reach users and organizations that dont purchase the new chips directly, when they are used in shared supercomputers or cloud infrastructure that can be accessed remotely.\n\nThe creation of the A100 chip also indicates how important big data analysis has become. Advances in computer technology are now being driven in large part by demand for better processing of big data. When Nvidia was founded in 1993, its focus was making high-performance processors for personal computers so that users could enjoy better visuals when playing computer games. In the fall of 2020, Nvidias revenue from its data center products surpassed its revenue from gaming equipment for the first time."},{id:1661,__component:w,title:"New Chips Reduce Power Usage and Server Footprint of an AI Data Center",type:x,caption:"Compared with current state-of-the-art data centers, a facility equipped with A100 chip processors would be able to analyze more data at higher speeds. Nvidia claims that customers using the chip for AI processing would pay less than 10% of their current costs, and save even more for other data processing tasks.",image:{data:{id:1422,attributes:{name:"BD_104_03_Nvidia_charts_020ce0b319.svg",alternativeText:a,caption:a,width:H,height:368,formats:a,hash:"BD_104_03_Nvidia_charts_020ce0b319_d0d1da2038",ext:r,mime:s,size:48.26,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_104_03_Nvidia_charts_020ce0b319_d0d1da2038.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jI,updatedAt:jI}}}},{id:3293,__component:f,text:"Big data analytics is being used to fight COVID-19",slug:"big-data-analytics-is-being-used-to-fight-covid-19"},{id:1719,__component:v,text:hu},{id:6231,__component:d,text:"Since the start of the pandemic, big data analytics has been used to track the spread of COVID-19, understand its symptoms and effects, and help individuals make better decisions to avoid it.\n\nIn May 2020, researchers at Kings College London analyzed symptom data gathered from a smartphone app. Over 18,000 people who had been tested for COVID-19 used the app to self-report their symptoms and test results. Using this information, the researchers [confirmed](https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fs41591-020-0916-2) that loss of smell and taste was a key indicator of COVID-19 infection, alongside the more common symptoms of coughing and running a fever. Meanwhile, in China, financial technology company Ant Group worked with the government to create a [health code](https:\u002F\u002Fwww.nytimes.com\u002F2020\u002F03\u002F01\u002Fbusiness\u002Fchina-coronavirus-surveillance.html) system, a way to quickly identify an individuals risk of infection. Chinese citizens enter their personal information and travel history into a smartphone app, which analyzes data on where the disease has been detected to generate a color code: green allows users to travel freely in public spaces, while yellow and red require them to quarantine. In many regions, the app is scanned whenever someone uses public transportation or enters a public space, and a persons code is automatically updated if they recently encountered an infected person. The system [seems to be successful](https:\u002F\u002Fwww.wired.co.uk\u002Farticle\u002Fchina-coronavirus-health-code-qr) at limiting COVID-19 spread, though it [reduces individual privacy](https:\u002F\u002Fwww.nytimes.com\u002F2020\u002F03\u002F01\u002Fbusiness\u002Fchina-coronavirus-surveillance.html).\n"},{id:1662,__component:w,title:"China's Color-Coded COVID-19 App",type:x,caption:"The app, developed by Alipay and WeChat, assesses risk of infection by analyzing health information uploaded daily by the user, tracking the users location, and tracking the health code status of the users personal contacts. In many regions, individuals are required to show the health code app to access public spaces. Image from SAGE Journal Publications. [(CC BY) 4.0](https:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F \"Creative Commons CC BY 4.0 license description\")",image:{data:{id:1424,attributes:{name:"BD_104_04_china_health_code_app_5f4d9f26ef.png",alternativeText:a,caption:a,width:jn,height:1409,formats:{large:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8.png",hash:"large_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8",mime:p,name:"large_BD_104_04_china_health_code_app_5f4d9f26ef.png",path:a,size:280.55,width:P,height:jx},small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8.png",hash:"small_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8",mime:p,name:"small_BD_104_04_china_health_code_app_5f4d9f26ef.png",path:a,size:90.35,width:y,height:282},medium:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8.png",hash:"medium_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8",mime:p,name:"medium_BD_104_04_china_health_code_app_5f4d9f26ef.png",path:a,size:175.97,width:B,height:423},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8.png",hash:"thumbnail_BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8",mime:p,name:"thumbnail_BD_104_04_china_health_code_app_5f4d9f26ef.png",path:a,size:28.74,width:aI,height:aR}},hash:"BD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8",ext:o,mime:p,size:226.18,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_104_04_china_health_code_app_5f4d9f26ef_c745f9f5c8.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:jJ,updatedAt:jJ}}}},{id:6232,__component:d,text:"Other big data initiatives have focused on understanding the dangers of the disease. Researchers at Johns Hopkins University used publicly available health data and information about COVID-19 deaths to identify [what factors](https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fs41591-020-01191-8) put someone most at risk for serious illness and to create a [risk calculator](https:\u002F\u002Fcovid19risktools.com:8443\u002Friskcalculator) that can help individuals assess the risk of infection. At the same time, researchers at New York University built a [system](https:\u002F\u002Fwww.techscience.com\u002Fcmc\u002Fv63n1\u002F38464) that predicted which early symptoms would likely lead to serious complications. This can be used by hospitals to identify wholl need intensive care.\n\nSeveral major organizations recognized the importance of big data analytics in fighting COVID-19 and pushed to make useful data accessible. In March 2020, the White House Office of Science and Technology Policy launched a [database](https:\u002F\u002Fwww.semanticscholar.org\u002Fcord19) of scientific literature about the novel coronavirus, assembled by the Allen Institute for AI, the Chan Zuckerberg Initiative, Georgetown University, Microsoft, and the National Institutes of Health. Google also collected datasets related to COVID-19 and made them [easily accessible](https:\u002F\u002Fcloud.google.com\u002Fblog\u002Fproducts\u002Fdata-analytics\u002Ffree-public-datasets-for-covid19) to the public. In February 2021, a group of researchers representing several universities [released anonymized data](https:\u002F\u002Fglobal.health\u002F) on over 10 million cases of COVID-19 from around the world."},{id:1718,__component:v,text:hv},{id:6233,__component:d,text:"In some ways, big data analytics is a natural fit for the challenges posed by COVID-19. As the virus spreads, more data can be gathered on who has been infected, how they might have contracted the disease, and the diseases effects. Analyzing this data provides knowledge on how this still largely unknown virus spreads and how it can be stymied.\n\nHowever, the COVID-19 pandemic has also affirmed the challenges of executing a big data analytics strategy. Big data analytics needs a large amount of accurate and unbiased data, which requires a systematic and scalable process for gathering data. But data collection during the pandemic has been limited: In the U.S., each state has had its own method for collecting and reporting testing data and number of COVID-19 cases to the federal government, which didnt release the data publicly. Some government agencies intended to use big data analytics to fight the pandemic, but without a good system for gathering data, [the early rise in cases was missed](https:\u002F\u002Fwww.theatlantic.com\u002Fscience\u002Farchive\u002F2021\u002F03\u002Famericas-coronavirus-catastrophe-began-with-data\u002F618287\u002F). Even in China, [health codes varied between regions](https:\u002F\u002Fwww.jmir.org\u002F2020\u002F10\u002Fe21980\u002F), making it difficult to conduct a large-scale analysis of how the disease has spread. Countries that have been highly successful in limiting the spread of the disease, such as South Korea, have [generally relied more](https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fd41586-020-03518-4) on contact tracingpersonally following up on each infected persons recent history through interviews, and then informing anyone they met during that time. This is small data analysis: human effort, rather than automated systems, thoroughly investigating a small number of important data points. While big data analytics is a powerful tool, and one that would be well-suited to tackling COVID-19, it only works as part of a broader strategy.\n"},{id:3294,__component:f,text:jK,slug:"regulators-criticized-big-data-analytics-companies-on-privacy-grounds"},{id:1720,__component:v,text:jK},{id:1721,__component:v,text:hu},{id:6235,__component:d,text:"In February 2021, Canadas federal privacy commissioner declared that facial recognition company Clearview AI had violated Canadian privacy laws by recording and storing citizens faces without their consent. Clearviews software, intended for use by law enforcement, matches face images with a database of names and social media profiles. The company has acquired over 3 billion images posted on social media networks and other public websites. While Privacy Commissioner Daniel Therrien ruled this was a violation, his office doesnt have the authority to fine Clearview, and its unknown what consequences the American company will face.\n\nIn a similar case in January 2021, the U.S. Federal Trade Commission found that photo-sharing company Everalbum misled users about its privacy protections. Everalbums app uses facial recognition technology to automatically tag users friends in photos. Despite Everalbum claiming that it only applied facial recognition when users activated that feature, the technology was always active in most U.S. states. The company also retained the pictures indefinitely, despite claiming that photos would be deleted when users deleted their accounts. The FTC ruled that Everalbum must delete all photos from users who deleted their accounts, and that the company must also delete all algorithms that were trained on those photos. (Facial recognition software must be trained on example images of faces; for more, see [How AI Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-ai-works).)\n"},{id:1722,__component:v,text:hv},{id:6234,__component:d,text:"Cases like Clearview and Everalbum show how complicated privacy laws are when it comes to biometric data collection. In some U.S. states, clear laws already address these issues directly. Since 2008, Illinois has had a privacy law that makes it illegal to gather biometric information about citizens without their consent. Notably, Everalbums app always allowed Illinois residents to opt out of the technology while not giving that option to residents of other states. In 2020, Clearview was found to have violated laws in Illinois, and now specifically allows Illinois residents to contact the company to opt out of being included in its database.\n\nHowever, in many jurisdictions in the U.S. and elsewhere, the laws are not so clear. As [controversial](https:\u002F\u002Fwww.nytimes.com\u002F2020\u002F01\u002F18\u002Ftechnology\u002Fclearview-privacy-facial-recognition.html) as Clearviews massive database of faces is, its unclear if the company violated any Canadian laws. As Clearviews lawyer said in a statement, Clearview AI is a search engine that collects public data just as much larger companies do, including Google, which is permitted to operate in Canada. Yet, despite this, Canadas privacy commissioner ruled Clearviews actions are mass surveillance, and therefore illegal. The Clearview and Everalbum cases may indicate that regulators are getting more concerned about individual privacy, and a shift to more restrictive rules could be ahead.\n\nAs the field of big data grows, and more personal information is gathered for analysis, privacy laws may grow more restrictive; if so, companies doing big data collection or analysis will need to keep up with evolving guidelines. Everalbums settlement with the FTC is particularly interesting in this regard, as the order extends not just to photos but to the AI systems developed from them. If the order sets a precedent that AI systems built from improperly collected data are illegal, and must be discarded, then organizations building AI products will face new obstacles. Some experts favor those obstacles, arguing they are necessary for technology companies to take privacy concerns seriously, but they would also limit companies ability to innovate and create new products reliant on big data.\n"},{id:1723,__component:v,text:gL},{id:1005,__component:aM,items:"- [How big data analytics is used today in industry](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-is-used-today-in-industry)"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:133,__component:bB,audioUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioHowBigDataAnalyticsisUsedinIndustryToday.mp3"},{id:5666,__component:d,text:"Collecting, sorting, and processing large quantities of data can reveal patterns in complicated systems such as markets, customer bases, and supply chains. Organizations that interact with such systems can use big data analytics to guide decision-making in many ways.\n\nIn this article, we focus on applications of big data that significantly use analytics, typically toward some business goal. Some applications do not. An organization might collect data simply to catalogue or reference, to share with researchers or to sell to advertisers. For example, the U.S. Census gathers large amounts of data about citizens, using [big data software](https:\u002F\u002Fwww.cloudera.com\u002Fabout\u002Fcustomers\u002Fus-census-bureau.html) to store the information for reference purposes, but not to perform sophisticated statistical analysis.\n\nToday, organizations are gathering and using more data than ever before, using increasingly powerful computers and sophisticated algorithms.\n"},{id:2986,__component:f,text:jg,slug:"analyzing-trends"},{id:1546,__component:v,text:"In individual or consumer behavior"},{id:5663,__component:d,text:"Trend monitoring, once thought of as more an art than a science, has been transformed with big data analytics. For instance, many public relations firms use big data analytics to track a brands reputation. [Cision](https:\u002F\u002Fwww.cision.com\u002F) software monitors over 7 million blogs, podcasts, social media platforms, and sources of online, print, and broadcast news. It records each time a brand is mentioned, letting public relations firms track whos talking about their brand and what theyre saying. It also analyzes which journalists, outlets, and social media commentators are the most influential with a brands target audience, making it easier to target PR pitches.\n\nIn industries that havent traditionally monitored trends, big data analytics can enable them to do so. Software from [Lex Machina](https:\u002F\u002Flexmachina.com\u002F) analyzes legal data, so that lawyers can understand trends in how judges rule, how long a case is likely to last, and how well a specific firm has performed before a specific judge. [Premonition](https:\u002F\u002Fpremonition.ai\u002F) provides similar software for litigants, letting users pick lawyers based on their win rates on similar cases in front of similar judges.\n\nGovernments gather and analyze large amounts of data on their citizens. During the COVID-19 pandemic, the Chinese government in particular has tracked the movements of citizens [through smartphone apps](https:\u002F\u002Fwww.nytimes.com\u002F2020\u002F03\u002F01\u002Fbusiness\u002Fchina-coronavirus-surveillance.html) to stay informed about when and where residents of a high-risk area travel, potentially spreading the virus. (For more on big data analytics during the pandemic, see [Recent Developments in Big Data Analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Frecent-developments-bd#big-data-analytics-is-being-used-to-fight-covid-19)).\n"},{id:1547,__component:v,text:"In markets"},{id:5664,__component:d,text:"Markets generate lots of data, as supply and demand change and prices shift. The smartphone app Hopper analyzes market trends to help users get the [best rates on air travel and hotels](https:\u002F\u002Fwww.cardrates.com\u002Fnews\u002Fhopper-analyzes-travel-data-to-forecast-savings-on-flights-and-hotels\u002F). Users indicate when and where theyre planning to travel, usually months before the trip. Then the app tracks the fluctuating prices of flights and hotels, recording over 25 billion prices per day for trips all over the world. It compares these changing prices with its records of price shifts since 2015, to predict when flight and hotel rates will be at their lowest and advise the user when to buy them.\n\nBig data analysis can be particularly useful for trading stocks. Stock traders are [increasingly relying](https:\u002F\u002Fwww.bloomberg.com\u002Fprofessional\u002Fblog\u002Fasset-managers-survival-to-rely-on-big-data-prowess-technology\u002F) on *alternative data*, as the finance industry calls data other than stock prices and corporate reports. Alternative data might be cargo shipping records that show how goods are moving around the world, cell-phone geolocation data that shows how many people visit a particular store, auto insurance records that reveal how many cars are being sold, or online product reviews or posts about a company on social media. Big data analytics is needed to understand these large, often {%unstructured%}|Data that is difficult to organize into tables (e.g., text, videos, pictures, maps, and social media posts).| data sets, which can reveal more about a companys performance than stock prices."},{id:1440,__component:w,title:"Alternative Data Provides Additional Insight for Investors",type:x,caption:a,image:{data:{id:jL,attributes:{name:"BD_105_alternative_data_v3_a0db636122.svg",alternativeText:a,caption:a,width:H,height:687,formats:a,hash:"BD_105_alternative_data_v3_a0db636122_177e00df6a",ext:r,mime:s,size:127.17,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_105_alternative_data_v3_a0db636122_177e00df6a.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jM,updatedAt:jM}}}},{id:5667,__component:d,text:"Alternative data became particularly important in 2020, when the global economy was disrupted by the COVID-19 pandemic. According to Hinesh Kalian, director of data science at investment firm Man Group, demand for alternative data increased in 2020 because when there is market volatility, investment managers will tend to seek information which is timely and accurate. While traditional market data is still useful, alternative data is raw, unique, and timely, giving companies that use it an edge.\n\nAs the demand for alternative data grows, major financial services companies are purchasing sources of data. In 2018, Nasdaq, Inc., acquired Quandl, a major vendor of alternative data sets, including vehicle traffic data from [internet of things](https:\u002F\u002Fhorizonapp.mit.edu\u002Ftopic\u002Finternet-of-things-iot) sensors and satellite imagery of areas where a lot of construction is underway. In December 2020, Bloomberg [acquired](https:\u002F\u002Fwww.bloomberg.com\u002Fcompany\u002Fpress\u002Fsecond-measure\u002F) Second Measure, which collects and cleans anonymized data on billions of credit card purchases. S & P Global, Inc., has also [announced](https:\u002F\u002Fwww.spglobal.com\u002Fmarketintelligence\u002Fen\u002Fnews-insights\u002Flatest-news-headlines\u002Fs-p-global-ihs-markit-merger-2020-s-largest-deal-shows-value-of-financial-data-61516189) a merger with IHS Markit, which provides alternative data sets such as live ship tracking.\n"},{id:2988,__component:f,text:jh,slug:"recommending-products"},{id:5665,__component:d,text:"By analyzing large amounts of customer data, businesses can better understand and target their audience. A common application of big data analytics is in personalized recommendation systems, software that recommends new products or content to users. Amazon pioneered the use of big data analytics to recommend products to shoppers. Its first automated recommendation system, created in the early 2000s, identified products that were frequently bought together and recommended products related to the one a customer was looking at. (Before this, Amazon recommended purchases based on employee-created lists of related products, but these could be biased or miss niche interests.)\n\nToday, Amazons recommendation algorithm uses many sources of data, including product ratings and reviews and a shoppers browsing history; what theyve viewed, bought, added to a cart, rated, or reviewed. Users see these recommendations when they visit the Amazon.com home page and as they browse, and also sometimes receive them via email. Researchers at the University of Florida and the Wharton business school estimate that Amazons recommendation of a product [increases its sales](https:\u002F\u002Fnews.ufl.edu\u002Farticles\u002F2018\u002F09\u002Fhow-helpful-are-product-recommendations-really.html) by 9%.\n\nEntertainment platforms use big data analytics to recommend content to customers. Netflix tracks users when theyre on the service, gathering detailed data about their viewing habits. Recommendations arent based only on what a user has watched in the past and what they have indicated they enjoyed, but also on the time of day theyre using the service, the device theyre using, and how long theyve been watching. Netflix then compares this data with data from other users and recommends what similar users have watched and enjoyed in the past. As of 2018, Netflix estimated that [80% of the content](https:\u002F\u002Fresearch.netflix.com\u002Fresearch-area\u002Frecommendations) viewed was recommended by this system, with the remainder coming from user searches. (The order in which search results are given is also based on big data analytics.)"},{id:1441,__component:w,title:"Netflix Recommendations Based on User Data",type:x,caption:a,image:{data:{id:jN,attributes:{name:"BD_105_recommendation_netflix_v5_dbb610556f.svg",alternativeText:a,caption:a,width:H,height:885,formats:a,hash:"BD_105_recommendation_netflix_v5_dbb610556f_0856f23d40",ext:r,mime:s,size:162.89,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_105_recommendation_netflix_v5_dbb610556f_0856f23d40.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jO,updatedAt:jO}}}},{id:5670,__component:d,text:"A similar recommendation system used by Disney+ tries to [understand users](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Finsights-teradata\u002F2020\u002F04\u002F21\u002Fhow-disney-plus-personalizes-your-viewing-experience\u002F?sh=1d9ec6ac3b6e) through their data for instance, based on what a household has watched, the algorithm will model them as including young children, older children, or no children. Music-streaming service Spotify [uses big data analysis](https:\u002F\u002Fengineering.atspotify.com\u002F2020\u002F01\u002F16\u002Ffor-your-ears-only-personalizing-spotify-home-with-machine-learning\u002F) to automatically suggest songs and artists, and even create playlists, for each user. Its recommendations are based not only on what a user has listened to in the past but also on how often theyve followed past recommendations.\n\nAutomatic recommendation systems have their downsides. In March 2019, *Wired* reported that Amazon was [recommending pseudoscience books](https:\u002F\u002Fwww.wired.com\u002Fstory\u002Famazon-and-the-spread-of-health-misinformation\u002F) to users interested in learning about vaccines or cancer treatments, and NBC reported that a book [spreading conspiracy theories](https:\u002F\u002Fwww.nbcnews.com\u002Ftech\u002Ftech-news\u002Famazon-qanon-conspiracy-book-climbs-charts-algorithmic-push-n979181) was becoming an Amazon top seller, driven by recommendations.\n"},{id:2987,__component:f,text:ji,slug:"setting-prices"},{id:5668,__component:d,text:"By analyzing large amounts of data on customer behavior, businesses can set prices more quickly and accurately, especially in markets where demand fluctuates. Dayton Superior, a concrete construction company, uses big data analytics to automatically set prices for their services around the world, adapting the price to the conditions of the local market. Previously, the company published an annual price list, then negotiated any discounts with clients individually. This made it difficult to compete with local vendors, who could set prices more quickly and accurately. When the company switched to a data-driven approach, it worked with a software provider to [create a pricing optimization system](https:\u002F\u002Fwww.cio.com\u002Farticle\u002F3054543\u002Fglobal-construction-company-uses-analytics-to-make-pricing-local.html) that automatically took into account data from markets around the world as well as data from Dayton Superiors sales history and the guidance of their domain experts. The new system lets sales reps pick prices more quickly, and the greater consistency in pricing has increased customer satisfaction.\n\nRide-hailing company Uber uses big data analytics to set prices for each user. It automatically sets prices based on the supply and demand of cars in the area, but it also [adjusts the price for each route](https:\u002F\u002Fwww.bloomberg.com\u002Fnews\u002Farticles\u002F2017-05-19\u002Fuber-s-future-may-rely-on-predicting-how-much-you-re-willing-to-pay) based on what past data suggests users are willing to pay. For instance, rides to or from expensive areas could cost more.\n"},{id:2989,__component:f,text:jj,slug:"improving-logistics-and-transportation"},{id:5669,__component:d,text:"Retail, construction, manufacturing, and shipping companies analyze data to increase efficiency, putting goods and equipment where they need to be, at the right time, and for the lowest cost. For example, Finnish retail chain SOK uses big data analytics to keep shelves stocked. Store managers record data on their sales and inventory, and an algorithm [analyzes the data](https:\u002F\u002Fwww.solvoyo.com\u002Fresources\u002Fautomated-replenishment-with-big-data-analytics\u002F) to generate weekly sales forecasts, calculate optimum inventory for each store, and recommend purchase orders. Construction company Skanska analyzes big data to optimize performance on job sites, tracking worker location and activity with wearable devices to determine where to place tools and resources (for more on wearable devices and related technology, see [Internet of Things](https:\u002F\u002Fhorizonapp.mit.edu\u002Ftopic\u002Finternet-of-things-iot)). This has [cut the amount of time workers spend](https:\u002F\u002Fwww.constructiondive.com\u002Fnews\u002Fbetting-on-big-data-how-construction-firms-are-leveraging-digitized-job-si\u002F431047\u002F) moving around a job site by one-third.\n\nUPS uses big data analytics to optimize how it transports packages. The company [takes into account](https:\u002F\u002Fwww.wsj.com\u002Farticles\u002Fups-it-chief-says-ambitious-analytics-project-will-improve-logistics-planning-1531854955?tesla=y) data on package weights, shapes, and sizes, cross-referencing that with historical trends in demand and shipping capacity, to determine the optimal route for each truck. Their routing software also [updates routes in real time](https:\u002F\u002Fbusiness-review.eu\u002Fnews\u002Fups-to-enhance-orion-with-continuous-delivery-route-optimization-210711) if road conditions change. Local delivery drivers can make over a hundred deliveries a day, so optimizing routes is essential; the company estimates the software has saved about a hundred million miles of driving per year. Amazon goes a step further: Its patented [anticipatory shipping](https:\u002F\u002Fwww.npr.org\u002F2018\u002F11\u002F21\u002F660168325\u002Foptimized-prime-how-ai-and-anticipation-power-amazons-1-hour-deliveries) system moves goods around the world before theyve been ordered, using analysis of past orders on its platform to predict where products will be in demand. \n"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:3723,__component:f,text:"A - M",slug:"a-m"},{id:1110,__component:E,name:"AI (artificial intelligence)",definition:"A computer system trained to extrapolate from data in order to make automated decisions or predictions. Artificial intelligence differs from *analytics* in that once its mathematical models have been trained, they can, when provided with new data, make decisions and predictions more accurately. See [How AI Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-ai-works).",slug:"ai-artificial-intelligence"},{id:1123,__component:E,name:"Algorithm",definition:"A set of rules or instructions that a human or a computer follows to perform a task.",slug:"algorithm"},{id:1126,__component:E,name:"Analytics",definition:"The study of patterns in data in order to improve decision making.",slug:"analytics"},{id:1111,__component:E,name:"Big data",definition:"A large collection of data, usually gathered rapidly and continuously from multiple sources. Although there is some disagreement about how much data qualifies a set as big, most experts describe a data set as big data if it is too large to be stored on a single computer. Experts also use the term *big data* to refer more broadly to the study of how to gather, store, analyze, and visualize such collections of data, and ways to use it to inform decision making.",slug:"big-data"},{id:1129,__component:E,name:"The cloud",definition:"Any network of computing resources (i.e., servers that store data and computers that process it) that can be used remotely.",slug:"the-cloud"},{id:1125,__component:E,name:"Data cleaning",definition:"The process of removing or correcting inaccurate data within a data set. Data sets are often gathered quickly from multiple sources, and can contain typographical mistakes, incomplete information, or other errors that must be corrected before analysis. Sometimes referred to as *data cleansing* or *data scrubbing*.",slug:"data-cleaning"},{id:1120,__component:E,name:"Data lake",definition:"A centralized repository that holds raw data (i.e., data that has not been cleaned or analyzed yet), typically from multiple sources.",slug:"data-lake"},{id:1112,__component:E,name:"Data mining",definition:"Analyzing large data sets to discover new information. Unlike typical data analysis, which measures the strength of a hypothesis, data mining is the act of exploring a data set without a hypothesis to find previously unknown patterns.",slug:"data-mining"},{id:1117,__component:E,name:"Data modeling",definition:"Creating a diagram that represents how data moves through an organization and how it is used. The diagram illustrates the types of data each department needs and how they are related. *Data modeling* can also describe how data is stored and organized (in terms of tables and relationships) in a database.",slug:"data-modeling"},{id:1114,__component:E,name:"Data pipeline",definition:"An organizations process for collecting, organizing, and sharing data.",slug:"data-pipeline"},{id:1124,__component:E,name:"Data science",definition:"A broad, catchall term for the study of data. Its practitioners are data scientists, who use knowledge of statistics and computer science to convert data into useful information.",slug:aW},{id:1113,__component:E,name:"Data visualization",definition:"A visual representation of data (such as a graph) or the process of making such a representation. The same data can be visualized many different ways; the process of data visualization is an art as well as a science.",slug:"data-visualization"},{id:1119,__component:E,name:"Data warehouse",definition:"A centralized repository of data from multiple sources that allows for easy access and analysis. A data warehouse may exist on local servers or in the cloud.",slug:"data-warehouse"},{id:1121,__component:E,name:"Data wrangling",definition:"Converting data from one format to another to prepare it for statistical analysis. This is particularly important when data comes from different sources (e.g. one source may provide a list of customer names as \\[First Name, Last Name] while another source may provide it as \\[Last Name, First Name]).",slug:"data-wrangling"},{id:1118,__component:E,name:"Forecasting",definition:"Looking at historical data measured over a period of time and extrapolating forward predictions (e.g., predicting how much gold a mine will produce by looking at its past gold production). Forecasting is narrower than *predictive analytics*, which looks at several different data types to make predictions about the future.",slug:"forecasting"},{id:1115,__component:E,name:"Machine learning",definition:"A subfield of *artificial intelligence* involving systems that can be trained to interpret or extrapolate from data without depending on explicit, preprogrammed rules.",slug:"machine-learning"},{id:3724,__component:f,text:"N - Z",slug:"n-z"},{id:1116,__component:E,name:"Predictive analytics",definition:"A broad term for using several different types of data to make granular predictions about the future. For example, predicting what a customer will buy next by looking at past purchases, demographic data, and disposable income to predict what a customer will buy next.",slug:"predictive-analytics"},{id:1127,__component:E,name:"Pricing optimization",definition:"The use of data analysis to choose prices that will yield the most profit, typically by companies with large amounts of data on customer behavior, historic sales trends, and inventory costs.",slug:"pricing-optimization"},{id:1128,__component:E,name:jk,definition:"Data that is organized and formatted, typically in tables, so that it is easily searchable (e.g., a list of names related to phone numbers and addresses).",slug:"structured-data"},{id:1122,__component:E,name:"Server",definition:"A high-powered computer that organizations use to store and process data sets. Servers are typically installed in rows of server towers inside a data center.",slug:"server"},{id:1130,__component:E,name:jl,definition:"Data that is difficult to organize into tables (e.g., text, videos, pictures, maps, and social media posts).",slug:"unstructured-data"}],foundation_topic:{data:{id:I,attributes:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi,icon:{data:{id:bC,attributes:{name:bD,alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:bE,ext:r,mime:s,size:bj,url:bF,previewUrl:a,provider:j,provider_metadata:a,createdAt:O,updatedAt:O}}},image:{data:{id:bG,attributes:{name:bH,alternativeText:a,caption:a,width:bI,height:bJ,formats:{large:{ext:l,url:bK,hash:bL,mime:h,name:bM,path:a,size:bN,width:P,height:bO},small:{ext:l,url:bP,hash:bQ,mime:h,name:bR,path:a,size:bS,width:y,height:bT},medium:{ext:l,url:bU,hash:bV,mime:h,name:bW,path:a,size:bX,width:B,height:Q},thumbnail:{ext:l,url:bY,hash:bZ,mime:h,name:b_,path:a,size:b$,width:aI,height:ca}},hash:cb,ext:l,mime:h,size:cc,url:cd,previewUrl:a,provider:j,provider_metadata:a,createdAt:R,updatedAt:R}}},technical:{data:[]},upcomingEvents:{data:[{id:ce,attributes:{title:cf,slug:cg,dateTime:ch,description:ci,bio:cj,registerURL:ck,duration:k,showTime:i,video:cl,featuredDescription:a,createdAt:S,updatedAt:S,publishedAt:cm}},{id:Q,attributes:{title:cn,slug:co,dateTime:cp,description:cq,bio:cr,registerURL:cs,duration:k,showTime:i,video:ct,featuredDescription:a,createdAt:T,updatedAt:T,publishedAt:cu}},{id:cv,attributes:{title:cw,slug:cx,dateTime:cy,description:cz,bio:cA,registerURL:cB,duration:k,showTime:i,video:cC,featuredDescription:a,createdAt:U,updatedAt:U,publishedAt:cD}},{id:cE,attributes:{title:cF,slug:cG,dateTime:cH,description:cI,bio:cJ,registerURL:cK,duration:k,showTime:i,video:cL,featuredDescription:a,createdAt:V,updatedAt:V,publishedAt:cM}},{id:cN,attributes:{title:cO,slug:cP,dateTime:cQ,description:cR,bio:cS,registerURL:cT,duration:k,showTime:i,video:cU,featuredDescription:a,createdAt:W,updatedAt:W,publishedAt:cV}},{id:cW,attributes:{title:cX,slug:cY,dateTime:cZ,description:c_,bio:c$,registerURL:da,duration:k,showTime:i,video:db,featuredDescription:a,createdAt:X,updatedAt:X,publishedAt:dc}},{id:dd,attributes:{title:de,slug:df,dateTime:dg,description:dh,bio:di,registerURL:dj,duration:k,showTime:i,video:dk,featuredDescription:a,createdAt:Y,updatedAt:Y,publishedAt:dl}},{id:dm,attributes:{title:dn,slug:do0,dateTime:dp,description:dq,bio:dr,registerURL:ds,duration:k,showTime:i,video:dt,featuredDescription:a,createdAt:Z,updatedAt:Z,publishedAt:du}},{id:dv,attributes:{title:dw,slug:dx,dateTime:dy,description:dz,bio:dA,registerURL:a,duration:k,showTime:i,video:dB,featuredDescription:a,createdAt:_,updatedAt:_,publishedAt:dC}}]},podcasts:{data:[]},foundations:{data:[{id:dD,attributes:{title:dE,slug:dF,status:b,publishedOn:t,lastUpdated:t,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:$,updatedAt:$,publishedAt:dG,locale:e}},{id:dH,attributes:{title:dI,slug:dJ,status:b,publishedOn:t,lastUpdated:t,readingTime:dK,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aa,updatedAt:aa,publishedAt:dL,locale:e}},{id:dM,attributes:{title:bk,slug:dN,status:b,publishedOn:g,lastUpdated:g,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ac,updatedAt:ac,publishedAt:dO,locale:e}},{id:dP,attributes:{title:bl,slug:dQ,status:b,publishedOn:ad,lastUpdated:ad,readingTime:ab,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ae,updatedAt:ae,publishedAt:dR,locale:e}},{id:aQ,attributes:{title:a$,slug:ba,status:b,publishedOn:F,lastUpdated:F,readingTime:bb,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:G,updatedAt:G,publishedAt:bc,locale:e}},{id:dS,attributes:{title:dT,slug:dU,status:b,publishedOn:af,lastUpdated:af,readingTime:dV,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ag,updatedAt:ag,publishedAt:dW,locale:e}},{id:dX,attributes:{title:dY,slug:dZ,status:b,publishedOn:ah,lastUpdated:ah,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aj,updatedAt:aj,publishedAt:d_,locale:e}},{id:d$,attributes:{title:aS,slug:ea,status:b,publishedOn:g,lastUpdated:g,readingTime:u,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ak,updatedAt:ak,publishedAt:eb,locale:e}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG}}]}}}},technical_topic:{data:a},localizations:{data:[]}}}]},moreFromMIT:{data:[{id:ec,attributes:{title:ed,slug:ee,status:b,publishedOn:al,lastUpdated:al,readingTime:ef,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:am,updatedAt:am,publishedAt:eg,locale:e,pdf:{data:a},logo:{data:{id:1628,attributes:{name:"CISR_logo_color_8c0365d951.png",alternativeText:a,caption:a,width:578,height:495,formats:{small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_CISR_logo_color_8c0365d951_53609cf02f.png",hash:"small_CISR_logo_color_8c0365d951_53609cf02f",mime:p,name:"small_CISR_logo_color_8c0365d951.png",path:a,size:62.89,width:y,height:ic},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_CISR_logo_color_8c0365d951_53609cf02f.png",hash:"thumbnail_CISR_logo_color_8c0365d951_53609cf02f",mime:p,name:"thumbnail_CISR_logo_color_8c0365d951.png",path:a,size:18.17,width:182,height:z}},hash:"CISR_logo_color_8c0365d951_53609cf02f",ext:o,mime:p,size:9.19,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FCISR_logo_color_8c0365d951_53609cf02f.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:jP,updatedAt:jP}}},contentBlocks:[{id:3829,__component:f,text:"Abstract",slug:"abstract"},{id:7134,__component:d,text:"MIT CISR research has found that interorganizational data sharing is a top concern of companies; leaders often find data sharing costly, slow, and risky. Interorganizational data sharing, however, is requisite for new value creation in the digital economy. Digital opportunities require data sharing 2.0: cross-company sharing of complementary data assets and capabilities, which fills data gaps and allows companies, often collaboratively, to develop innovative solutions. This briefing introduces three sets of practicescurated content, designated channels, and repeatable controlsthat help companies accelerate data sharing 2.0."},{id:3828,__component:f,text:ie,slug:jQ},{id:7137,__component:d,text:"A 2017 survey conducted by *MIT Sloan Management Review* found that higher levels of data sharing with customers, suppliers, and competitors are associated with higher levels of innovation.(1) MIT CISR research finds that simply sharing data in old ways does not pay off. Instead, companies must share __new data__ in __new ways__ to realize the full value potential of digital opportunities.\n\nHistorically, companies have shared minimal dataand only when it was requiredto execute a transaction, solve a well-defined problem, or comply with regulation. They engaged in __data sharing 1.0__: *sharing transactional data assets and capabilities to enable or preserve an existing value proposition.*\n\nToday, companies are beginning to appreciate data sharing as an opportunity rather than an obligation, opening doors for new value creation. Companies that sell products through platform marketplaces see opportunity in data about the characteristics and behaviors of all platform consumers, including those who looked at their products but did not buy them. Companies that manufacture sensored products see opportunity in data about how and why their customers are using the productsand how customers are making or saving money from that use. Companies that amass rich data assets that reflect customer journeys, supply chain activities, and manufacturing operations see opportunity in start-ups, crowds, and partners who can contribute analytics, skills, leading-edge tech, and outside-the-box thinking to help solve hard problems and search for blue ocean ideas. In order to exploit these opportunities, companies need to engage in __data sharing 2.0__: *sharing complementary data assets and capabilities to create new value propositions.*\n\nFor the past three years, MIT CISR has investigated interorganizational data sharing at twenty-three distinct organizations.(2) We explored why and how companies share data, and how data sharing activities contribute to value and innovation from digital initiatives. This briefing illustrates several data sharing 2.0 use cases from the research, and describes three sets of data sharing practices that position companies to fully exploit digital opportunities.\n"},{id:3832,__component:f,text:"Complementary Data Assets and Capabilities",slug:"complementary-data-assets-and-capabilities"},{id:7135,__component:d,text:"The magic of data sharing 2.0 is rooted in the cross-company sharing of complementary data assets and\u002For capabilities, which fills data gaps and allows companies, often collaboratively, to develop innovative or transformative solutions.\n\nPepsiCo and its retail partners share data about consumer behavior and product categories to grow retailer sales. In one case, a retailer provided PepsiCo with data about its purchases of fountain drink syrup from other providers, which PepsiCo combined with data on its own sales to the retailer. Then, drawing on advanced analytics capabilities and insights from the companys rich trove of anonymized shopper data, PepsiCo identified attributes about sales, stores, and shoppers that influenced syrup usage. Analyzing the complementary data using machine learning produced specific action items intended to generate a win-win outcome for consumers, the retailer, and PepsiCo by optimizing sales of both the companys own and other providers beverages.(3)\n\nSchneider Electric and its business customers share data via Schneiders IoT platform EcoStruxure. The platform hosts data and algorithms that, when combined with customers sensor and operations data, optimize energy management for customers.(4) As a greater number of customers leverage the platforms data science capabilities, the underlying models learn and improve.(5) For example, energy is the second largest operational cost (after labor) for EcoStruxure customer Hilton Hotels & Resorts. Hilton has achieved three percent savings per year in energy procurement, and more in energy efficiency, using the Schneiders Ecostruxure Resource Adviser digital offering that shows real-time consumption and pricing data and helps optimize resource efficiency.(6)\n\nHealth insurance provider Anthem and a variety of health partners share data to co-create AI-based solutions for complex healthcare problems.(7) Anthem assembled a data set of twelve years of de-identified medical claims, prescription claims, and lab data associated with 45 million individuals. Then, the company made the data available using its Data Sandbox, a platform certified to legally and acceptably provision the data to vetted users. The health partnersacademic and commercial organizations and NGOscontributed algorithms, analysis techniques, and novel perspectives. The Data Sandbox currently hosts fifteen projects completed or in process.\n\nIn all of these examples, companies identified and created new value by unleashing the synergisitic combination of complementary data assets and capabilities."},{id:3830,__component:f,text:"Data Sharing 2.0 Accelerators",slug:"data-sharing-2-0-accelerators"},{id:7136,__component:d,text:"Although opportunities enabled by data sharing 2.0 can be incredibly exciting, companies hesitate to engage in interorganizational data sharing beyond requisite transactions. Among the reasons why: more and new data sharing comes with added cost and risk, particularly when sharing requires new systems and governance; the nature of data is sensitive (e.g., company intellectual property, personal health, financial performance); and the nature of desired data usage is novel (e.g., data wrapping).(8) It takes time for companies to figure out what people, processes, and technologies are required for safe, effective executionand to put them in place. MIT CISR research identified three sets of practices that help companies more easily and confidently accelerate data sharing 2.0: curated content, designated channels, and repeatable controls (see figure 1)."},{id:1885,__component:w,title:"BD CISR Figure 1",type:x,caption:"__FIGURE 1: DATA SHARING 2.0 ACCELERATORS AND ACTIVITIES__\nSource: 24 interviews with executives representing 23 companies, 20192020",image:{data:{id:1629,attributes:{name:"2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9.png",alternativeText:a,caption:a,width:1300,height:bm,formats:{large:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8.png",hash:"large_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8",mime:p,name:"large_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9.png",path:a,size:491.43,width:P,height:440},small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8.png",hash:"small_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8",mime:p,name:"small_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9.png",path:a,size:136.92,width:y,height:220},medium:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8.png",hash:"medium_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8",mime:p,name:"medium_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9.png",path:a,size:292.74,width:B,height:jR},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8.png",hash:"thumbnail_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8",mime:p,name:"thumbnail_2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9.png",path:a,size:41.57,width:aI,height:108}},hash:"2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8",ext:o,mime:p,size:151.05,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002F2020_1001_Data_Sharing_Wixom_Sebastian_Gregory_Figure1_ecd0ebd7b9_96a73b29b8.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:jS,updatedAt:jS}}}},{id:2063,__component:v,text:"Curated Content"},{id:7138,__component:d,text:"Creating, purposely assembling, and curating data for interorganizational sharing accelerates data sharing 2.0 by establishing a prepared set of material with which to innovate. Curation can include removing personally identifiable information, converting field values to conform with industry standard codes, compiling records into higher levels of aggregation, and extracting insights for sharing that exclude underlying data. Curation can also include pre-vetting the companys ownership of the data and its right to share the data for specified use cases.\n\nIn 2011, BBVA created a data set comprising four million anonymized credit card payment transactions for its innovation team to use in developing new information-based services. The company aggregated and masked the data and performed legal and compliance inspections on it so that the team could feel assured of its appropriateness for market-facing use. Initially, the static data set informed pilot solutions for urban planners and disaster recovery strategists.(9) Today, the data set is one of many that supports informational solutions available via the companys API market.(10)"},{id:2061,__component:v,text:"Designated Channels"},{id:7141,__component:d,text:"Establishing channels that enable secure data exchange accelerates data sharing 2.0 by simultaneously protecting data and easing data access and transfer. One way channels can manifest is as a __data platform__, either internal (e.g., an API-enabled data lake) or external (e.g., a third-party hosted platform). Another way is as __secure data movement__, whereby data is exchanged using blockchain technology or some form of masking and encryption. Yet another is as __data-at-rest__ sharing techniques, which manage access rights across tenants of a cloud offering or common platform (e.g., Snowflake),(11) or federate analysis such that only the results of queries and algorithms and not the underlying data are available to participants.\n\nIn December 2018, Maersk and IBM launched the TradeLens platform that was purpose-built to share data about global shipping events and documents, with the goal to facilitate the end-to-end journey of a container. The underlying blockchain architecture provides high levels of data security. Ocean carriers can host and manage a blockchain node and become trust anchors. The permissioning structure (i.e., who can access what data) is based on a United Nations standard, and only the hash value of commercially sensitive information (typically documents) is stored on the blockchain; authorized companies can see if information has changed without seeing the underlying data.(12) The TradeLens platform currently plays a key role in convincing diverse stakeholders, including ocean carriers that compete with Maersk, to engage in the collaborative ecosystem arrangement. As a result, approximately 175 ecosystem members (ocean carriers, ports and terminals, inland transportation, and customs authorities) and customers (importers, exporters, and freight forwarders) currently participate in the interorganizational data sharing.(13)\n"},{id:2062,__component:v,text:"Repeatable Controls"},{id:7142,__component:d,text:"Moving from time-consuming, bespoke governance measures to standard operating procedures for oversight accelerates data sharing by enabling alignment in data and outcomes across the company. Many executives in our research described a heavy reliance on their companys security and legal teams to help create processes, procedures, and activities that could be generalized across sharing relationships. In some of these cases, project teams captured lessons learned, exceptions, and repeatable steps to build an onboarding playbook used to guide (and expedite) subsequent data sharing arrangements.\n\nIn 2016, a collection of institutions, including the Australia and New Zealand Banking Group (ANZ), National Australia Bank (NAB), and Westpac invested in a company called Data Republic that has evolved to serve the market in a data sharing escrow capacity. Data Republic offers a wide range of data sharing oversight services, such as a legal framework and licensing workflows, governed data downloads and transfers, and full audit trails and reporting of sharing activities.(14) ANZ chose to use Data Republic for some of its own data cross-company sharing needs in part because of the latters stringent security and governance control. After six months of deployment, ANZ reported a reduction in time to negotiate data sharing agreements and methologies from eighteen weeks to one week and rapid creation of shared data assets to help ANZ uncover potential opportunities.(15)"},{id:3831,__component:f,text:"Primed for Opportunity",slug:"primed-for-opportunity"},{id:7139,__component:d,text:"Companies will increasingly encounter exciting digital opportunities, but not all companies will be poised to create value from them. MIT CISR research indicates that data sharing 2.0 accelerators can help companies break through cost, risk, and timing obstacles to focus on new value creation. Get set for opportunityperform accelerator activities and prepare to create value."},{id:3833,__component:f,text:"Footnotes",slug:"references"},{id:7140,__component:d,text:"1. Sam Ransbotham and David Kiron, Analytics as a Source of Business Innovation, MIT Sloan Management Review, February 2017, [https:\u002F\u002Fsloanreview.mit.edu\u002Fprojects\u002Fanalytics-as-a-source-of-business-innovation](https:\u002F\u002Fsloanreview.mit.edu\u002Fprojects\u002Fanalytics-as-a-source-of-business-innovation).\n2. The research is based on 24 interviews with executives representing 23 companies, conducted from 20192020. The analysis also draws on the June 2018 MIT CISR Data Research Advisory Board discussion; research on information businesses led by Barbara Wixom; and research on digital partnering and on value in digital business led by Ina Sebastian.\n3. B. H. Wixom, PepsiCo Unlocks Granular Growth Using a Data-Driven Understanding of Shoppers, MIT Sloan CISR Working Paper No. 439, December 2019, [https:\u002F\u002Fcisr.mit.edu\u002Fpublication\u002FMIT_CISRwp439_PepsiCoDX_Wixom](https:\u002F\u002Fcisr.mit.edu\u002Fpublication\u002FMIT_CISRwp439_PepsiCoDX_Wixom).\n4. J. W. Ross, C. M. Beath, and K. Moloney, Schneider Electric: Connectivity Inspires a Digital Transformation, MIT Sloan CISR Working Paper No. 417, May 2017, [https:\u002F\u002Fcisr.mit.edu\u002Fpublication\u002FMIT_CISRwp417_SchneiderElectricTransformation_RossBeathMoloney](https:\u002F\u002Fcisr.mit.edu\u002Fpublication\u002FMIT_CISRwp417_SchneiderElectricTransformation_RossBeathMoloney).\n5. AI - From big bang to business outcomes: Paving the way for artificial intelligence's real value, Schneider Electric, [https:\u002F\u002Fai.se.com](https:\u002F\u002Fai.se.com).\n6. Hilton Customer Story, Life Is On, Schneider Electric, [https:\u002F\u002Fwww.se.com\u002Feg\u002Fen\u002Fwork\u002Fcampaign\u002Flife-is-on\u002Fcase-study\u002Fhilton.jsp](https:\u002F\u002Fwww.se.com\u002Feg\u002Fen\u002Fwork\u002Fcampaign\u002Flife-is-on\u002Fcase-study\u002Fhilton.jsp).\n7. Anthem AI Digital Data Sandbox, Anthem, Inc., [https:\u002F\u002Fwww.anthem.ai\u002Fsandbox](https:\u002F\u002Fwww.anthem.ai\u002Fsandbox).\n8. Barbara H. Wixom, Ronny M. Schritz, and Killian Farrell, Why Smart Companies Are Giving Customers More Data, MIT Sloan Management Review, May 19, 2020, [https:\u002F\u002Fsloanreview.mit.edu\u002Farticle\u002Fwhy-smart-companies-are-giving-customers-more-data\u002F](https:\u002F\u002Fsloanreview.mit.edu\u002Farticle\u002Fwhy-smart-companies-are-giving-customers-more-data\u002F).\n9. E. Alfaro, J. Murillo, F. Girardin, B. H. Wixom, and I. A. Someh, BBVA Fuels Digital Transformation Progress with a Data Science Center of Excellence, MIT Sloan CISR Working Paper No. 430, April 2018, [https:\u002F\u002Fcisr.mit.edu\u002Fpublication\u002FMIT_CISRwp430_BBVADataScienceCoE_AlfaroMurilloGirardinWixomSomeh](https:\u002F\u002Fcisr.mit.edu\u002Fpublication\u002FMIT_CISRwp430_BBVADataScienceCoE_AlfaroMurilloGirardinWixomSomeh).\n10. API Catalog, BBVA API Market, [https:\u002F\u002Fwww.bbvaapimarket.com\u002Fen\u002Fbanking-apis\u002F](https:\u002F\u002Fwww.bbvaapimarket.com\u002Fen\u002Fbanking-apis\u002F).\n11. Snowflake: The Cloud Data Platform, Snowflake, [https:\u002F\u002Fwww.snowflake.com\u002F](https:\u002F\u002Fwww.snowflake.com\u002F).\n12. Thomas Jensen, Jonas Hedman, and Stefan Henningsson, How TradeLens Delivers Business Value with Blockchain Technology, MIS Quarterly Executive Vol. 18: Iss. 4, Article 5, [https:\u002F\u002Faisel.aisnet.org\u002Fmisqe\u002Fvol18\u002Fiss4\u002F5](https:\u002F\u002Faisel.aisnet.org\u002Fmisqe\u002Fvol18\u002Fiss4\u002F5).\n13. Digitizing the Global Supply Chain, TradeLens, [https:\u002F\u002Fwww.tradelens.com\u002Fabout](https:\u002F\u002Fwww.tradelens.com\u002Fabout).\n14. About Data Republic, Data Republic, [https:\u002F\u002Fwww.datarepublic.com\u002Fanthem-onboarding\u002Fabout-data-republic](https:\u002F\u002Fwww.datarepublic.com\u002Fanthem-onboarding\u002Fabout-data-republic).\n15. Data sharing technology unlocks analytics insights for ANZ, Data Republic, [https:\u002F\u002Fwww.datarepublic.com\u002Fcase-study-anz](https:\u002F\u002Fwww.datarepublic.com\u002Fcase-study-anz).\n"}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}},{id:bm,attributes:{title:eh,slug:ei,status:b,publishedOn:an,lastUpdated:an,readingTime:ai,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ao,updatedAt:ao,publishedAt:ej,locale:e,pdf:{data:a},logo:{data:{id:1627,attributes:{name:"SMR_logo_3308d9b6cd.jpeg",alternativeText:a,caption:a,width:jT,height:jU,formats:{small:{ext:gH,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_SMR_logo_3308d9b6cd_8a55d0f819.jpeg",hash:"small_SMR_logo_3308d9b6cd_8a55d0f819",mime:h,name:"small_SMR_logo_3308d9b6cd.jpeg",path:a,size:jV,width:y,height:if0},thumbnail:{ext:gH,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_SMR_logo_3308d9b6cd_8a55d0f819.jpeg",hash:"thumbnail_SMR_logo_3308d9b6cd_8a55d0f819",mime:h,name:"thumbnail_SMR_logo_3308d9b6cd.jpeg",path:a,size:jW,width:aI,height:ig}},hash:"SMR_logo_3308d9b6cd_8a55d0f819",ext:gH,mime:h,size:jX,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FSMR_logo_3308d9b6cd_8a55d0f819.jpeg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jY,updatedAt:jY}}},contentBlocks:[{id:7128,__component:d,text:"Data science, including analytics, big data, and artificial intelligence, is no longer a novel concept. Nor is the important foundation of high-quality data. Both have contributed to impressive business successes  particularly among digital natives  yet overall progress among established companies has been painfully slow. Not only is the failure rate high, but companies have also proved unable to leverage successes in one part of the business to reap benefits in other areas. Too often, progress depends on a single leader, and it slows dramatically or reverses when that individual departs the company. In addition, companies are not seizing the strategic potential in their data. Wed estimate that less than 5% of companies use their data and data science to gain an effective competitive edge.\n\nOver the years, we have worked with dozens of companies on their data journeys, advising them on the approaches, techniques, and organizational changes needed to succeed with data, including quality, data science, and AI. From our perspective, these are the two biggest mistakes organizations make:"},{id:1150,__component:aM,items:"1. They underinvest in the organization (people, structure, or culture), process, and the strategic transformations needed to get on offense  in other words, to take full advantage of their data and the data analytics technologies at their disposal. \n2. They address data quality improperly, which leads them to waste critical resources (time and money) dealing with mundane issues. Bad data, in turn, breeds mistrust in the data, further slowing efforts to create advantage."},{id:7129,__component:d,text:"Although the details at each company differ, seeing data too narrowly  as the province of IT or the data science organization, not of the entire business  is a recurring theme. This causes companies to overlook the transformative potential in data and therefore underinvest in the organizational, process, and strategic changes cited above. Similarly, they blame technology for their quality woes and failures to capitalize on data, when the real problem is poor management.\n\nWeve all observed [how companies behave](https:\u002F\u002Fwww.innosight.com\u002Finsight\u002Fthe-transformation-20\u002F) when they are truly serious about something  how the goal changes from incremental progress to rapid transformation; how they muster both breadth and depth of resources; how they align and train people; how they communicate new values and new ways of working; and how senior leaders drive the effort. Indeed, it almost seems as if companies go overboard when they are truly serious about something. Amazons [Project D](https:\u002F\u002Fwww.bloomberg.com\u002Ffeatures\u002F2016-amazon-echo\u002F) initiative to develop the Echo\u002FAlexa smart speaker is a great illustration of that seriousness, with hundreds of employees, several startup acquisitions, heavy CEO involvement, and no expense spared. DBS Banks journey to being named [Worlds Best Digital Bank](https:\u002F\u002Fwww.dbs.com\u002Fnewsroom\u002FDBS_named_Worlds_Best_Digital_Bank_2018) by Euromoney is another good example. The companys CEO, Piyush Gupta, said the following upon receiving that award in 2018:"},{id:ig,__component:hn,text:"At DBS, we believe that banks tomorrow will look fundamentally different from banks today. Thats why we have spent the past three years deeply immersed in the digital agenda. This has been an all-encompassing journey, whether it is changing the culture and mindsets of our people, re-architecting our technology infrastructure, or leveraging big data, biometrics, and AI to make banking simple and seamless for customers."},{id:7131,__component:d,text:"The contrast with most companies data programs is stark  one can only conclude that many are not yet serious about data and data science. For those only beginning to explore data, this may be understandable. But, if youve been at it for three years or more, it is time to either get serious in addressing mistakes or invest your resources elsewhere  and expect to lose out to competitors."},{id:3825,__component:f,text:"Stop Wasting Effort on Data Quality",slug:"stop-wasting-effort-on-data-quality"},{id:7132,__component:d,text:"The obvious approach to addressing these mistakes is to identify wasted resources and reallocate them to more productive uses of data. This is no small task. While there may be budget items and people assigned to support analytics, AI, architecture, monetization, and so on, there are no budgets and people assigned to waste time and money on bad data. Rather, this is hidden away in day-in, day-out work  the salesperson who corrects errors in data received from marketing, the data scientist who spends 80% of his or her time wrangling data, the finance team that spends three-quarters of its time reconciling reports, the decision maker who doesnt believe the numbers and instructs his or her staff to validate them, and so forth. Indeed, almost all work is [plagued by bad data](https:\u002F\u002Fhbr.org\u002F2017\u002F09\u002Fonly-3-of-companies-data-meets-basic-quality-standards).\n\nThe secret to wasting less time and money involves changing ones approach from the current buyer\u002Fuser beware mentality, where everyone is left on their own to deal with bad data, to creating data correctly  [at the source](https:\u002F\u002Fhbr.org\u002F2020\u002F02\u002Fto-improve-data-quality-start-at-the-source). This works because finding and eliminating a single root cause can prevent thousands of future errors and eliminate the need to correct them downstream. This saves time and money  lots of it! The [cost of poor data](https:\u002F\u002Fsloanreview.mit.edu\u002Farticle\u002Fseizing-opportunity-in-data-quality\u002F) is on the order of 20% of revenue, and much of that expense can be eliminated permanently. Thats more than enough to fund the needed investments."},{id:3826,__component:f,text:"Get On Offense",slug:"get-on-offense"},{id:7130,__component:d,text:"Now consider the budgets for AI (as an example of offense-minded data efforts). It appears to us that, in many cases, the data science work to develop a new algorithm is funded well enough. Algorithm development is getting cheaper anyway, given that automated machine learning programs are doing more of the work. But useful algorithms die on the vine because the work to build processes, train people, address fear of change, and adapt the culture is substantially underfunded. Based on our experience, a good rule of thumb is that you should estimate that for every $1 you spend developing an algorithm, you must spend $100 to deploy and support it. A few of these dollars will go to building algorithms into work processes, and many more to training, [building a culture that embraces data](https:\u002F\u002Fhbr.org\u002F2019\u002F10\u002Fbuilding-a-culture-that-embraces-data-and-ai), and change management. Most companies arent spending this money yet, and it explains their lack of production AI deployments."},{id:3827,__component:f,text:"Make Bold Moves",slug:"make-bold-moves"},{id:7133,__component:d,text:"What tangible steps should business leaders take to demonstrate that they are serious about data? First, they should more tightly couple their business and data strategies with an eye toward driving revenue growth. From the data perspective, opportunity abounds in [fully exploiting proprietary data](https:\u002F\u002Fhbr.org\u002F2020\u002F05\u002Fyour-organization-needs-a-proprietary-data-strategy), driving analytics into every nook and cranny of the company, and augmenting virtually every decision using AI. You cannot  and should not  do them all, so you must select those most closely aligned with your business strategies. One sign that youre on the right track is that there will be fewer data efforts. But those you do have will be far larger, more comprehensive, and more closely managed.\n\nSecond, get everyone fully engaged. After all, everyone is technically involved in your data efforts already. They interpret data correctly, or they do not; they create data correctly, or not; they use data to improve their work, or not; and they contribute to larger data initiatives, or not. Today, there are far too many nots. Similarly, managers push back against the nots, or they do not, and more senior leaders get in front of them, or not. So you must reach out to people, educate them, and enroll them in the effort, even as you grow increasingly intolerant of the inefficiencies stemming from bad data. This is going to take some time. One sign that youre on the right track is that morale will improve. In our experience, once people get the hang of it, most of them [find data work quite enjoyable](https:\u002F\u002Fhbr.org\u002F2019\u002F10\u002Fmost-analytics-projects-dont-require-much-data). Importantly, in the data space, talent wins.\n\nThird, draw a clear distinction between the management of data and the management of technology. Just as a movie is a different sort of asset than streaming technology, data and tech are different sorts of assets. Each demands its own specialized management. Yet today, too many companies subordinate data to tech. The result is that topics such as data architecture do not get the attention they deserve, leading to such absurdities as a bank having 130,000 databases, not including spreadsheets. Meanwhile, technology programs spend too much time dealing with the consequences of having systems that dont talk to one another and spending too little time introducing new technologies to employees. One sign that youre on the right track is that technology departments will become more effective and, in time, strategic.\n\nFinally, now is a good time to start thinking about the longer-term roles data will play in your company. It is easy enough to recite the mantra Data is the new oil. And according to The Economist, [data is now worth up to $2 trillion](https:\u002F\u002Fwww.economist.com\u002Fspecial-report\u002F2020\u002F02\u002F20\u002Fa-deluge-of-data-is-giving-rise-to-a-new-economy) in the U.S. alone. But, of course, not all data is created equally. Some data  such as proprietary data, data needed to run the company, and data associated with other key assets  is so important that it should be treated as an asset in its own right. At the very least, you should make sure that end-to-end accountabilities for this data are clear.\n\nWe fully recognize how challenging these recommendations will prove to be. Yet they signal great opportunity, especially for the first companies in their sectors to embrace them. The needed approaches, methods, and technologies are widely available and have proved themselves over and over among digital natives and at the department level for established companies. It is clear enough that the future depends on data, so sooner or later, you have no real choice. As in all things, *audentes Fortuna iuvat*  fortune favors the brave."}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}},{id:ek,attributes:{title:el,slug:em,status:b,publishedOn:ap,lastUpdated:ap,readingTime:en,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:aq,updatedAt:aq,publishedAt:eo,locale:e,pdf:{data:a},logo:{data:{id:1626,attributes:{name:"SMR_logo_ac5373cca8.jpeg",alternativeText:a,caption:a,width:jT,height:jU,formats:{small:{ext:gH,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_SMR_logo_ac5373cca8_460cc7bf1f.jpeg",hash:"small_SMR_logo_ac5373cca8_460cc7bf1f",mime:h,name:"small_SMR_logo_ac5373cca8.jpeg",path:a,size:jV,width:y,height:if0},thumbnail:{ext:gH,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_SMR_logo_ac5373cca8_460cc7bf1f.jpeg",hash:"thumbnail_SMR_logo_ac5373cca8_460cc7bf1f",mime:h,name:"thumbnail_SMR_logo_ac5373cca8.jpeg",path:a,size:jW,width:aI,height:ig}},hash:"SMR_logo_ac5373cca8_460cc7bf1f",ext:gH,mime:h,size:jX,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FSMR_logo_ac5373cca8_460cc7bf1f.jpeg",previewUrl:a,provider:j,provider_metadata:a,createdAt:jZ,updatedAt:jZ}}},contentBlocks:[{id:7122,__component:d,text:"The days of companies wondering whether they should jump on the data analytics bandwagon, or having a single data analyst on staff, are gone. For firms today, the focus has turned to building the right team to fully harness all that data has to offer.\n\nAs companies add to their data teams, analytics jobs are increasingly popular  data scientist and data engineer were both in the top 10 of LinkedIns 2020 [Emerging Jobs Report](https:\u002F\u002Fbusiness.linkedin.com\u002Fcontent\u002Fdam\u002Fme\u002Fbusiness\u002Fen-us\u002Ftalent-solutions\u002Femerging-jobs-report\u002FEmerging_Jobs_Report_U.S._FINAL.pdf), determined by earnings potential, job satisfaction, and number of job openings. Companies are expected to continue to add and grow data teams amid the rise of the internet of things, artificial intelligence, and machine learning.\n\nRyan Sutton, a district president for tech staffing firm Robert Half Technology and The Creative Group, said more companies are embracing what he calls a highly organized data approach. A lot of larger companies, for example insurance companies, have a progressive approach to data overall, and the structure of their organization is focused on data  a data team, we could even argue, he said. \\[Companies are] even creating dedicated org charts specifically to tackle this, whether that is a department or whether it is a group of people that are allocated to different departments that collaborate frequently on data.\n\nThe overarching trend in data in the last 10 is just growth, he added. It's how it's evolved and how it's grown.\n\nDigital roles are changing along with increased adoption and advances in technology, and titles are imprecise as different companies and sectors use different names for similar jobs. Some might have one go-to digital analyst who does a range of tasks, while others are on their way to building a team of experts in different areas.\n\nAs companies look at fully embracing data and ramping up their analytics departments, creating the right operation is key. Here are some key roles to consider when building a data dream team."},{id:3820,__component:f,text:"Data engineer",slug:"data-engineer"},{id:7123,__component:d,text:"Data engineers are a core part of a data analytics operation. Engineers collect and manage data, and manage storage of the data. Their work is the foundation of a data operation as they take large amounts of raw data and prepare it for others who make business decisions, write prediction algorithms, and the like.\n\nI would really want a pair of really, really good data engineers. Those people are the heart and guts of your pipeline, said Althea Davis, the enterprise data governance manager at Etihad Aviation Group, who previously worked in senior data governance positions at Heineken and Ing bank.\n\nData engineers tend to have software engineering or computer science backgrounds, according to Michelle Li, the program director of MIT Sloans [Master of Business Analytics](https:\u002F\u002Fmitsloan.mit.edu\u002Fmaster-of-business-analytics) program and former director of the Global Technology Group at UBS Investment Bank. The data engineers are really the backbone, she said. If you're building a house, they're the structural engineer."},{id:3823,__component:f,text:"Data scientist",slug:"data-scientist"},{id:7125,__component:d,text:"While engineers maintain the data, data scientists figure out what to do with it. The two positions are the starting point for most companies with big data strategies, Sutton said.\n\nData scientists are highly sought positions. I would say data scientist is the most popular title that students want coming straight from a program like ours, Li said. Some companies have changed traditional roles like operations research, marketing, or data analyst into the data scientist or research scientist title, she said.\n\nData scientists tend to be a bit more business-facing, while data engineers focus more on infrastructure, scale, and data quality, said Tim Valicenti, a 2018 graduate of the MIT Sloan MBAn program and senior analyst at McKinsey, a role that blends data scientist with classic management consultant. What kind of insights can I draw from the data, and where can I bring in the business acumen, he said.\n\nData scientists also use artificial intelligence and machine learning to drive analytics and derive insights.\n\nWhile traditionally research and data scientists had PhDs, that is no longer a requirement of the job, Li said. In fact, students leaving the MIT Sloan business analytics program often get jobs with scientist in the title.\n\nSutton said that change can mean grouping people with different skills under the same title. I think even data scientist sometimes gets misunderstood, where there are true, proper, data scientists, and then there are very good data-savvy people, he said."},{id:3821,__component:f,text:"Data translators",slug:"data-translators"},{id:7127,__component:d,text:"While data engineers and data scientists are established roles, several newer titles are taking analytics operations to the next level  like data translators, who serve as a bridge of sorts between data and traditional business operations by translating the insights gained from analytics into actions the companies can take to gain value.\n\nA data translator is somebody who understands the algorithms, who understands what the data scientist does, but then uses that in the business setting, Li said. We've seen a lot of MBA students or managers who don't necessarily have the deep-dive math or analytic skills, but they are able to provide business insight. They still have to know roughly what the models do to be a translator, but they don't have to do the programming themselves.\n\nAt McKinsey, Valicenti said, data translators fall between data science consultants and classic consultants as they manage a data science team and communicate needs and results to clients in other parts of the organization, like executives. You can't really expect C-suites to know, or need to know, all this kind of data science jargon and machine learning, he said. \\[Executives] know what machine learning is on a high level, but they don't always have the time to learn what the details are. Translators are necessary to bridge communication between executive stakeholders and analytics-heavy teams.\n\nAs companies invest more resources and become more reliant on data, some translators can also take on training roles and educate others about how to use data. They will train the field team on how to leverage the reports, how to understand the reports, how to somewhat lightly customize the reports, Sutton said. So those data translators, I think, are really just a glimpse of how companies have evolved.\n\nLike other analytics jobs, the data translator role is known by other names, in this case data curator or [data storyteller](https:\u002F\u002Fmitsloan.mit.edu\u002Fideas-made-to-matter\u002Fnext-chapter-analytics-data-storytelling). I wouldn't be surprised if we start hearing data whisperer, Sutton said."},{id:3824,__component:f,text:"Knowledge engineers, ontologists, and more",slug:"knowledge-engineers-ontologists-and-more"},{id:7126,__component:d,text:"Evolutions in companies technical capabilities and the way they use data are leading to new roles. Some build off existing roles and add in new focuses, like machine learning engineers  data engineers with specific knowledge in machine learning and artificial intelligence.\n\nOne of the hot titles that I would grab off the market immediately is a title called knowledge engineer, said Etihad Aviations Davis. Knowledge engineers build intelligence into computer systems  they create brains, of a sort, that can mimic human decisions. Beyond statistics, knowledge engineers  or a similar title, ontologists  work on [providing semantics](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Fkalevleetaru\u002F2019\u002F01\u002F15\u002Fwhy-machine-learning-needs-semantics-not-just-statistics\u002F#4a20b91077b5): how the information relates to the rest of the world.\n\nWhile traditional analytics allow companies to analyze past trends and events, ontologists take broader view, acting as a sort of company brain that takes the outcomes of analytics findings and combines it with information from inside and outside the company to answer a question.\n\nNotions reside in the minds of many staffers. Ontologists help include the edge data citizen who has notions to be brought into the fold, Davis said. It is these bits of knowledge from the larger corporate audience who never get heard in traditional data management.\n\nThey take the whole natural language questioning that business has to a whole other level. They actually consume the results of advanced analytics into their knowledge graphs and ontologies and come up with real answers to business questions, she said.\n\nYou and I would logically say, I wonder when this COVID-19 thing is going to end and people are going to start flying again. With a good knowledge graph and ontology  ontology that allows your brain, your company brain, to communicate with all the knowledge of the world if it wanted to  you would be able to answer that question, Davis said. Not in the sense that it's 100% absolutely right, but you would actually be able to get to something much faster than another company that's still working on the paradigm of hindsight-focused business intelligence."},{id:3822,__component:f,text:"So, whos going to lead?",slug:"so-whos-going-to-lead"},{id:7124,__component:d,text:"Any team needs a leader, and so far companies are taking varied approaches when it comes to who is in charge of their data operation. Some companies are [adding chief data officers](https:\u002F\u002Fmitsloan.mit.edu\u002Fideas-made-to-matter\u002Fmake-room-executive-suite-here-comes-cdo-2-0), or even chief data analytics officers, to the C-suite. According to a 2020 [NewVantage Partners survey](http:\u002F\u002Fnewvantage.com\u002Fwp-content\u002Fuploads\u002F2020\u002F01\u002FNewVantage-Partners-Big-Data-and-AI-Executive-Survey-2020-1.pdf) of more than 70 executives from Fortune 1000 and other leading companies, about 57% of the companies had appointed a designated chief data\u002Fdata analytics officer. Yet only 28% of the respondents said that role is settled and established, and about 27% said there is no single point of accountability for data. There was some discrepancy between types of organizations, with 64% of [financial firms appointing a CDO](https:\u002F\u002Fmitsloan.mit.edu\u002Fideas-made-to-matter\u002F3-challenges-chief-data-officers-finance), but only 48% of life sciences companies.\n\nSome chief information officers are also in charge of information security, a growing concern for companies that amass a great deal of data. Stephanie Balouras, a cybersecurity expert with Forrester, [told MIT Technology Review](https:\u002F\u002Fwww.technologyreview.com\u002F2020\u002F02\u002F24\u002F276013\u002Fcybersecurity-in-2020-the-rise-of-the-ciso\u002F) that at some companies, especially smaller ones, information security is also the responsibility of the CIO or another IT executive. Other companies appoint a chief information security officer  publicly traded companies are required to have one. Balouras said companies should consider filling this role, and that it should report to others high up in the organization.\n\nAt some organizations, data teams report to the chief administrative officer or chief operating officer, Sutton said, while others put general vice presidents of technology or other leaders in charge, sometimes with subset titles like business technology, automation, digital transformation, or technology development. Youll have chief digital officers, youll have general vice presidents of technology, he said. The titles can vary from company to company."}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}}]},moreBeyondMIT:{data:[{id:ep,attributes:{title:eq,slug:er,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:ar,updatedAt:ar,publishedAt:es,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:j_,__component:gQ,title:"Data Science Foundations",url:"https:\u002F\u002Fcognitiveclass.ai\u002Flearn\u002Fdata-science",onDemand:i,startDate:a,duration:"Approximately 10 hours",description:"A three-course introduction to data science, covering the basic principles of data analysis, data analysis workflow, and common data science tools.\n\nCourse by IBM Cognitive Class.",price:j$,image:{data:a}},{id:232,__component:gQ,title:"Big Data Fundamentals",url:"https:\u002F\u002Fcognitiveclass.ai\u002Flearn\u002Fbig-data",onDemand:i,startDate:a,duration:"Approximately 12 hours",description:"Three courses introducing basic big data concepts and terms, as well as the popular Hadoop and Spark platforms for data analysis.\n\nCourse by IBM Cognitive Class.",price:j$,image:{data:a}},{id:235,__component:gQ,title:"Big Data Specialization",url:"https:\u002F\u002Fwww.coursera.org\u002Fspecializations\u002Fbig-data",onDemand:i,startDate:a,duration:"Approximately 8 months",description:"A series of six courses aimed at students without programming experience. Uses pre-written code for popular data analysis tools and covers how to store and access large data sets and how to use statistics and machine learning to analyze them. Culminates in a project where learners explore, analyze, and report on a large data set.\n\nCourse by UC San Diego, hosted on Coursera.",price:ka,image:{data:a}},{id:237,__component:gQ,title:"Executive Data Science Specialization",url:"https:\u002F\u002Fwww.coursera.org\u002Fspecializations\u002Fexecutive-data-science",onDemand:i,startDate:a,duration:kb,description:"A five-course introduction to big data for executives. Courses cover the basic principles of data analysis and how to build and manage an effective team of data scientists.\n\nCourse by Johns Hopkins University, hosted on Coursera.",price:"$39 per month",image:{data:a}},{id:234,__component:gQ,title:"Data Engineering, Big Data, and Machine Learning on GCP Specialization",url:"https:\u002F\u002Fwww.coursera.org\u002Fspecializations\u002Fgcp-data-machine-learning",onDemand:i,startDate:a,duration:kb,description:"A five-course introduction to using Google Cloud Platform for big data analytics. Courses cover storing and accessing data on the cloud, applying machine learning to data, and creating a full data analysis pipeline using real-world tools.\n\nCourse by Google Cloud, hosted on Coursera.",price:ka,image:{data:a}},{id:236,__component:gQ,title:"Data Science and Big Data Analytics: Making Data-Driven Decisions",url:"https:\u002F\u002Flearn-xpro.mit.edu\u002Fdata-science",onDemand:c,startDate:"2021-01-25T05:00:00.000Z",duration:"7 weeks",description:"An introduction to data science techniques led by MIT faculty with regular projects. For managers and analysts with background knowledge of statistics and preferably familiarity with programming languages Python and R.\n\nCourse by MIT xPro.",price:"$899",image:{data:a}},{id:233,__component:gQ,title:"Analytics Management: Business Lessons from the Sports Data Revolution",url:"https:\u002F\u002Fexecutive.mit.edu\u002Fopenenrollment\u002Fprogram\u002Fanalytics-management-business-lessons-from-the-sports-data-revolution\u002F",onDemand:c,startDate:"2021-04-22T05:00:00.000Z",duration:"2 days",description:"A guide for executives on how to create a data analytics strategy. Uses the sports industrys success with data analytics to explore how to create a data analytics team and make data-driven decisions.\n\nCourse by MIT Sloan School of Management.",price:"$3,900",image:{data:a}}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}},{id:et,attributes:{title:eu,slug:ev,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:as,updatedAt:as,publishedAt:ew,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:3812,__component:f,text:"General",slug:"general"},{id:id,__component:gV,title:aL,slug:aW,author:"John D. Kelleher and Brendan Tierney",year:aX,pages:"280",publisher:kc,publisherURL:"https:\u002F\u002Fmitpress.mit.edu\u002Fbooks\u002Fdata-science",amazonRating:a,amazonURL:a,goodreadsRating:a,goodreadsURL:a,description:"An introduction to data science. Covers the history of the field, the basics of how data science works, and ethical and legal challenges of working with big data.",alternatives:a,prerequisites:hw,image:{data:a}},{id:kd,__component:gV,title:"Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are",slug:"everybody-lies-big-data-new-data-and-what-the-internet-can-tell-us-about-who",author:"Seth Stephens-Davidowitz",year:ke,pages:"352",publisher:"Dey Street Books",publisherURL:"https:\u002F\u002Fwww.harpercollins.com\u002Fproducts\u002Feverybody-lies-seth-stephens-davidowitz?variant=32123670822946",amazonRating:a,amazonURL:a,goodreadsRating:a,goodreadsURL:a,description:"An exploration of how data analysis can reveal surprising information about peoples subconscious opinions and desires. Combines anecdotes with a deeper discussion of how powerful data analysis can really be.",alternatives:a,prerequisites:hw,image:{data:a}},{id:409,__component:gV,title:"Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy",slug:"weapons-of-math-destruction-how-big-data-increases-inequality-and-threatens",author:"Cathy ONeil",year:L,pages:"272",publisher:"Crown",publisherURL:"https:\u002F\u002Fweaponsofmathdestructionbook.com\u002F",amazonRating:a,amazonURL:a,goodreadsRating:a,goodreadsURL:a,description:"An exploration of the potential harms of data-driven decision making that was longlisted for the National Book Award. Demonstrates that biased algorithms, flawed data gathering, and imperfect application in the wrong fields can increase inequality and reduce individual freedom. ",alternatives:a,prerequisites:hw,image:{data:a}},{id:3814,__component:f,text:"Technical",slug:"technical"},{id:410,__component:gV,title:"Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems",slug:"designing-data-intensive-applications-the-big-ideas-behind-reliable-scalable",author:"Martin Kleppmann",year:ke,pages:"616",publisher:"O'Reilly",publisherURL:"https:\u002F\u002Fwww.oreilly.com\u002Flibrary\u002Fview\u002Fdesigning-data-intensive-applications\u002F9781491903063\u002F",amazonRating:a,amazonURL:a,goodreadsRating:a,goodreadsURL:a,description:"A guide to building software that can handle large amounts of data. Describes popular online tools for storing and processing data and compares their pros and cons, exploring the tradeoffs between size, efficiency, and reliability.",alternatives:a,prerequisites:"Experience building web-based services and familiarity with relational databases",image:{data:a}},{id:kf,__component:gV,title:"Mathematics of Big Data: Spreadsheets, Databases, Matrices, and Graphs",slug:"mathematics-of-big-data-spreadsheets-databases-matrices-and-graphs",author:"Jeremy Kepner and Hayden Jananthan ",year:aX,pages:"448",publisher:kc,publisherURL:"https:\u002F\u002Fmitpress.mit.edu\u002Fbooks\u002Fmathematics-big-data",amazonRating:a,amazonURL:a,goodreadsRating:a,goodreadsURL:a,description:"An introduction to the mathematics of big data analysis geared toward data scientists. Explores how different data analytics tools are rooted in similar mathematical concepts, and how these concepts can be used to solve data analytics problems across industries.",alternatives:a,prerequisites:"Some familiarity with computer science and linear algebra",image:{data:a}},{id:3813,__component:f,text:"Business",slug:"business"},{id:411,__component:gV,title:"Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die",slug:"predictive-analytics-the-power-to-predict-who-will-click-buy-lie-or-die",author:"Eric Siegel",year:L,pages:"368",publisher:"Wiley",publisherURL:"https:\u002F\u002Fwww.wiley.com\u002Fen-us\u002FPredictive+Analytics%3A+The+Power+to+Predict+Who+Will+Click%2C+Buy%2C+Lie%2C+or+Die%2C+Revised+and+Updated-p-9781119153658",amazonRating:a,amazonURL:a,goodreadsRating:a,goodreadsURL:a,description:"An accessible introduction to predictive analytics, examining through case studies how businesses and organizations use data to predict and change behavior.",alternatives:a,prerequisites:hw,image:{data:a}}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}},{id:ex,attributes:{title:ey,slug:ez,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:at,updatedAt:at,publishedAt:eA,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:160,__component:ih,title:"ODSC East 2021",slug:"odsc-east-2021",url:"https:\u002F\u002Fodsc.com\u002Fboston\u002F",location:"Boston, MA",description:"This comprehensive Open Data Science Conference is for data science practitioners. It emphasizes bringing together the data science community to explore the full breadth of topics and tools. Features 13 focus areas, hands-on training sessions for data science tools, and networking sessions.",audience:"Data scientists and researchers.",price:"$99$1,716",date:"May 911, 2023",image:{data:a}},{id:158,__component:ih,title:"Enterprise Data World",slug:"enterprise-data-world",url:"https:\u002F\u002Fedw2022digital.dataversity.net\u002Findex.cfm",location:"Virtual",description:"A data management conference that emphasizes training audience members at different proficiency levels. Features talks, tutorials, and workshops at multiple levels including business\u002Fnontechnical, introductory, and intermediate. Conference presented by Dataversity, a producer of educational resources on data management.",audience:"Data analytics professionals and managers.",price:"$499$1,499",date:"March 27-31, 2023",image:{data:a}},{id:159,__component:ih,title:"World Data Summit",slug:"world-data-summit",url:"https:\u002F\u002Fworlddatasummit.com\u002F",location:"Amsterdam",description:"A conference for senior data analysts, managers, and executives. Speakers and workshops will teach attendees about data analysis and visualization, as well as about how to translate data analysis into better company strategy.",audience:"Senior data analysts, managers, and executives.",price:"4951,595 (roughly $590$1,900)",date:"May 17-19. 2023",image:{data:a}}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}},{id:eB,attributes:{title:eC,slug:eD,status:b,publishedOn:g,lastUpdated:g,readingTime:a,owner:a,lead:a,author:a,authorURL:a,authorBiography:a,featuredDescription:a,createdAt:au,updatedAt:au,publishedAt:eE,locale:e,pdf:{data:a},logo:{data:a},contentBlocks:[{id:3815,__component:f,text:"Articles",slug:"articles"},{id:933,__component:aT,title:"Calls Mount to Ease Big Techs Grip on Your Data",description:"A news story on the growing calls to regulate how technology companies gather and use personal data. Politicians and academics are voicing various concerns about how data collection impacts individual privacy. In addition, selling personal data is a way for companies to make money. Some believe individuals should be compensated when their data generates revenue.",url:"https:\u002F\u002Fwww.nytimes.com\u002F2019\u002F07\u002F25\u002Fbusiness\u002Fcalls-mount-to-ease-big-techs-grip-on-your-data.html",metadata:"- The New York Times\n- 2019\n"},{id:941,__component:aT,title:"How to Actually Put Your Data Analytics to Good Use",description:"An overview of how companies can develop a data analytics strategy, authored by data specialists. The article covers everything from getting started to getting buy-in from employees.",url:"https:\u002F\u002Fhbr.org\u002F2018\u002F10\u002Fhow-to-actually-put-your-data-analysis-to-good-use?ab=at_articlepage_recommendedarticles_bottom1x1",metadata:"- Harvard Business Review\n- 2018\n"},{id:934,__component:aT,title:"How Big Data Harms Poor Communities",description:"This article argues that big data analysis by police forces and welfare programs is increasingly limiting the individual freedom of people in poor communities in the United States.",url:"https:\u002F\u002Fwww.theatlantic.com\u002Ftechnology\u002Farchive\u002F2016\u002F04\u002Fhow-big-data-harms-poor-communities\u002F477423\u002F",metadata:"- The Atlantic\n- 2016\n"},{id:3816,__component:f,text:"Industry Reports",slug:"industry-reports"},{id:937,__component:aT,title:"Top 10 Trends in Data and Analytics, 2020",description:"An overview of trends in data analysis, data visualization, and data-driven decision making in industry.",url:"https:\u002F\u002Fwww.gartner.com\u002Fdoc\u002Freprints?id=1-2411F2KU&ct=200827&st=sb?utm_campaign=WREC_200928_ac1216_Top-10-Trends-in-Data-and-Analytics-2020.02.Converted",metadata:"- Gartner\n- May 2020\n"},{id:3817,__component:f,text:"Newsletters",slug:"newsletters"},{id:943,__component:aT,title:"OReilly Data and AI",description:"A list and synopsis of news stories about recent developments in AI and data science, plus listings of educational opportunities and resources.",url:"https:\u002F\u002Fwww.oreilly.com\u002Femails\u002Fnewsletters\u002F",metadata:"- Weekly"},{id:938,__component:aT,title:"The Week in Data",description:"A newsletter from the Open Data Institute, a nonprofit organization promoting open access to data. Contains a roundup of stories covering recent developments in AI and data science, with an emphasis on data analysis for the public good.",url:"https:\u002F\u002Ftheodi.org\u002Fknowledge-opinion\u002Fthe-week-in-data\u002F",metadata:kg},{id:940,__component:aT,title:"Data Elixir",description:"A newsletter aimed at data scientists and engineers, presenting links from around the web on whats new in data science and data visualization.",url:"https:\u002F\u002Fdataelixir.com",metadata:kg},{id:3819,__component:f,text:"Online Communities",slug:"online-communities"},{id:935,__component:aT,title:"Kaggle",description:"An online data science community known for its public data-analysis competitions. Organizations present data analysis problems to the Kaggle community and give them access to real data. Whoever submits the best data-analysis algorithm wins a cash prize and\u002For exposure in exchange for giving the organization the right to use their algorithm. Kaggle also offers courses and forums for community discussion.",url:"https:\u002F\u002Fwww.kaggle.com\u002F",metadata:a},{id:936,__component:aT,title:"Data Science Salon",description:"An organizer of data science events. Includes a series of virtual small conferences, webinars, a community Slack, and podcasts featuring interviews with data scientists, as well as a podcast specifically focusing on elevating women in data science.",url:"https:\u002F\u002Fwww.datascience.salon\u002F",metadata:a},{id:3818,__component:f,text:"Podcasts",slug:"podcasts"},{id:939,__component:aT,title:"Making Data Simple",description:"Hosted by Al Martin, IBMs VP of Data and AI Expert Services and Learning, this podcast from IBM features data scientists discussing how data analysis is used in business.",url:"https:\u002F\u002Fpodcasts.apple.com\u002Fus\u002Fpodcast\u002Fmaking-data-simple\u002Fid605818735",metadata:"- Approximately 30 minutes\n- Weekly\n- Since 2017\n"},{id:944,__component:aT,title:"Not So Standard Deviations",description:"Two data scientists discuss the latest news in big data analytics, from both academia and industry.",url:"https:\u002F\u002Fnssdeviations.com\u002F",metadata:"- Approximately one hour\n- Every other week\n- Since 2015\n"},{id:942,__component:aT,title:"Data Skeptic",description:"Interviews with data scientists and researchers. The hosts discuss data analysis, using the scientific method to evaluate the validity of research claims and approaches.",url:"http:\u002F\u002Fdataskeptic.com\u002F",metadata:"- Approximately 30 minutes\n- Weekly\n- Since 2014\n"}],foundation_topic:{data:a},technical_topic:{data:a},localizations:{data:[]}}}]},ebooks:{data:[{id:eF,attributes:{title:eG,slug:aW,summary:eH,featuredDescription:a,level:m,prerequisites:a,year:aX,createdAt:eI,updatedAt:eJ,publishedAt:eK,bookCover:{data:{id:1997,attributes:{name:"Data_Science_7b93b342c8.jpg",alternativeText:a,caption:a,width:gI,height:757,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Data_Science_7b93b342c8_9423fe9683.jpg",hash:"small_Data_Science_7b93b342c8_9423fe9683",mime:h,name:"small_Data_Science_7b93b342c8.jpg",path:a,size:17.9,width:363,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Data_Science_7b93b342c8_9423fe9683.jpg",hash:"medium_Data_Science_7b93b342c8_9423fe9683",mime:h,name:"medium_Data_Science_7b93b342c8.jpg",path:a,size:32.23,width:545,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Data_Science_7b93b342c8_9423fe9683.jpg",hash:"thumbnail_Data_Science_7b93b342c8_9423fe9683",mime:h,name:"thumbnail_Data_Science_7b93b342c8.jpg",path:a,size:3.28,width:113,height:z}},hash:"Data_Science_7b93b342c8_9423fe9683",ext:l,mime:h,size:33.06,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FData_Science_7b93b342c8_9423fe9683.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kh,updatedAt:kh}}},bookPdf:{data:{id:1996,attributes:{name:"Data_Science_2d2e5edc3c.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"Data_Science_2d2e5edc3c_a9a8987c7b",ext:gM,mime:gN,size:2283.58,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FData_Science_2d2e5edc3c_a9a8987c7b.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:ki,updatedAt:ki}}},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:739,name:"Brendan Tierney",info:a},{id:740,name:"John D. Kelleher",info:"John D. Kelleher is Academic Leader of the Information, Communication, and Entertainment Research Institute at Technological University Dublin. He is the coauthor of *Data Science* and the author of *Deep Learning*, both in the MIT Press Essential Knowledge series."}],bookChapters:[{id:8452,chapterTitle:hx,bookTitle:aL,pageNumber:kj,chapterPdf:{data:a}},{id:8454,chapterTitle:"Series Foreword",bookTitle:aL,pageNumber:kk,chapterPdf:{data:a}},{id:8453,chapterTitle:kl,bookTitle:aL,pageNumber:ii,chapterPdf:{data:a}},{id:8456,chapterTitle:"1: What Is Data Science?",bookTitle:aL,pageNumber:km,chapterPdf:{data:a}},{id:8459,chapterTitle:"2: What Are Data, and What Is a Data Set?",bookTitle:aL,pageNumber:"55",chapterPdf:{data:a}},{id:8457,chapterTitle:"3: A Data Science Ecosystem",bookTitle:aL,pageNumber:"85",chapterPdf:{data:a}},{id:8458,chapterTitle:"4: Machine Learning 101",bookTitle:aL,pageNumber:"113",chapterPdf:{data:a}},{id:8455,chapterTitle:"5: Standard Data Science Tasks",bookTitle:aL,pageNumber:kn,chapterPdf:{data:a}},{id:8461,chapterTitle:"6: Privacy and Ethics",bookTitle:aL,pageNumber:"197",chapterPdf:{data:a}},{id:8460,chapterTitle:"7: Future and Principles of Success",bookTitle:aL,pageNumber:"235",chapterPdf:{data:a}},{id:8463,chapterTitle:gW,bookTitle:aL,pageNumber:"263",chapterPdf:{data:a}},{id:8466,chapterTitle:hy,bookTitle:aL,pageNumber:"269",chapterPdf:{data:a}},{id:8464,chapterTitle:aS,bookTitle:aL,pageNumber:"255",chapterPdf:{data:a}},{id:8465,chapterTitle:gO,bookTitle:aL,pageNumber:"277",chapterPdf:{data:a}},{id:8462,chapterTitle:"Further Readings",bookTitle:aL,pageNumber:"267",chapterPdf:{data:a}}]}},{id:aP,attributes:{title:C,slug:eL,summary:eM,featuredDescription:a,level:m,prerequisites:a,year:eN,createdAt:eO,updatedAt:eP,publishedAt:eQ,bookCover:{data:{id:1926,attributes:{name:"Big_Data_Little_Data_No_Data_Cover_f5e8615a56.jpg",alternativeText:a,caption:a,width:gI,height:ko,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d.jpg",hash:"small_Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d",mime:h,name:"small_Big_Data_Little_Data_No_Data_Cover_f5e8615a56.jpg",path:a,size:25.63,width:ij,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d.jpg",hash:"medium_Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d",mime:h,name:"medium_Big_Data_Little_Data_No_Data_Cover_f5e8615a56.jpg",path:a,size:45.8,width:kp,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d.jpg",hash:"thumbnail_Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d",mime:h,name:"thumbnail_Big_Data_Little_Data_No_Data_Cover_f5e8615a56.jpg",path:a,size:4.15,width:hz,height:z}},hash:"Big_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d",ext:l,mime:h,size:52.11,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBig_Data_Little_Data_No_Data_Cover_f5e8615a56_8bc2a9013d.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kq,updatedAt:kq}}},bookPdf:{data:{id:1927,attributes:{name:"Big_Data_Little_Data_No_Data_af4512b061.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"Big_Data_Little_Data_No_Data_af4512b061_9a886b2e42",ext:gM,mime:gN,size:6819.73,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBig_Data_Little_Data_No_Data_af4512b061_9a886b2e42.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kr,updatedAt:kr}}},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:688,name:"Christine L. Borgman",info:"Christine L. Borgman is Professor and Presidential Chair in Information Studies at the University of California, Los Angeles. She is the author of *From Gutenberg to the Global Information Infrastructure and Scholarship in the Digital Age* (both winners of the Best Information Science Book award from ASIS&T), published by the MIT Press."}],bookChapters:[{id:7840,chapterTitle:kl,bookTitle:C,pageNumber:"18",chapterPdf:{data:a}},{id:7841,chapterTitle:hx,bookTitle:C,pageNumber:ik,chapterPdf:{data:a}},{id:7851,chapterTitle:"Part I: Data and Scholarship",bookTitle:C,pageNumber:"28",chapterPdf:{data:a}},{id:7854,chapterTitle:"1: Provocations",bookTitle:C,pageNumber:ks,chapterPdf:{data:a}},{id:7856,chapterTitle:"2: What Are Data?",bookTitle:C,pageNumber:"44",chapterPdf:{data:a}},{id:7842,chapterTitle:"3: Data Scholarship",bookTitle:C,pageNumber:kt,chapterPdf:{data:a}},{id:7849,chapterTitle:"4: Data Diversity",bookTitle:C,pageNumber:ku,chapterPdf:{data:a}},{id:7844,chapterTitle:"Part II: Case Studies in Data Scholarship",bookTitle:C,pageNumber:"108",chapterPdf:{data:a}},{id:7843,chapterTitle:"5: Data Scholarship in the Sciences",bookTitle:C,pageNumber:"110",chapterPdf:{data:a}},{id:7847,chapterTitle:"6: Data Scholarship in the Social Sciences",bookTitle:C,pageNumber:"152",chapterPdf:{data:a}},{id:7853,chapterTitle:"7: Data Scholarship in the Humanities",bookTitle:C,pageNumber:kv,chapterPdf:{data:a}},{id:7846,chapterTitle:"Part III: Data Policy and Practice",bookTitle:C,pageNumber:kw,chapterPdf:{data:a}},{id:7845,chapterTitle:"8: Sharing, Releasing, and Reusing Data",bookTitle:C,pageNumber:"232",chapterPdf:{data:a}},{id:7855,chapterTitle:"9: Credit, Attribution, and Discovery of Data",bookTitle:C,pageNumber:"268",chapterPdf:{data:a}},{id:7848,chapterTitle:"10: What to Keep and Why",bookTitle:C,pageNumber:"298",chapterPdf:{data:a}},{id:7850,chapterTitle:hy,bookTitle:C,pageNumber:"316",chapterPdf:{data:a}},{id:7852,chapterTitle:gO,bookTitle:C,pageNumber:kx,chapterPdf:{data:a}}]}},{id:eR,attributes:{title:eS,slug:eT,summary:eU,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:eV,updatedAt:eW,publishedAt:eX,bookCover:{data:{id:1970,attributes:{name:"Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5.jpg",alternativeText:a,caption:a,width:gI,height:816,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845.jpg",hash:"small_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845",mime:h,name:"small_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5.jpg",path:a,size:32.69,width:337,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845.jpg",hash:"medium_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845",mime:h,name:"medium_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5.jpg",path:a,size:57.98,width:506,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845.jpg",hash:"thumbnail_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845",mime:h,name:"thumbnail_Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5.jpg",path:a,size:3.87,width:hz,height:z}},hash:"Trusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845",ext:l,mime:h,size:65.53,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FTrusted_Data_A_New_Framework_for_Identity_and_Data_Sharing_8cc07ff2c5_289eb51845.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:ky,updatedAt:ky}}},bookPdf:{data:{id:1969,attributes:{name:"Trusted_Data_806bc83150.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"Trusted_Data_806bc83150_654a88947d",ext:gM,mime:gN,size:.26,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FTrusted_Data_806bc83150_654a88947d.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kz,updatedAt:kz}}},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:713,name:"David L. Shrier",info:"David L. Shrier holds a dual appointment as a Lecturer at the MIT Media Lab and an Associate Fellow at the Sad Business School at the University of Oxford. He is coeditor of *New Solutions for Cybersecurity* (MIT Press)."},{id:jp,name:"Thomas Hardjono",info:"Thomas Hardjono is the CTO of MIT Connection Science and Engineering and Technical Director for the Internet Trust Consortium under MIT Connection Science."},{id:715,name:"Alex Pentland",info:"Alex Pentland holds a triple appointment at MIT in the Media Lab (SA+P), School of Engineering, and Sloan School of Management. He directs MIT's Connection Science initiative, the Human Dynamics Laboratory, and the MIT Media Lab Entrepreneurship Program. He is the author of *Honest Signals: How They Shape Our World* and coeditor of *New Solutions for Cybersecurity* (both published by the MIT Press). One of the most-cited computer scientists in the world, with international awards in the Arts, Sciences, and Engineering, he was chosen by Newsweek as one of the 100 Americans likely to shape this century."}],bookChapters:[{id:8170,chapterTitle:"1: Trust in Digital Societies",bookTitle:aJ,pageNumber:hA,chapterPdf:{data:a}},{id:8171,chapterTitle:"2: Towards an Internet of Trusted Data",bookTitle:aJ,pageNumber:ik,chapterPdf:{data:a}},{id:8175,chapterTitle:"3: Core Identities for Future Transaction Systems",bookTitle:aJ,pageNumber:"48",chapterPdf:{data:a}},{id:8186,chapterTitle:"4: MIT Open Algorithms",bookTitle:aJ,pageNumber:kA,chapterPdf:{data:a}},{id:8181,chapterTitle:"5: Building a Data-Rich Society",bookTitle:aJ,pageNumber:il,chapterPdf:{data:a}},{id:8185,chapterTitle:"6: The New Deal on Data",bookTitle:aJ,pageNumber:"136",chapterPdf:{data:a}},{id:8172,chapterTitle:"7: The Rise of Decentralized Personal Data Markets",bookTitle:aJ,pageNumber:"162",chapterPdf:{data:a}},{id:8179,chapterTitle:"8: Enabling Humanitarian Use of Mobile Phone Data",bookTitle:aJ,pageNumber:"174",chapterPdf:{data:a}},{id:8173,chapterTitle:"9: Living Labs for Trusted Data",bookTitle:aJ,pageNumber:kv,chapterPdf:{data:a}},{id:8184,chapterTitle:"10: Active Fairness in Algorithmic Decision-Making",bookTitle:aJ,pageNumber:"202",chapterPdf:{data:a}},{id:8178,chapterTitle:"11: Beyond GDPR: Employing AI to Make Personal Data Useful to Consumers",bookTitle:aJ,pageNumber:kB,chapterPdf:{data:a}},{id:8182,chapterTitle:"12: Social Capital Accounting",bookTitle:aJ,pageNumber:"234",chapterPdf:{data:a}},{id:8174,chapterTitle:"13: Tradecoin: Towards a More Stable Digital Currency",bookTitle:aJ,pageNumber:"246",chapterPdf:{data:a}},{id:8180,chapterTitle:"Appendix A: Personal Data: The Emergence of a New Asset Class",bookTitle:aJ,pageNumber:"278",chapterPdf:{data:a}},{id:8176,chapterTitle:"Appendix B: A World that Counts: Mobilising the Data Revolution for Sustainable Development",bookTitle:aJ,pageNumber:"330",chapterPdf:{data:a}},{id:8183,chapterTitle:"Contributor Biographies",bookTitle:aJ,pageNumber:"378",chapterPdf:{data:a}},{id:8177,chapterTitle:gO,bookTitle:aJ,pageNumber:kx,chapterPdf:{data:a}}]}},{id:eY,attributes:{title:eZ,slug:e_,summary:e$,featuredDescription:a,level:m,prerequisites:a,year:fa,createdAt:fb,updatedAt:fc,publishedAt:fd,bookCover:{data:{id:1948,attributes:{name:"Smart_Cities_EKS_Cover_c1ff985c86.jpg",alternativeText:a,caption:a,width:gI,height:770,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5.jpg",hash:"small_Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5",mime:h,name:"small_Smart_Cities_EKS_Cover_c1ff985c86.jpg",path:a,size:15.14,width:357,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5.jpg",hash:"medium_Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5",mime:h,name:"medium_Smart_Cities_EKS_Cover_c1ff985c86.jpg",path:a,size:26.06,width:536,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5.jpg",hash:"thumbnail_Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5",mime:h,name:"thumbnail_Smart_Cities_EKS_Cover_c1ff985c86.jpg",path:a,size:2.7,width:111,height:z}},hash:"Smart_Cities_EKS_Cover_c1ff985c86_078172e3c5",ext:l,mime:h,size:26.9,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FSmart_Cities_EKS_Cover_c1ff985c86_078172e3c5.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kC,updatedAt:kC}}},bookPdf:{data:{id:1947,attributes:{name:"Smart_Cities_EKS_3ec5f21675.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"Smart_Cities_EKS_3ec5f21675_6a0b6f951e",ext:gM,mime:gN,size:2164.5,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FSmart_Cities_EKS_3ec5f21675_6a0b6f951e.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kD,updatedAt:kD}}},topic:{data:[{id:38,attributes:{topic:kE,createdAt:"2023-10-18T21:44:46.907Z",updatedAt:"2023-10-18T21:49:42.132Z",publishedAt:"2023-03-01T20:40:52.537Z",siteTopic:{data:{id:77,attributes:{title:kE,slug:"internet-of-things-iot",subtitle:"Networks of connected devices",underDevelopment:c,hidden:a,knowledgeChecks:"https:\u002F\u002Fdigitalu.af.mil\u002Fapp\u002Fassessments\u002Fdabe206f-9bb3-4302-b61c-c651e14c7767\u002Fa3e9529b-4d49-4636-9120-0a9f1489aa93",createdAt:"2023-10-18T21:48:39.051Z",updatedAt:"2023-11-16T14:06:20.294Z",publishedAt:"2023-02-22T14:47:35.500Z"}}}}}]},authors:[{id:701,name:"Germaine R. Halegoua",info:"Germaine R. Halegoua is Associate Professor in the Department of Film and Media Studies at the University of Kansas."}],bookChapters:[{id:8007,chapterTitle:ie,bookTitle:gJ,pageNumber:ii,chapterPdf:{data:a}},{id:8011,chapterTitle:"1: An Introduction to Smart Cities",bookTitle:gJ,pageNumber:"21",chapterPdf:{data:a}},{id:8008,chapterTitle:"2: Model for Smart City Development",bookTitle:gJ,pageNumber:"63",chapterPdf:{data:a}},{id:8010,chapterTitle:"3: Smart City Technologies",bookTitle:gJ,pageNumber:"105",chapterPdf:{data:a}},{id:8014,chapterTitle:"4: Citizen Input and Engagement",bookTitle:gJ,pageNumber:"145",chapterPdf:{data:a}},{id:8013,chapterTitle:"5: Future Directions for Smart Cities",bookTitle:gJ,pageNumber:"187",chapterPdf:{data:a}},{id:8012,chapterTitle:gW,bookTitle:gJ,pageNumber:"211",chapterPdf:{data:a}},{id:8015,chapterTitle:aS,bookTitle:gJ,pageNumber:"207",chapterPdf:{data:a}},{id:8009,chapterTitle:kF,bookTitle:gJ,pageNumber:"227",chapterPdf:{data:a}}]}},{id:fe,attributes:{title:J,slug:ff,summary:fg,featuredDescription:a,level:m,prerequisites:a,year:av,createdAt:fh,updatedAt:fi,publishedAt:fj,bookCover:{data:{id:1933,attributes:{name:"The_Smart_Enough_City_Cover_643dca147b.jpg",alternativeText:a,caption:a,width:gI,height:831,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_The_Smart_Enough_City_Cover_643dca147b_75ca677953.jpg",hash:"small_The_Smart_Enough_City_Cover_643dca147b_75ca677953",mime:h,name:"small_The_Smart_Enough_City_Cover_643dca147b.jpg",path:a,size:35.03,width:331,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_The_Smart_Enough_City_Cover_643dca147b_75ca677953.jpg",hash:"medium_The_Smart_Enough_City_Cover_643dca147b_75ca677953",mime:h,name:"medium_The_Smart_Enough_City_Cover_643dca147b.jpg",path:a,size:63.04,width:496,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_The_Smart_Enough_City_Cover_643dca147b_75ca677953.jpg",hash:"thumbnail_The_Smart_Enough_City_Cover_643dca147b_75ca677953",mime:h,name:"thumbnail_The_Smart_Enough_City_Cover_643dca147b.jpg",path:a,size:5.48,width:103,height:z}},hash:"The_Smart_Enough_City_Cover_643dca147b_75ca677953",ext:l,mime:h,size:75.38,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FThe_Smart_Enough_City_Cover_643dca147b_75ca677953.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kG,updatedAt:kG}}},bookPdf:{data:{id:1934,attributes:{name:"The_Smart_Enough_City_fb92f2fad3.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"The_Smart_Enough_City_fb92f2fad3_6c2fed3ff2",ext:gM,mime:gN,size:6386.9,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FThe_Smart_Enough_City_fb92f2fad3_6c2fed3ff2.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kH,updatedAt:kH}}},topic:{data:[{id:34,attributes:{topic:kI,createdAt:"2023-10-18T21:44:46.832Z",updatedAt:"2023-10-18T21:49:42.031Z",publishedAt:"2023-03-01T20:40:31.630Z",siteTopic:{data:{id:68,attributes:{title:kI,slug:"artificial-intelligence",subtitle:"Computers that make decisions and predictions",underDevelopment:c,hidden:a,knowledgeChecks:"https:\u002F\u002Fdigitalu.af.mil\u002Fapp\u002Fassessments\u002Fdabe206f-9bb3-4302-b61c-c651e14c7767\u002Fe6befaa1-c360-42df-9f38-fbc42e7619bb",createdAt:"2023-10-18T21:48:28.220Z",updatedAt:"2023-11-16T14:06:19.395Z",publishedAt:"2023-02-22T14:25:03.631Z"}}}}}]},authors:[{id:692,name:"Ben Green",info:"Ben Green is an Affiliate and former Fellow at the Berkman Klein Center for Internet and Society at Harvard University and a PhD candidate in Applied Mathematics at Harvard's John A. Paulson School of Engineering and Applied Sciences. From 2016 to 2017 he was a Data Scientist in the City of Boston's Department of Innovation and Technology."}],bookChapters:[{id:7908,chapterTitle:"Foreword",bookTitle:J,pageNumber:hA,chapterPdf:{data:a}},{id:7916,chapterTitle:"1: Acknowledgments",bookTitle:J,pageNumber:im,chapterPdf:{data:a}},{id:7909,chapterTitle:"2: The Livable City: The Limits and Dangers of New Technology",bookTitle:J,pageNumber:ks,chapterPdf:{data:a}},{id:7911,chapterTitle:"3: The Democratic City: The Social Determinants of Technologys Impacts",bookTitle:J,pageNumber:"54",chapterPdf:{data:a}},{id:7917,chapterTitle:"4: The Just City: Machine Learnings Social and Political Foundations",bookTitle:J,pageNumber:"78",chapterPdf:{data:a}},{id:7912,chapterTitle:"5: The Responsible City: Avoiding Technologys Undemocratic Social Contracts",bookTitle:J,pageNumber:kJ,chapterPdf:{data:a}},{id:7910,chapterTitle:"6: The Innovative City: The Relationship between Technical and Nontechnical Change in City Government",bookTitle:J,pageNumber:kK,chapterPdf:{data:a}},{id:7913,chapterTitle:"7: The Smart Enough City: Lessons from the Past and a Framework for the Future",bookTitle:J,pageNumber:"158",chapterPdf:{data:a}},{id:7914,chapterTitle:gW,bookTitle:J,pageNumber:kL,chapterPdf:{data:a}},{id:7915,chapterTitle:hy,bookTitle:J,pageNumber:kM,chapterPdf:{data:a}},{id:7918,chapterTitle:gO,bookTitle:J,pageNumber:kw,chapterPdf:{data:a}}]}},{id:bn,attributes:{title:K,slug:fk,summary:fl,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fm,updatedAt:fn,publishedAt:fo,bookCover:{data:{id:1895,attributes:{name:"Self_Tracking_EKS_Cover_65dab4e415.jpg",alternativeText:a,caption:a,width:gI,height:765,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7.jpg",hash:"small_Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7",mime:h,name:"small_Self_Tracking_EKS_Cover_65dab4e415.jpg",path:a,size:13.31,width:359,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7.jpg",hash:"medium_Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7",mime:h,name:"medium_Self_Tracking_EKS_Cover_65dab4e415.jpg",path:a,size:23.97,width:539,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7.jpg",hash:"thumbnail_Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7",mime:h,name:"thumbnail_Self_Tracking_EKS_Cover_65dab4e415.jpg",path:a,size:2.63,width:in0,height:z}},hash:"Self_Tracking_EKS_Cover_65dab4e415_e25c8deab7",ext:l,mime:h,size:24.5,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FSelf_Tracking_EKS_Cover_65dab4e415_e25c8deab7.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kN,updatedAt:kN}}},bookPdf:{data:a},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:666,name:"Dawn Nafus",info:"Dawn Nafus is Senior Research Scientist at Intel Labs and the editor of Quantified: Biosensing Technologies in Everyday Life (MIT Press)."},{id:667,name:"Gina Neff",info:"Gina Neff is Associate Professor of Communication and Sociology and a senior data scientist at the University of Washington. She is the author of *Venture Labor: Work and the Burden of Risk in Innovative Industries* (MIT Press)."}],bookChapters:[{id:7594,chapterTitle:hx,bookTitle:K,pageNumber:hA,chapterPdf:{data:a}},{id:7595,chapterTitle:"1: An Introduction to Self-Tracking",bookTitle:K,pageNumber:im,chapterPdf:{data:a}},{id:7596,chapterTitle:"2: What Is at Stake? The Personal Gets Political",bookTitle:K,pageNumber:"50",chapterPdf:{data:a}},{id:7597,chapterTitle:"3: Making Sense of Data",bookTitle:K,pageNumber:ku,chapterPdf:{data:a}},{id:7600,chapterTitle:"4: Self-Tracking and the Technology Industry",bookTitle:K,pageNumber:kO,chapterPdf:{data:a}},{id:7604,chapterTitle:"5: Self-Tracking and Medicine",bookTitle:K,pageNumber:kP,chapterPdf:{data:a}},{id:7599,chapterTitle:"6: Future Directions for Self-Tracking",bookTitle:K,pageNumber:kL,chapterPdf:{data:a}},{id:7603,chapterTitle:gW,bookTitle:K,pageNumber:kM,chapterPdf:{data:a}},{id:7598,chapterTitle:kF,bookTitle:K,pageNumber:"222",chapterPdf:{data:a}},{id:7601,chapterTitle:aS,bookTitle:K,pageNumber:"218",chapterPdf:{data:a}},{id:7602,chapterTitle:gO,bookTitle:K,pageNumber:"226",chapterPdf:{data:a}}]}},{id:fp,attributes:{title:fq,slug:fr,summary:fs,featuredDescription:a,level:m,prerequisites:a,year:ft,createdAt:fu,updatedAt:fv,publishedAt:fw,bookCover:{data:{id:1875,attributes:{name:"Reality_Mining_93798e0ec6.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"Reality_Mining_93798e0ec6_3270d5fdad",ext:gM,mime:gN,size:1087.37,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FReality_Mining_93798e0ec6_3270d5fdad.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kQ,updatedAt:kQ}}},bookPdf:{data:{id:1876,attributes:{name:"Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8.png",alternativeText:a,caption:a,width:906,height:1352,formats:{large:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34.png",hash:"large_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34",mime:p,name:"large_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8.png",path:a,size:445.48,width:670,height:P},small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34.png",hash:"small_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34",mime:p,name:"small_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8.png",path:a,size:125.7,width:335,height:y},medium:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34.png",hash:"medium_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34",mime:p,name:"medium_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8.png",path:a,size:264.18,width:503,height:B},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34.png",hash:"thumbnail_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34",mime:p,name:"thumbnail_Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8.png",path:a,size:17.64,width:hz,height:z}},hash:"Screen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34",ext:o,mime:p,size:254.13,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FScreen_Shot_2021_01_22_at_11_39_40_AM_fdd8bb38b8_bb4b735b34.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:kR,updatedAt:kR}}},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:653,name:"Kate Greene",info:"Kate Greene is an essayist, journalist, poet, and former laser physicist based in San Francisco. Her work has appeared in *Harvard Review*, *Slate*, *Discover*, *The Economis*t, and the *New Yorker*, among other publications."},{id:652,name:"Nathan Eagle",info:"Nathan Eagle, one of the 50 people who will change the world on the 2012 *Wired* Smart List, is the cofounder and CEO of Jana, a company that helps global brands reach customers in emerging markets via mobile airtime. He holds faculty positions at Harvard and Northeastern Universities."}],bookChapters:[{id:7410,chapterTitle:ie,bookTitle:M,pageNumber:"8",chapterPdf:{data:a}},{id:7411,chapterTitle:"Part I: The Individual (One Person)",bookTitle:M,pageNumber:im,chapterPdf:{data:a}},{id:7412,chapterTitle:"1: Mobile Phones, Sensors, and Lifelogging: Collecting Data from Individuals While Considering Privacy",bookTitle:M,pageNumber:"16",chapterPdf:{data:a}},{id:7420,chapterTitle:"2: Using Personal Data in a Privacy-Sensitive Way to Make a Persons Life Easier and Healthier",bookTitle:M,pageNumber:"38",chapterPdf:{data:a}},{id:7417,chapterTitle:"Part II: The Neighborhood and the Organization (10 to 1,000 People)",bookTitle:M,pageNumber:kt,chapterPdf:{data:a}},{id:7413,chapterTitle:"3: Gathering Data from Small Heterogeneous Groups",bookTitle:M,pageNumber:"60",chapterPdf:{data:a}},{id:7414,chapterTitle:"4: Engineering and Policy: Building More Efficient Businesses, Enabling Hyperlocal Politics, Life Queries, and Opportunity Searches",bookTitle:M,pageNumber:"76",chapterPdf:{data:a}},{id:7428,chapterTitle:"Part III: The City (1,000 to 1,000,000 People)",bookTitle:M,pageNumber:kA,chapterPdf:{data:a}},{id:7426,chapterTitle:"5: Traffic Data, Crime Stats, and Closed-Circuit Cameras: Accumulating Urban Analytics",bookTitle:M,pageNumber:"92",chapterPdf:{data:a}},{id:7421,chapterTitle:"6: Engineering and Policy: Optimizing Resource Allocation",bookTitle:M,pageNumber:kJ,chapterPdf:{data:a}},{id:7418,chapterTitle:"Part: IV The Nation (1 Million to 100 Million People)",bookTitle:M,pageNumber:il,chapterPdf:{data:a}},{id:7427,chapterTitle:"7: Taking the Pulse of a Nation: Census, Mobile Phones, and Internet Giants",bookTitle:M,pageNumber:kO,chapterPdf:{data:a}},{id:7423,chapterTitle:"8: Engineering and Policy: Addressing National Sentiment, Economic Deficits, and Disasters",bookTitle:M,pageNumber:kK,chapterPdf:{data:a}},{id:7415,chapterTitle:"Part V: Reality Mining the Worlds Data (100 Million to 7 Billion People)",bookTitle:M,pageNumber:kP,chapterPdf:{data:a}},{id:7416,chapterTitle:"9: Gathering the Worlds Data: Global Census, International Travel and Commerce, and Planetary-Scale Communication",bookTitle:M,pageNumber:"150",chapterPdf:{data:a}},{id:7424,chapterTitle:"10: Engineering a Safer and Healthier World",bookTitle:M,pageNumber:"160",chapterPdf:{data:a}},{id:7422,chapterTitle:"Conclusion",bookTitle:M,pageNumber:"172",chapterPdf:{data:a}},{id:7419,chapterTitle:gW,bookTitle:M,pageNumber:"176",chapterPdf:{data:a}},{id:7425,chapterTitle:gO,bookTitle:M,pageNumber:"198",chapterPdf:{data:a}}]}},{id:bo,attributes:{title:fx,slug:fy,summary:fz,featuredDescription:a,level:m,prerequisites:a,year:L,createdAt:fA,updatedAt:fB,publishedAt:fC,bookCover:{data:{id:1860,attributes:{name:"Streaming_Sharing_Stealing_931e543c9d.jpg",alternativeText:a,caption:a,width:gI,height:ko,formats:{small:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Streaming_Sharing_Stealing_931e543c9d_815fea82ae.jpg",hash:"small_Streaming_Sharing_Stealing_931e543c9d_815fea82ae",mime:h,name:"small_Streaming_Sharing_Stealing_931e543c9d.jpg",path:a,size:20.95,width:ij,height:y},medium:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Streaming_Sharing_Stealing_931e543c9d_815fea82ae.jpg",hash:"medium_Streaming_Sharing_Stealing_931e543c9d_815fea82ae",mime:h,name:"medium_Streaming_Sharing_Stealing_931e543c9d.jpg",path:a,size:36.25,width:kp,height:B},thumbnail:{ext:l,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Streaming_Sharing_Stealing_931e543c9d_815fea82ae.jpg",hash:"thumbnail_Streaming_Sharing_Stealing_931e543c9d_815fea82ae",mime:h,name:"thumbnail_Streaming_Sharing_Stealing_931e543c9d.jpg",path:a,size:4.23,width:hz,height:z}},hash:"Streaming_Sharing_Stealing_931e543c9d_815fea82ae",ext:l,mime:h,size:40.64,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FStreaming_Sharing_Stealing_931e543c9d_815fea82ae.jpg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kS,updatedAt:kS}}},bookPdf:{data:{id:1859,attributes:{name:"Streaming_Sharing_Stealing_1e7695750c.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"Streaming_Sharing_Stealing_1e7695750c_fbb08dcdb5",ext:gM,mime:gN,size:2972.5,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FStreaming_Sharing_Stealing_1e7695750c_fbb08dcdb5.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kT,updatedAt:kT}}},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:kU,name:"Rahul Telang",info:"Rahul Telang is Professor of Information Systems and Management at Carnegie Mellon University's Heinz College. He is Codirector (with Michael D. Smith) of the Initiative for Digital Entertainment Analytics (IDEA) at Carnegie Mellon."},{id:kV,name:"Michael D. Smith",info:"Michael D. Smith is Professor of Information Systems and Marketing at Carnegie Mellon University's Heinz College. He is Codirector (with Rahul Telang) of the Initiative for Digital Entertainment Analytics (IDEA) at Carnegie Mellon."}],bookChapters:[{id:7267,chapterTitle:hx,bookTitle:aK,pageNumber:ii,chapterPdf:{data:a}},{id:7280,chapterTitle:"Part I: Good Times, Bad Times",bookTitle:aK,pageNumber:kj,chapterPdf:{data:a}},{id:7268,chapterTitle:"1: House of Cards",bookTitle:aK,pageNumber:km,chapterPdf:{data:a}},{id:7270,chapterTitle:"2: Back in Time",bookTitle:aK,pageNumber:"31",chapterPdf:{data:a}},{id:7282,chapterTitle:"3: For a Few Dollars More",bookTitle:aK,pageNumber:"45",chapterPdf:{data:a}},{id:7281,chapterTitle:"4: The Perfect Storm",bookTitle:aK,pageNumber:"61",chapterPdf:{data:a}},{id:7283,chapterTitle:"Part II: Changes",bookTitle:aK,pageNumber:"75",chapterPdf:{data:a}},{id:7269,chapterTitle:"5: Blockbusters and the Long Tail",bookTitle:aK,pageNumber:"77",chapterPdf:{data:a}},{id:7271,chapterTitle:"6: Raised on Robbery",bookTitle:aK,pageNumber:"93",chapterPdf:{data:a}},{id:7275,chapterTitle:"7: Power to the People",bookTitle:aK,pageNumber:"117",chapterPdf:{data:a}},{id:7273,chapterTitle:"8: Revenge of the Nerds",bookTitle:aK,pageNumber:"131",chapterPdf:{data:a}},{id:7276,chapterTitle:"Part III: A New Hope",bookTitle:aK,pageNumber:kn,chapterPdf:{data:a}},{id:7272,chapterTitle:"9: Moneyball",bookTitle:aK,pageNumber:"147",chapterPdf:{data:a}},{id:7279,chapterTitle:"10: Pride and Prejudice",bookTitle:aK,pageNumber:"169",chapterPdf:{data:a}},{id:7278,chapterTitle:"11: The Show Must Go On",bookTitle:aK,pageNumber:"189",chapterPdf:{data:a}},{id:7277,chapterTitle:gW,bookTitle:aK,pageNumber:"201",chapterPdf:{data:a}},{id:7274,chapterTitle:gO,bookTitle:aK,pageNumber:"223",chapterPdf:{data:a}}]}},{id:fD,attributes:{title:D,slug:fE,summary:fF,featuredDescription:a,level:aw,prerequisites:a,year:fG,createdAt:fH,updatedAt:fI,publishedAt:fJ,bookCover:{data:{id:1818,attributes:{name:"High_Performance_Big_Data_Computing_d44c053c15.jpeg",alternativeText:a,caption:a,width:520,height:663,formats:{small:{ext:gH,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_High_Performance_Big_Data_Computing_d44c053c15_f94d787024.jpeg",hash:"small_High_Performance_Big_Data_Computing_d44c053c15_f94d787024",mime:h,name:"small_High_Performance_Big_Data_Computing_d44c053c15.jpeg",path:a,size:38.33,width:392,height:y},thumbnail:{ext:gH,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_High_Performance_Big_Data_Computing_d44c053c15_f94d787024.jpeg",hash:"thumbnail_High_Performance_Big_Data_Computing_d44c053c15_f94d787024",mime:h,name:"thumbnail_High_Performance_Big_Data_Computing_d44c053c15.jpeg",path:a,size:5.25,width:bo,height:z}},hash:"High_Performance_Big_Data_Computing_d44c053c15_f94d787024",ext:gH,mime:h,size:53.23,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FHigh_Performance_Big_Data_Computing_d44c053c15_f94d787024.jpeg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kW,updatedAt:kW}}},bookPdf:{data:{id:1817,attributes:{name:"High_Performance_Big_Data_Computing_86c4cdbc3e.pdf",alternativeText:a,caption:a,width:a,height:a,formats:a,hash:"High_Performance_Big_Data_Computing_86c4cdbc3e_6e3f0f54d2",ext:gM,mime:gN,size:.24,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FHigh_Performance_Big_Data_Computing_86c4cdbc3e_6e3f0f54d2.pdf",previewUrl:a,provider:j,provider_metadata:a,createdAt:kX,updatedAt:kX}}},topic:{data:[{id:gR,attributes:{topic:A,createdAt:aO,updatedAt:aO,publishedAt:gS,siteTopic:{data:a}}}]},authors:[{id:609,name:"Xiaoyi Lu",info:"Xiaoyi Lu is an Assistant Professor in the Department of Computer Science and Engineering at the University of California, Merced."},{id:610,name:"Dhabaleswar K. Panda",info:"Dhabaleswar K. Panda is Professor and University Distinguished Scholar of Computer Science and Engineering at the Ohio State University."},{id:611,name:"Dipti Shankar",info:"Dipti Shankar is currently working at SAP in Germany."}],bookChapters:[{id:6936,chapterTitle:"Front Matter",bookTitle:D,pageNumber:"4",chapterPdf:{data:a}},{id:6927,chapterTitle:"Acknowledgements",bookTitle:D,pageNumber:kk,chapterPdf:{data:a}},{id:6937,chapterTitle:"1: Introduction",bookTitle:D,pageNumber:hA,chapterPdf:{data:a}},{id:6929,chapterTitle:"2: Parallel Programming Models and Systems",bookTitle:D,pageNumber:ik,chapterPdf:{data:a}},{id:6931,chapterTitle:"3: Parallel and Distributed Storage Systems",bookTitle:D,pageNumber:"46",chapterPdf:{data:a}},{id:6935,chapterTitle:"4: HPC Architectures and Trends",bookTitle:D,pageNumber:"70",chapterPdf:{data:a}},{id:6932,chapterTitle:"5: Opportunities and Challenges in Accelerating Big Data Computing",bookTitle:D,pageNumber:"102",chapterPdf:{data:a}},{id:6926,chapterTitle:"6: Benchmarking Big Data Systems",bookTitle:D,pageNumber:il,chapterPdf:{data:a}},{id:6934,chapterTitle:"7: Accelerations with RDMA",bookTitle:D,pageNumber:"130",chapterPdf:{data:a}},{id:6933,chapterTitle:"8: Accelerations with Multicore\u002FAccelerator Technologies",bookTitle:D,pageNumber:"154",chapterPdf:{data:a}},{id:6938,chapterTitle:"9: Accelerations with High-Performance Storage Technologies",bookTitle:D,pageNumber:"168",chapterPdf:{data:a}},{id:6925,chapterTitle:"10: Deep Learning over Big Data",bookTitle:D,pageNumber:"184",chapterPdf:{data:a}},{id:6928,chapterTitle:"11: Designs with Cloud Technologies",bookTitle:D,pageNumber:"204",chapterPdf:{data:a}},{id:6930,chapterTitle:"12: Frontier Research on High-Performance Big Data Computing",bookTitle:D,pageNumber:kB,chapterPdf:{data:a}},{id:6924,chapterTitle:hy,bookTitle:D,pageNumber:"236",chapterPdf:{data:a}},{id:6939,chapterTitle:gO,bookTitle:D,pageNumber:"266",chapterPdf:{data:a}}]}}]},videoCollection:{data:[{id:fK,attributes:{title:fL,slug:fM,description:ax,descriptionSummary:ax,collectionHeaderText:fN,publisher:fO,level:aw,createdAt:fP,updatedAt:fQ,publishedAt:fR,collectionIcon:{data:{id:2318,attributes:{name:"Big_Data_ICON_af48f3e285.svg",alternativeText:a,caption:a,width:aV,height:aR,formats:a,hash:"Big_Data_ICON_af48f3e285_80f77589d2",ext:r,mime:s,size:bj,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBig_Data_ICON_af48f3e285_80f77589d2.svg",previewUrl:a,provider:j,provider_metadata:a,createdAt:kY,updatedAt:kY}}},modules:{data:[{id:144,attributes:{title:"Clustering",slug:"clustering",description:"How do we get from raw data to improving the level of performance? The answer is found in this opening module, which introduces us to the tools and techniques developed to make sense of unstructured data and discover hidden patterns.",prerequisite:a,duration:kZ,objective:a,createdAt:"2023-10-18T21:49:04.737Z",updatedAt:"2023-10-18T21:49:45.663Z",publishedAt:"2023-03-16T13:13:40.794Z",chapters:{data:[{id:1255,attributes:{title:"What Is Unsupervised Learning and Its Challenges?",slug:"what-is-unsupervised-learning-and-its-challenges",duration:"7:08",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience11WhatIsUnsupervisedLearningandItsChallenges",objective:"Start off the course by learning what unsupervised learning is and understand what its challenges are.",englishTranscript:"Hi, and welcome to this module on unsupervised learning. So what is unsupervised learning? In unsupervised learning, you are trying to discover hidden patterns in data, when we don't have any labels. We'll talk about what hidden patterns are and what labels are shortly, and we'll talk through a lot of real data examples. But before that, let's introduce ourselves. I am Stefanie Jegelka. I'm Tamara Broderick. Both Stefanie and I are professors at MIT with the Department of Electrical Engineering and Computer Science and also with the Center for Statistics at the Institute for Data Systems in Society. I work on machine learning methods for all sorts of communatorial data, things like networks, subsets or images, from theory to making them work on real data.\n\nI work on machine learning methods for Bayesian and non-parametric Bayesian learning, that scale to very large data sets. I often work on methods for unsupervised learning. Stephanie and I will be your instructors for this module. So, what is unsupervised learning, again? And, in fact, is supervised learning a thing at all? This is a great observation. First, let's step back to what learning even means here. In machine learning in statistics, we're typically trying to find hidden patterns in data. Ideally, we want these hidden patterns to help us in some way. For instance, to help us understand some scientific result, to improve our user experience, or to help us maximize profit in some investment. Supervise learning is when we learn from data but we have labels for all the data we've seen so far. Unsupervised learning is when we learn from data but we don't have any labels. Some examples would be really helpful. Absolutely, let's start by thinking about email. I don't know about you, but for me it's really hard to keep my inbox in check. I get so many e-mails everyday. And a big problem here is spam. In fact, it would be an even bigger problem if e-mail providers, like Gmail, weren't so effective at keeping spam out of our inboxes.\n\nBut how do they know whether a particular e-mail is spam or not? So that's our first example of a machine learning problem. Every machine learning problem has a data set. That is, a collection of data points that help us learn. Here, our data set will be all the e-mails that are sent to some MIT professor over, say, a month. Each data point will be a single e-mail. Whenever I get an e-mail, I can quickly tell whether it's spam. So I might hit a button to label any particular e-mail as spam or not spam. Now we can imagine that each of our datapoints has one of two labels, spam or not spam. In the future, I keep getting emails but of course I won't know in advance which label it should have, spam or not spam. So the machine learning problem is to predict whether a new label for a new email is spam or not spam. That is, we want to predict the label of the next email. If our machine learning algorithm works, we can put all the spam in a separate folder, and not have to worry about it. This spam problem, is an example of supervised learning. And now we can start to see why it's called supervised learning. You can imagine some teacher, or supervisor, telling you the label of each data point. That is, whether each e-mail is spam or not spam. And we often want to check how well our machine learning algorithm is doing too. Then, the supervisor might be able to tell us whether the labels we predicted were correct.\n\nWhen I got a new e-mail, and I said it was spam using my algorithm, was I really correct about that, or did I just throw a really important e-mail into the trash? Okay, so that's supervised learning. So what is, then, unsupervised learning? What do I learn here? Let's try another example of a machine learning problem. Now imagine, I'm looking at my emails, and I realize, man, I have way too many emails. It would actually be super helpful if I could read all the emails that are on the same topic at the same time. So, I might run a machine learning algorithm that groups together similar emails. It turns out I'm rushing around, so I don't have the time to label emails spam or not spam or anything else. I don't even have the time to come up with topics. But, after I've run my machine learning algorithm, I find that there are natural groups of emails in my inbox. There are all the e-mails about upcoming and past hiking trips. There are bunch of e-mails about the machine learning class I taught last semester.\n\nAnd there are e-mails about a cool machine learning research project I'm currently working on. Now this is an example of an unsupervised learning problem. You didn't have any labels because you didn't have time to give any labels. And since no one else made labels for each email, there's no supervisor. In the previous supervised learning example, the machine learning algorithm had to find hidden patterns in the data that helped predict whether any e-mail is spam or not spam. In this unsupervised example, the machine learning algorithm only has the contexts of the e-mails to find hidden patterns. There aren't any labels to help it out. Unfortunately, my email client doesn't actually group my emails by topic, yet. But you actually see this kind of thing all the time on other websites. For instance, have you looked at Google News recently? Google could just list all the news articles it finds and order them by, say, the time they were published. But that would be overwhelming, and when you go to their site you'd never be able to find out what the interesting news stories are. Instead, Google groups together all the articles that are about a similar story.\n\nThat way you can see the major trends and then dive into lots of different articles on a single topic. Yes. Same thing with Facebook trending stories. If you're on Facebook, you'll see a list of big stories. But these are just groupings of a lot of individual articles and posts. You'll see the individual post if you click on the big story. We're starting to see some of the benefits and challenges of unsupervised learning. Sometimes, it's really expensive to get labels. I can always hire someone to come up with topics for my emails and to label each email with some topic, but then I'd have to pay that person's salary, plus it would take them quite some time to label everything. Humans take a long time to read emails, much longer than a computer does. That's the whole problem to start with, tt's cheaper for me in terms of both time and money if a machine learning algorithm can sort my emails automatically. On the other hand, it's often a lot harder to do unsupervised learning. The labels give us a lot of information. We can identify words that will tend to occur in spam emails, and words that tend to occur in emails that aren't spam. When we don't know what kind of hidden patterns we are looking for, it can be mush harder to find the pattern that's actually important. My algorithm may group together all the emails that come in before noon and make a separate group for all the emails that come in the afternoon. Well, that's not very helpful to me. But since I didn't tell the algorithm what I wanted, well, maybe I shouldn't be surprised. Plus it's easier to tell how well a supervised learning algorithm is performing.\n\nJust check what percentage of spam and not spam emails it's getting correct. But is there a really quantitative way to check whether my emails are getting a good grouping by topic? Or a way to check whether Google News is finding a good grouping of stories. These are hard problems. In the next video, we'll talk about some more supervised, and unsupervised learning problems, to make super clear what the difference is, and to get a sense of what the many different flavors of unsupervised learning. Then in the remainder of this module, we'll start to dig deep into how unsupervised learning really works in practice. We'll see how to actually build an unsupervised learning algorithm to help find these patterns in your data. We'll talk about methods for evaluating these algorithms, and we'll see some more real life examples of unsupervised learning in action.",createdAt:k_,updatedAt:k_,publishedAt:"2023-03-16T01:26:03.837Z"}},{id:1254,attributes:{title:"Tools and Applications: How They Fit Together",slug:"tools-and-applications-how-they-fit-together",duration:k$,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience12ToolsandApplicationsHowTheyFitTogether",objective:"Now you know what unsupervised learning is, let's learn how to identify these types of problems.",englishTranscript:"Welcome back. In the last video, we introduced two types of data analysis problems, supervised learning and unsupervised learning. We talked about why unsupervised learning is both particularly challenging and particularly important. And we discussed a data example. For supervised learning, we imagined we had labels on a bunch of emails, whether they were spam or not spam. And we wanted to predict the label of future emails. For unsupervised learning, we imagined that the emails were unlabeled. And we wanted to find hidden patterns in the data. For instance, we'd like to group the emails by topic. One interesting thing about these examples was that, for both cases, we had essentially the same data. It was really whether we had labels and what we wanted to do with the data that determined whether the problem was supervised or unsupervised.\n\nThis time, we will go through some more examples to test your knowledge of supervised and unsupervised learning, and also to get a taste of the really wide range of machine learning problems that are out there. OK, so here's a data analysis problem. Suppose I've gone around and measured the concentration in the air of a certain type of pollution. I measured the amounts of pollution at different locations, under different weather conditions, and at different times of day and different days of the week. Now I'd like to predict the air pollution concentration at a new location with known weather and known date and time. Take a moment to convince yourself whether this is supervised or unsupervised learning. Pause your video if you need to. This is an example of supervised learning. The data point is the collection of information about your surroundings, what your location is, what the weather is, what date and time it is. The label is the air pollution concentration, and we observe that. You'd like to predict the label under new conditions.\n\nIn the email spam example, our label was spam or not spam. That was a categorical label, because the label was a category. There's no inherent order on those labels. In this pollution example, the label is numerical and has an order. 2,000 particles per cubic centimeter is less than, say, 6,000 particles per cubic centimeter. I've got another data analysis problem. When Frank Sampson was a PhD student in the '60s, he spent some month in a New England monastery. He followed the novices' training there, and recorded their interactions with each other. We'd like to discover what the friendships or social groups among the novice monks are. Is this supervised or unsupervised learning? It's unsupervised learning. We don't have any labels that tell us which social groups exist or who belongs to which groups. We need to discover these patterns in the data in an unsupervised way. The Sampson data is old, but this is actually a modern problem. We often want to detect communities or groups in large social networks, like those on Facebook or Twitter or even Fitbit, only that today our networks are way bigger than what Sampson could record in his notebook back in the '60s.\n\nOK, here's another one. NASA uses rocket boosters to launch spacecraft into orbit. It's expensive to build a whole new rocket booster for every slight tweak, and it's difficult to recreate real atmospheric conditions in a wind tunnel for testing these out. So NASA uses computer simulations. But it takes a long time to run the computer simulations for different rocket booster's specifications. So NASA would like to measure the simulated lift, say, for different rocket booster's specifications and predict the simulated lift under other specifications. Is this supervised learning or unsupervised learning? It's supervised learning. The data point is the rocket booster's specifications, details that go into the rocket booster as built. The label is the simulated lift. We'd like to predict the simulated lift at new data points. It's interesting here that we're putting machine learning on top of other computer simulations. It's turtles all the way down. How about this one? I have some small cameras that I've installed to monitor birds' nests in New England. I don't have a lot of bandwidth to send video, so I'd like to compress the recorded videos before I send it to my laptop. Can I use supervised or unsupervised learning to help me out? It sounds quite different from problems we've encountered before, but you can think of this as an unsupervised learning problem.\n\nYes, if you consider the pixels at each time step as a data point, we can group together pixels that are similar in color and similar in time to achieve compression. We learn more about this in a later video. OK, one last example. Suppose I'm working at a computer science MOOC, one of these Massive Open Online Courses. And for a particular exercise, I'm having all of the students submit some code to implement a particular algorithm. I'd like to understand all the different types of errors they make so that I can easily grade the work and formulate my next lecture accordingly. Supervised or unsupervised learning? It's unsupervised. You can't anticipate all possible errors in advance, so it would be difficult to even try to label this data. You are trying to find hidden patterns in the types of responses students give based solely on their homework answers. Exactly. And this might help me personalize the educational experience going forward.\n\nStudents who made similar mistakes might profit from doing the same kinds of exercises next. Well, at this point, we've seen a really wide range of different supervised and unsupervised learning problems. But that's still only the top of the iceberg. In other modules, you'll see further examples of supervised learning. But for the rest of this module, we will be diving deeper into supervised learning. Today we've become experts at identifying unsupervised learning problems. But in the remaining videos, we'll see particular algorithms for unsupervised learning and how they work in practice. We'll develop a whole toolbox for unsupervised learning. Not only will you know an unsupervised learning problem when you see one, but you'll know how to solve it, too.",createdAt:la,updatedAt:la,publishedAt:"2023-03-16T01:26:02.320Z"}},{id:1253,attributes:{title:"What Is Clustering?",slug:"what-is-clustering",duration:lb,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience13WhatIsClustering",objective:"There are a lot of different problems that can be solved with unsupervised learning. Now learn about the most popular problem, clustering.",englishTranscript:"Hi, and welcome to the sub-module on clustering. So far in this module on unsupervised learning, we've introduced the concept of an unsupervised learning problem in data analysis. We learned that unsupervised learning means finding hidden patterns in data when we aren't given any labels. We saw in the previous videos that there are a lot of different problems that we can solve with unsupervised learning. But there's one special problem that stands out over decades now as being the most popular version of unsupervised learning. And this problem is called clustering. What is clustering? In clustering, our goal is to group data according to similarity. Let's examine each of these three key words, group, data, and similarity in more detail to find out what clustering is really about. Let's start with data. Imagine that I'm leading an archaeological dig.\n\nAnd every time someone on my team unearths an artifact, I have them record the position of the artifact. In particular, I can put down some marker near the archeological site. And we can record how far north and how far east each artifact was found relative to my marker. This is the artifact location. The location for one artifact will be a data point for us. When I list all of the location of the different artifacts that's my data set. Maybe I have some colleagues who are historians. I consult with them before my archaeological dig, and they tell me that three families lived in the area that we're exploring. I bet you're already able to identify where the three families live, and which artifacts belong to which family. Let's slow down this process to understand what you're implicitly doing. First, there's a notion of similarity here. In this archaeology example, we say that two artifacts are similar, or close, if they are close in physical distance. In other words, if they are close in distances the crow flies. For instance, these two artifacts are relatively close.\n\nThe distance between them is the length of this line. And these two artifacts are relatively far. The distance between them is clearly much larger. Second, when you think of where the three families live, you're grouping the data according to this distance. You're assigning each data point to one, and only one, family group. So that data points that are physically close, tend to belong to the same group. That is, you're grouping together similar data points. When we assign each datapoint to one, and only one group like this, we call the groups clusters. Here there are three clusters, one for each family. Clustering them is the unsupervised learning problem where you take your data and assign each data point to exactly one group, or cluster. And, of course, we want these clusters to be meaningful. We want them to give us some interesting insight into our data set. In this example, we imagine the clusters correspond to three different families who used to live in the area. The data points in any cluster are pottery shards or other items that we believe a single family discarded when they lived at this spot. Identifying the items that we think belong to one family will let us perform further scientific, or anthropological, or historical analysis of the artifacts.\n\nNow remember, in an earlier video we discussed the difference between supervised and unsupervised learning problems. There's a supervised learning problem that is somewhat similar to, but not the same as, clustering. That problem is called classification. In classification, like in supervised learning in general, we're given labels. For instance, consider our archeology example. If we were running classification on this data, we might not only know that there are three families, but we might know their names as well. Maybe the names are Lannister, Stark, and Targaryen. Classification is what we called our supervised learning problem when the labels are categorical. Remember, this means that the labels had no order, like the family names I just mentioned. Now, let's go back to our archaeology example. If we had a classification problem here, we'd have been given a bunch of data points with labels. For instance, you might imagine each family actually made a lot of pots with a sigil, or other identifying mark. Then our goal would be to find labels for any remaining unlabeled artifacts. That is, our goal will be to predict labels for new data points given the labels at all data points. By contrasting clustering, we don't know the families or their names.\n\nWe might know that there are three families. But we don't know anything at all beyond that number. And none of our artifacts come with labels. We have to discover how to group the artifacts from their locations alone. Labels may be hard to come by for various reasons. In the archeology example, we just might not be lucky enough for families from thousands of years ago to have carefully labeled their pottery for our benefit. On the plus side, clustering let's us find hidden groupings in data even when we can't run classification. But just as we saw when we compared supervised and unsupervised learning, clustering can be a more difficult undertaking than classification, since we don't have the information contained in the labels. You might be thinking to yourself at this point, well, that archaeology problem was easy. I could find the clusters myself. Why did I need a computer to do it? But it's not always so easy. Here we have two dimensions to each data point, the distance north and the distance east. Often, we are dealing with much higher dimensional data. In this case, we can't just make a picture of our data and look at it. Also, in the archeology example, we could plot each data point and look at them all at once. Well, you'll notice we had a lot less than a hundred data points in this example. If we had billions of data points, it wouldn't be so easy to visualize what's going on. Also, in this case, our data was numerical, each location was a number. That made it easy to make a picture and easy to see the distance between two points.\n\nBut sometimes, our data isn't numerical. It might take the form of words, or pictures, or genomes or something else entirely. And again, it isn't so easy to immediately see the hidden patterns in these types of data. But it will turn out that we can still get a lot of information from clustering. Finally, you might have hundreds or thousands of different data sets that need clustering. In this case, you simply don't have the time to find and write down all the clusters by yourself. It would be better to automate the process. In the remainder of this sub-module, we'll talk more about why clustering is both an important and a difficult problem. We'll see many more practical examples of clustering. We'll learn algorithms that let you find clusterings hidden in data in an automated way. We'll talk about why clustering has been so popular for so long. But we'll also talk about the limitations of clustering, and why you might be interested in other types of unsupervised learning problems as well.",createdAt:lc,updatedAt:lc,publishedAt:"2023-03-16T01:26:00.842Z"}},{id:1252,attributes:{title:"When to Use Clustering",slug:"when-to-use-clustering",duration:ld,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience14WhentoUseClustering",objective:"In the last video you learned that clustering is a particular form of unsupervised learning. Now go through more examples of clustering and see why and when you might want to use it in practice",englishTranscript:"Welcome back. In the last video we learned that clustering is a particular form of unsupervised learning. Generally in unsupervised learning we might be trying to find any hidden pattern in a set of data points without labels. In clustering, we want to find a latent grouping such that each data point belongs to exactly one group called a cluster. Last time we saw a particular example of clustering in action. We thought about how me might use clustering to help us understand the arrangement of artifacts at an archeological dig. In this video, we'll go through more examples of clustering and see why and when you might want to use clustering in practice. To start, sometimes we have a collection of data but we're not quite sure what to do with it yet. We might want to explore the data without a particular end goal in mind. Perhaps the data will suggest interesting avenues for further analysis. In this case, we say that we're performing exploratory data analysis. For instance, I might imagine that I have some data on meetup.com users.\n\nSo, roughly each data point corresponds to a person. In order to apply clustering I have to define a notion of similarity too. So, I'll say similarity is the number of common interests between two people. Well I can't actually feed a person or a user into my computer so, I need to be more precise about what constitutes a data point here. Perhaps what I've actually collected for each person is a list or vector of whether that person is interested in each meetup.com group on the meetup website. Then it's easy to calculate the similarity between two users. It's the total number of groups those two people are both interested in, after I apply clustering I might hope to find groups of users with broadly similar interests. Maybe after I run clustering it turns out these users are interested in playing music and watching musical performances. These users are interested in outdoor sports. And these users are interested in hackathons. What are some other times I might use clustering?\n\nBasically, I'm going to use clustering for problems that kind of look like classification. But when I don't have any labels that I could use to divide up my population and the direction I'm interested in. For one, I might just not know the labels in advance. Perhaps I have a corpus of documents. Like I have all the documents on Wikipedia or all the past articles from WIRED magazine or all the user reviews on Yelp. I would like to find out what topics or themes are represented in these documents. In general. I don't know the possible topics in advance so I might guess at some topics, but I can't really be sure in advance that I've got them all. Or topics I think of might not be the ones that would really appear in the corpus. So, ideally I'd like to discover the topics.\n\nIn this case, we can apply cluster. You might say each data point corresponds to a word and we can measure the similarity of two words by counting how many documents they both appear in finally, we can group together similar words to form our cluster. Okay, but we can't actually calculate how many documents two words both occur in just by looking at the letters of those words. So, we need to be more precise about what constitutes a data point. Here the data point that corresponds to any particular word could be a list or vector. Of whether this word occurs in each document, yes or no. Then we can actually calculate the overall similarity of two words. A student in my class last semester Julia, looked at research abstracts from professors like Stephanie and Me here at MIT in the Department of Electrical Engineering and Computer Science. She found the following words that tend to co occur.\n\nWords like temperature, graphene, devices and magnetic go together. Words like algorithm, model and data go together. And words like quantum, state, channel and energy go together. It turns out that the first set of words have to do with fabrication or manufacturing. The second set of words are related to data science and the last set of words are related to physics. But don't worry if you wouldn't have thought of these topics on your own. Here's the really neat thing. This student didn't use any advance knowledge about what MIT professors are working on. The different topics in the data came out in an automated way. This is just one of many applications where we might not know the labels of our clusters in advance.\n\nSometimes we don't have labels because it's expensive to label data. For instance, it might take a while for a human to label each data point. And the labels might be changing too quickly to keep up. Take the example of Google News we mentioned in a previous video. We might want to put news documents about similar topics in a cluster. But the topics in the news are changing everyday. For instance just hours after a new Game of Thrones episode comes out, there are great many articles about that episode. We would like to detect all those articles and group them together. And we would like to do this even when some surprise wildness about happens that we didn't anticipate in advance. In this case our data plan might be a list of which topics occur in a given document and how similarity measure between two documents might be account of how many topics are shared by the two documents or how similar the topic proportions are between the two documents. Alternatively it might be expensive to label data simply because there's way too much of it.\n\nFor instance there are ton of image sharing services right now. And as a result there's a wealth of images online. Consider some recent news articles about the discovery of a new species of bird. These birds were found to live in parts of India and China and this picture of the bird appeared in the news articles about it. We might want to automatically break up the image into the pixels corresponding to the bird, the pixels corresponding to the hand, and the pixels corresponding to the wall. In general, it's time intensive to label a substantial number of pictures and pixels within them. So, we might use clustering instead. In this case, each data point corresponds to a single pixel. And our measure of similarity might be how similar the pixel is to another pixel, in both color And location. Equivalently we might say our measure of this similarity is some notion of distance and color between pixels plus distance in location of the pixels in the image.\n\nSo, what does our data point need to tell us to calculate this similarity. Our data point for each pixel could consist of the color of the pixel. Followed by the location of the pixel. In this video we've seen a lot of different reasons to use clustering. We've also seen a lot of different types of data where you might apply clustering. In the next videos, we'll learn about the most popular algorithm for clustering or probably for any type of unsupervised learning.",createdAt:le,updatedAt:le,publishedAt:"2023-03-16T01:25:59.493Z"}},{id:1251,attributes:{title:"K-Means Preliminaries",slug:"k-means-preliminaries",duration:lf,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience15KMeansPreliminaries",objective:"Now that you have learned about clustering, learn about the most popular algorithm for clustering.",englishTranscript:"Welcome back, in the last two videos, we got a sense of what clustering is, as a particular form of unsupervised learning problem. Next, we're going to learn what is not only the most popular algorithm for clustering, but quite possibly the most popular algorithm within unsupervised learning. This algorithm is called the k-means algorithm. Why has this particular algorithm been so popular for so long? First of all, it's fast, not only are the steps of the algorithm simple and quick to run, but it turns out the algorithm can easily take advantage of parallel computing. By running calculations simultaneously across multiple cores or processors, we can get even further speed ups and running time. And finally, it's fast in programmer time.\n\nWe'll see that the algorithm is straight forward to understand, and to code, and doesn't require the large number of parameter tweaks some other algorithms need. It's easy to dismiss this last advantage, but when you need results on a timeline, it's no small consideration. Now, before we can learn the k-means algorithm, we need to understand the set-up and assumptions that go into the k-means clustering problem. Any method, in machine learning or statistics, will have assumptions built into it. To use any method effectively in practice, you have to know what those assumptions are. Our first assumption, in this case, is that we can express any data point as a list, or vector, of continuous values.\n\nFor instance, recall our archaeology example, we can write the location of an artifact as a list, or vector, with two elements. Some distance, say in meters east of a marker near our site, and some distance north of the marker, also in meters. This data point is 1.5 meters east, and 4.3 meters north of the marker. So our data set is a list of all these vectors. We have one vector for each data point. Here, we suppose that we have capital N data points. In general, the information we collect for any data point, doesn't have to be a distance relative to some marker in an archaeology dig. Usually we'll just call each component of the vector a feature. In this example we have two features, one for east and one for north. But more generally, we might have any finite number of features. Often we'll call the number of features d, where you might think of d as standing for dimension.\n\nNow, recall that clustering is grouping data according to similarity. We've defined what a data point is for the k-means clustering problem, now we need to define what similarity means in the k-means clustering problem. And remember, last time we talked briefly about how it's equivalent to define dissimilarity for two data points, instead of similarity. And that's what we'll do here, we'll say that the dissimilarity between two data points, in the archaeology example, is the distance as the crow flies. That is, the distance between these two points is the length of the line connecting them. This is sometimes called the euclidean distance. One pair of data points that is farther in euclidean distance than another pair, will also be farther away in the square of the euclidean distance. In this case, it will turn out we get the same result from k-means clustering, If we define dissimilarity as squared euclidean distance, and this is straightforward to calculate. The square of the distance between two data points, is the square of the distance between them in one direction, say east, plus the square of the distance between them in another direction, say north. Instead of writing out both square differences, a convenient way to write this sum is using a summation symbol.\n\nThis notation is like a for loop from computer science. Let's suppose capital D is two for the moment, since we have two directions. Then this notation says, for each index, little d, from 1 to capital D, add the distance in the little d direction. When capital D is 2, there are 2 terms in the sum. But remember, more generally, we might have more than 2 features and this notation let's us have as many as we want. Okay, we've got our data, we defined our dissimilarity, finally we need to group the data. What does this mean? What is the output we expect from our algorithm? Well, the reason k-means is called k-means, is that we say upfront that we expect some number, k, of clusters. In the archaeology example, we chose k equal to 3. We'll talk later about what happens when you don't know k in advance, but the k-means clustering problem assumes you do know k.\n\nNow, for each of these k clusters, we plan to get out a description of the cluster. In particular, we wanna know roughly what the cluster looks like, and which data points belong to it. To be more precise, to get a sense of what each cluster looks like, we'll get a cluster center for each cluster. Let's call the cluster center for the little k cluster, mu sub little k. And then we want to know which data points are assigned to each cluster. So, let capital S sub little k, be the set of data points assigned to cluster little k. Remember that in clustering, every data point has to be assigned to exactly one cluster. In this video, we've discussed the set-up and assumptions that go into the k-means clustering problem.\n\nNote, that the k-means clustering problem is more specific than clustering in general. We assumed that the data points can be expressed as continuous numbers. That's not really true if we have a yes\u002Fno vector, like in some of our examples from last time. Even if we encode yes as one, and no as zero, we can't get any other values than those two in our vector. We also assumed a particular form for our dissimilarity. There are lots of other forms we might use, and we'll discuss some of them later. And finally, we assume that there are exactly k clusters where k is known in advance, that isn't always true in applications. But now that we've got this setup in hand, in the next video, we'll see the k-means clustering algorithm in action.\n",createdAt:lg,updatedAt:lg,publishedAt:"2023-03-16T01:25:58.396Z"}},{id:1250,attributes:{title:"The K-Means Algorithm",slug:"the-k-means-algorithm",duration:"6:10",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience16TheKMeansAlgorithm",objective:"In the last video, we set up the k-means clustering problem as a particular subset of general clustering problems. Now develop the k-means algorithm",englishTranscript:"Welcome back. In the last video, we set up the k-means clustering problem as a particular subset of general clustering problems. In particular, we assumed that each data point is a vector of continuous values, and we're using square Euclidean distance as our dissimilarity measure between two data points. In this video, we'll finally see exactly what defines the k-means clustering problem, and we'll see how the k-means algorithm is a particular algorithm one might use to solve this problem.\n\nRemember, last time we saw that the output of the k-means clustering problem should be a set of cluster centers, mu sub 1 to mu sub capital K, and a set of assignments of data points to clusters. That is, capital S sub little k is the set of data points assigned to the little k cluster. The k-means clustering problem tells us exactly how we should choose the cluster centers and the assignments of data points to clusters. As we've said before, we want to group data points according to similarity, so the broad idea will be that we want to minimize dissimilarity within each cluster. We've defined dissimilarity between two data points. What's the global dissimilarity? Well, first, we'll say that the dissimilarity between any data point and a cluster center is the distance from the data point to the cluster center. We're still using square Euclidean distance here. Then the dissimilarity within a cluster will be the sum over the distances between data points in that cluster and the cluster center. And finally, the global dissimilarity will be the sum over the cluster dissimilarities. To recap, we calculate the global dissimilarity by iterating over each cluster, over each data point within the cluster, and over each feature of the data point. So now we can finally define the k-means clustering problem precisely.\n\nThe k-means clustering problem is to minimize this global dissimilarity. We will typically call this quantity the k-means objective function. We will want to minimize it by choosing a set of k cluster centers and assignments of data points to clusters. Remember, we're still assuming that we know the value of k. OK, so that's the problem, but how do we solve it? As with most problems in machine learning, there is no single silver bullet that will always get us the best answer. But in this case, there is an algorithm that seems to perform very well in practice, and it's called the k-means algorithm, or sometimes Lloyd's algorithm. Let's start by covering the algorithm in broad strokes, and then we'll get into the details. We don't know the cluster centers in advance, so we start by initializing them to some values. Then we alternate between two steps. One, we assign each data point to the cluster with the closest cluster center. And two, we update the cluster center to be the mean of all the data points in its cluster. We iterate these last two steps until we can't make any more changes. That is, we repeat these steps until we converge.\n\nNow let's talk through these steps in a little more detail. First, we need to initialize the cluster centers. You don't want to choose cluster centers that are far from all of your data. One option is to draw the cluster centers uniformly at random from the existing data points. There are other, more sophisticated initializations you might try as well, such as k-means plus plus. You might also see these in popular statistical software. However we choose to initialize the cluster centers, once we have some values for the centers, we can start iterating the main steps of the algorithm. Recall, we repeat these two steps until nothing changes. That might mean we check that the assignments of data points to clusters don't change between one iteration and the next, or we might check that the k-means objective function-- that global dissimilarity-- doesn't change between one iteration and the next. Now let's focus on the two steps within each iteration. First, we assign each data point to the cluster with the closest center. That is, for each data point, we compute the distance to all k cluster centers. Suppose little k has the closest center.\n\nThen we assign this data point to belong to cluster little k. Note that this calculation can be done completely separately for every data point. Therefore, it's extremely easy to divide up the data points across cores or processors and perform these calculations separately for speed-ups and running time. This is called an embarrassingly parallel computation. Finally, once we have the assignments of data points of clusters, we recalculate the cluster centers. In particular, we visit each cluster in turn. For the little k cluster, we collect all of the data points in this cluster and compute their mean. That is, for each feature, we sum up the values of the data points in this feature and divide by the total number of data points in the cluster. The result is another vector with capital D features, and this is the new cluster center value. Let's see a quick animation of the k means algorithm in practice.\n\nWe start with some initialization. For instance, as we said before, we might choose three of our data points at random to be the first cluster centers. Then we iterate between assigning data points to clusters and choosing the cluster centers. We again assign data points to clusters and calculate the new cluster centers, and again, and again. When we've reached this stationary point, we stop. A first and immediate question to ask is, does this algorithm always stop in finite time? Luckily for us, the answer is yes, always. You might try to convince yourself why this is true after finishing this video. We'll leave a more careful evaluation of the algorithm output until next time. In this video, we've defined what the k-means clustering problem is, and we've developed the k-means algorithm as a way to solve it. In the next video, we'll talk about how to know whether the results you're getting out of this algorithm are any good.",createdAt:lh,updatedAt:lh,publishedAt:"2023-03-16T01:25:56.809Z"}},{id:1249,attributes:{title:"How to Evaluate Clustering",slug:"how-to-evaluate-clustering",duration:"6:06",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience17HowtoEvaluateClustering",objective:"The last couple of videos have set up the k-means algorithm. Learn how to evaluate the output of this algorithm",englishTranscript:"Welcome back. In the previous video, we define the K-means clustering problem and described an algorithm that you might use to get a solution to the K-means clustering problem. This algorithm is often called the K-means algorithm, or Lloyd's algorithm. Now that we have a clustering algorithm in hand, we have to think about how to evaluate its output. Our discussion won't be specific to the K-means algorithm. But that's certainly one of many cases where we want to know how good our output is. Before we talk about how to evaluate a clustering of data, let's think back to the supervised learning problem of classification. In this case, we're given a set of data with labels. And we want to predict the labels for some new data. For classification, evaluation is straightforward if we have enough data. We typically set aside some data where we know the labels. The data points we feed to the algorithm, together with their labels, will typically be called the training data.\n\nThe data points we set aside for evaluation purposes are often called the test data. We use our classification algorithm to predict the labels on the test data and compare to the true labels. Then, we have an absolute universal scale. What percentage of the test data points did the algorithm get right? We can easily compare other algorithms. A better algorithm gets more labels right. But we can also see whether an algorithm on its own is good or not. Did the algorithm get a suitable proportion of the labels correct? Sometimes in clustering one may have access to a ground truth clustering of the data. In the archeology example, perhaps in some cases, the artifact clusters were known.\n\nSo we can compare to the known clusters. Or perhaps someone was paid to cluster some images into segments by hand. It's still not quite as easy to evaluate the clustering as a classification would be since any permutation of the labels leads to an equally good clustering. The only thing that defines the clustering is whether two data points belong to the same cluster or to different clusters. Measurements like the Rand Index and Adjusted Rand Index help us measure whether the clustering found by our algorithm really captures the information in the ground truth clustering. They do so by counting the pairs of data points that belong to the same or different clusters according to the algorithm and the same or different clusters according to the ground truth. Then they normalize appropriately. Measures like the Rand Index are called external evaluations because they require outside information about a ground truth clustering. The problem of evaluation is typically even trickier in clustering. Because there's rarely any ground truth information that we can use for testing. If we already knew the clustering, we'd be done. So how do we evaluate whether a clustering is good or whether one clustering is better than another?\n\nYou might think, well, we can check what the value of the K-means objective function is across different clusters. This will let us compare two clusterings. But it doesn't tell us whether a particular clustering is good or not on its own. It also might not capture exactly the patterns we're really trying to find in the data as we'll see later in this video as well as in future videos. So how do we evaluate whether a clustering is good? The short answer is, no one agrees. But the longer answer is that researchers have developed a number of useful heuristics. The first, and perhaps most useful observation, is that we are often trying to find some particular meaningful latent pattern in our data via clustering. Since we have a conception of what that pattern is already, we might be able to check if we found it by visualizing the results of the clustering. For instance, when we saw the archeology data, it was obvious what the cluster should be. , Similarly consider again the image segmentation example. We would like our clustering to distinguish the three different types of objects in the image. And we can easily check this by visualizing the clustering of pixels. Or consider the topic modeling example where we clustered words together.\n\nWe can list out the clusters of words and ask ourselves, do these words go together? Do they form a cohesive topic? Or are they words that don't really have a meaningful association? Another option is to use special tools for visualization like the GGOBI tool. We might have a data set where D, the dimension, is much higher than two. For instance, we might have collected data on a large number of different health metrics, like age, daily calories consumed, daily water consumed, weekly alcohol consumption, weekly miles driven, weekly exercise and minutes and so on. But this data isn't an image or a text data set. So it might be hard to plot meaningful pictures of the data. Tools like GGOBI help us find meaningful visualizations of high dimensional data for evaluating the clustering. While these methods are often more qualitative than quantitative on their own, they can be made more quantitative by incorporating some form of crowdsourcing. For instance Amazon Mechanical Turk workers could be asked to weigh in on the quality of the clustering. But even with the help of Amazon Mechanical Turk, or other crowdsourcing platforms, human evaluation can be expensive in terms of both time and money. So it's worth considering other automated forms of evaluation.\n\nBy contrast to external evaluations, there are a number of measures for internal evaluation that depend only on the data at hand. The basic idea of these measures is typically to make sure that data within a cluster are relatively close to each other and data in different clusters are relatively far from each other. An example of this is the silhouette coefficient. Another example is to split the data set into two data sets applying clustering to each and comparing the clusterings found across the two sub data sets. There are also a number of measures for evaluation that are specific to a certain domain or application. In this video, we've discussed some of the ways to evaluate the output of the K-means algorithm and clustering algorithms more generally. So what if I evaluate my output and I'm not totally satisfied? What are some potential problems with the K-means algorithm and how can it be improved? We'll answer these questions in the next few videos.",createdAt:li,updatedAt:li,publishedAt:"2023-03-16T01:25:55.333Z"}},{id:1248,attributes:{title:"Beyond K-Means: What Really Makes a Cluster?",slug:"beyond-k-means-what-really-makes-a-cluster",duration:lj,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience18BeyondKMeansWhatReallyMakesaCluster",objective:"Now that you understand the algorithm, what happens if the output is unexpected or unwanted? Learn how to troubleshoot the k-means algorithm",englishTranscript:"Welcome back. In the last few videos we introduced the K-Means clustering problem and a particular algorithm for coming up with the solution to it. This algorithm is called the K-Means algorithm or Lloyd's algorithm. Then we talked about how we might start inspecting the output from K-Means algorithm. So what happens if that output isn't what we expected or wanted? What do we do now? In this video and the next few videos we'll talk about troubleshooting K-Means. How can we make K-Means clustering better? In answering this question we'll often find ourselves leavings K-Means behind for different clustering algorithms and even different clustering problems. The first thing to do though is to make sure we're getting the best possible output from the K-Means algorithm. Now we know that the K-Means algorithm will stop, eventually.\n\nTo see that this is true, note that there are only finitely many possible clusterings. And the K-Means algorithm will move to a new clustering only if the new clustering decreases the K-Means objective. But just because the algorithm will stop eventually doesn't mean that the algorithm will stop quickly. That being said, and practice the K-Means algorithm tends to be quite fast. But you might find that on a particular dataset, the algorithm is taking a while to run. What do you do? There are various speed ups that have been proposed. These are often based on the following idea. In the K-Means clustering algorithm, as we wrote it, we needed to calculate the distance from every data point, to every cluster center. But you don't actually need to check every possible cluster center to decide which is the closest for a given data point. This observation relies on a fact known as the Triangle Inequality, that lets us ignore cluster centers that are relatively far away from a given data point in our calculations. Another related issue is that when the K-Means algorithm stops, it doesn't necessarily stop at a minimum value of the K-Means objective. Remember, this objective is the global dissimilarity. When the K-Means algorithm stops at a value that isn't the minimum value of the K-Means objective, we call the objective value it does stop at a local optimum.\n\nWe wish we had access to the true minimum, the global optimum, but that's a very difficult problem that we can't solve in general. One option to try to get closer to the global optimum is to run the K-Means algorithm multiple times for multiple random initializations. Then finally, we take the configuration of cluster centers and cluster assignments with the lowest objective function value, that is, the least global dissimilarity. While this does require a more total computation, these multiple runs can be performed completely in parallel. Moreover, another option to get better output from our algorithm is to use smarter initializations, like in K-Means++. This also has the potential to produce total running time. In fact, K-Means++, unlike vanilla K-Means, comes with theoretical bounds on the running time. Okay, but imagine somehow we magically had access to the global optimum. We could know the actual set of cluster centers and assignments of data points to clusters that minimize the K-Means objective function. And we could still be dissatisfied with our results.\n\nIn the upcoming videos we'll talk about some examples of how this could happen and what we can do about it. In this case the issue isn't our K-Means algorithm. The issue is the K-Means clustering problem itself. In this video we focused on how to improve the K-Means algorithm to give better results when it comes to minimizing the K-Means objective. But we also hinted that there might be deeper issues with the K-Means clustering problem in certain application domains. Recall that clustering is grouping data according to similarity. In the next few videos we'll dissect how each of those terms might have a different meaning than the one we assumed in the K-Means clustering problem. Next, we'll start by seeing a number of different ways we might define grouping.",createdAt:lk,updatedAt:lk,publishedAt:"2023-03-16T01:25:54.201Z"}},{id:1247,attributes:{title:"Beyond K-Means: Other Notions of Distance",slug:"beyond-k-means-other-notions-of-distance",duration:"4:58",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience19BeyondKMeansOtherNotionsofDistance",objective:"Explore the notion of what makes a cluster and what motivates us to look at clustering problems and models beyond K-Means clustering.",englishTranscript:"Welcome back. In the last video we started to hint that there might be various reasons one might want to go beyond the K-Means clustering problem. In this video, we'll see some concrete examples of when that might happen. As we said last time clustering is grouping data according to similarity. In this video we will examine different notions of grouping than we've seen so far in the K-Means clustering problem. Probably the most immediate and obvious issue with K-Means clustering in a variety of applications that it requires the user to specify the number of clusters K. Sometimes this is completely fine.\n\nFor instance, K-Means can be used for image compression. In this application, each data point might be the red, green, and blue intensity values for each pixel. The K-Means algorithm finds K colors that can be used to approximate the true, and typically much wider range of colors in the image. Here, K might be determined by how much we want to compress the image. In this illustration, we see how the increase in K increases the quality of the compressed image. In many other cases though, K is unknown in advance. Suppose we get some new data and we need to determine both K as well as the latent clusters.\n\nOne option is to use popular heuristics. One heuristic arises from plotting the global dissimilarity across a wide range of values of K. We can't choose K immediately from such a figure because the K, being a subjective function will always decrease as we increase K. If we just chose K as the value that minimized the K mean subjective function, we'd have to choose K equal to capital N, the number of datapoints, but that's a useless clustering. An alternative heuristic is to look for an elbow in a plot global dissimilarity as a function of K. That is, a point in the figure where the gain from new clusters suddenly slows down. We can set K to be the number of clusters at this elbow. Other, more theoretically motivated options, for choosing K exists as well. These options include the gap statistic, as well as methods that change the objective of K-Means.\n\nThe idea of these latter methods is to explicitly account for the fact that we pay a price for more complex models. In particular, we can start with the original K-Means objective function, and add a penalty term that grows with the number of parameters. Some appropriate penalties are given by AIC, BIC, or other so called information criteria. Once these penalties are added, we can plot the new objective function as a function of K. And now there will typically be a non trivial minimum. We can choose K to be the number of clusters of this minimum. There are broader issues with choosing K though. Sometimes there isn't a clear right value of K. Suppose we have a bunch of data on various organisms we've studies in some environment. We might find that if we favor smaller clusters, we cluster the organisms roughly into species, which is great.\n\nWe've picked up an interesting and meaningful latent grouping of the organisms. But if we favor larger clusters, we might end up clustering the organisms at the genus, family, or order level, which are all useful and meaningful classifications used in biology. Clustering at any of these levels would lend insight into the problem, but each level would result in a different number of clusters. In this case, it might be useful to try a hierarchical clustering approach, such as agglomerative clustering. These approaches explicitly model that some clusters might be composed of further sub-clusters. Finally, all of the clustering we've talked about so far puts each data point into one and only one cluster. This is called hard clustering.\n\nOn the other hand, we might sometimes have clusters that aren't perfectly separated. Some data points might be on the border between two or more clusters. For instance, recall our archaeological dig example. We might have an artifact that happens to be close to the cluster centers corresponding to two different families. When we go back to the lab and analyze this artifact, it would make sense to keep in mind that we're not sure which family it belonged to. More broadly we'd often like to express our uncertainty about cluster a given data point belongs to. An alternative to hard clustering that allows us to do this is soft clustering. In soft clustering we might allow each data point to have a different degree of membership in each cluster.\n\nFor instance, a data point might have a probability distribution over its belonging in different clusters. For many data points, this probability distribution might express that we're very sure that the data point belongs in one particular cluster. But in other cases near the border between clusters, the probability distribution for a data point might favor one cluster, but also make it clear that the data point could reasonably belong in another cluster. In the archaeology example, we might have a best guess as to which family generated some piece of pottery. But we want to encode that we're not certain, and another family could reasonably have made and used the pottery.\n\nIn the soft clustering case we might use alternatives like fuzzy clustering and mixture models such as Gaussian mixture models instead of k-means clustering. In this video we saw a number of examples with a different notion of clustering than the notion encoded within K-Means clustering. We saw that we might not always know the number of clusters before we apply clustering Moreover we saw that clusters can sometimes be at multiple scales and even nested within each other. Finally, we saw that clusters don't have to be an all or nothing proposition. Often it's useful to express that we're uncertain about the cluster assignments of some data points. Here we've explored the notion of what makes a cluster and it has motivated us to look at clustering problems and models beyond K-Means clustering. In the next video we will similarly dissect the notion of similarity.",createdAt:ll,updatedAt:ll,publishedAt:"2023-03-16T01:25:52.606Z"}},{id:1246,attributes:{title:"Beyond K-Means: Grouping Data by Similarity",slug:"beyond-k-means-grouping-data-by-similarity",duration:"4:36",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience110BeyondKMeansGroupingDatabySimilarity",objective:"Learn about different notions of similarity and clustering other than the squared Euclidean distance required.",englishTranscript:"Welcome back. So far, we've seen that K-means clustering is a powerful framework for clustering. But in the previous videos, we've started to explore how K-means clustering isn't always the best clustering framework for a particular problem. So sometimes it behooves us to look at clustering frameworks beyond that of K-means. We recalled that clustering is grouping data by similarity. In the last video, we looked at how we might be interested in different notions of grouping, or clustering, beyond that provided by K-means. In this video, we're going to look at how we might sometimes want to define \"similarity\" differently than the definition used by K-means clustering. It's worth recalling that K-means clustering assumes that we have K spherical, equally-sized clusters. This follows from our use of the Euclidean distance as the dissimilarity measure for K-means.\n\nUsing the squared Euclidean distance in our objective function is equivalent to using Euclidean distance and doesn't change this fact. So what could go wrong with the K-means definition of similarity? First, let's consider the case where our clusters aren't equally sized. For instance, consider an example where we have small circles filled with data points adjacent to a very large circle filled with data points. We might naturally think that there are three clusters here, corresponding to the three circles. In the K-means objective, though, the clustering where all the data points in a large circle are assigned to the same cluster has relatively higher, or worse, objective value. And the clustering where the points in the large circle closest to the small circles are assigned to the outer clusters has a relatively lower, or better, objective value. This is because the data points at the edges of the large circle are relatively far from the cluster center there, but relatively close to the cluster centers in the small circles. What can we do if we really want to separate out the three circles as clusters? One option is to use a model that specifically encodes clusters of different sizes. For instance, in the last video we mentioned Gaussian mixture models.\n\nWhen the mixture components are allowed to have different covariances, these models can capture the kind of clusters we're looking for here. One useful algorithm for this type of model is called the expectation maximization, or EM algorithm. Another issue that might arise is that we might encounter outliers in our data. Outliers are data points that are relatively distant from most of the data points in the data set. Because of its use of Euclidean distance, K-means clustering is typically very sensitive to outliers. Consider this simple one-dimensional data set. If we run the K-means algorithm with K equals 2, we discover that there are two clusters, as we expect. But if we add a single point very far away from our data, we're now stuck with one cluster over all the main data set, and the second cluster contains only the outlier point. This clustering doesn't capture the two clusters we think really exist in this data.\n\nWhat can we do? One option is to use an alternative measure of distance that is less sensitive to outliers. To calculate the squared Euclidean distance, we added up the squared difference between every pair of features shared by two data points. This is sometimes called the L2 distance. An alternative distance is the L1 distance, where we say that the distance is the sum of the absolute values of the differences between every pair of features shared by two data points. Pretty much everything we already did can be adapted to this case. In particular, we can derive an iterative algorithm, like K-means, for this problem. The algorithm is called K-medoids, since the optimal cluster centers are now medoids instead of means. The medoid in one dimension is the familiar median. Finally, we might be interested in clusters of different shapes than spherical clusters.\n\nConsider this example data set. Here, there seem to be two identifiable groups, or clusters, of points. But one group is situated inside the other. If we applied K-means with two centers, we'd just split the data into two sides of some essentially straight line. To capture the two clusters we think really exist here, we'd have to try something different. For instance, we might define a radial notion of similarity. Consider splitting the data into polar coordinates, or use kernel methods. Another alternative is to use the agglomerative clustering methods we mentioned in the previous video. Another potential issue with applying K-means is that it requires continuous numerical features.\n\nIf we're dealing with text or binary yes-no features or other features that aren't continuous and numerical, we might consider alternative notions of similarity. Another option, though, is to transform our data into a form more amenable to K-means clustering. We'll talk more about this in the next video. In this video, we explored using different notions of similarity and clustering than the squared Euclidean distance required by K-means clustering. In the previous video, we looked at different notions of clustering and clusters than those required by K-means. In the next video, we'll look at what we can do when our data isn't already in the form assumed by K-means.",createdAt:lm,updatedAt:lm,publishedAt:"2023-03-16T01:25:51.288Z"}},{id:1245,attributes:{title:"Beyond K-Means: Data and Pre-Processing",slug:"beyond-k-means-data-and-pre-processing",duration:"5:37",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience111BeyondKMeansDataandPreProcessing",objective:"Take a closer look at data and how to prepare it for the k-means algorithm.",englishTranscript:"Welcome back. In the past few videos we've been talking about troubleshooting the K means algorithm. In particular, we've been working our way through the definition of clustering as grouping data according to similarity. We talked about what happens if we have a different notion of grouping than K means, and we talked about what happens if we need a different notion of similarity than K means. In this video we'll take a closer look at our data.\n\nWhen we first step through the K means algorithm, we imagined as an example data set that we were looking at the locations of archaeology artifacts. In particular for each data point, we recorded a distance east of some marker and a distance north of the same marker, but there are a lot of other types of data out there that I might collect. How do I know when it's okay to run K means, and is there anything I can do to make it possible to run K means? Before running K means on your data, there are a bunch of questions you want to ask yourself. We'll start through the big ones here. First question, is your data featurized? As an example, suppose you've made a fitness app and you've collected a bunch of data about your users, this person is 45 years old, five feet seven inches tall, has a terminal bachelor's degree, occasionally posts some public funds in your app and so on. Remember that K means needs a vector of data.\n\nSo the first thing we need to do is turn all of this information into an orderly vector. Each entry in the vector will have a particular meaning. The first entry could be a person's age, the second entry could be a person's height, the third could be a person's education, and the fourth could be how many of ounces of smoothies this person drink in the past week. And maybe there are a bunch of moe entries. Each of these entries is called a feature. It's a feature or a trait of the data with a very specific meaning. We can imagine a big table or a matrix that collects these features across all of the users of the app. Once we've organized our data like this it is featurized.\n\nNext, remember that K means needs its features to be valued as continuous numbers. So the second question we have to ask ourselves is, is each feature a continuous number? In our example, age is in years. So we're all set there. We can convert height into inches. So a five foot seven person would be 67 inches tall. We could express education as years of education and write 16 for a Bachelor's degree, and 8.2 ounces of smoothie is also fine. The third question gets at a subtle point about K means. Are these numbers commensurate? To see the issue here, recall that K means assumes that we have spherical clusters. So the clusters have basically the same width in all directions. Imagine we have the following data set but notice the axes. One axis ranges from 0 to 10 and the other ranges from 0 to 1. So if we plot the axes on the same scale this is what the data looks like to the K means algorithm. So we might want to put all the data on the same scale and typically a standardization or a normalization procedure is used. For instance, all data on each feature can be separately standardized to lie between -1 and 1.\n\nA common alternative is to normalize the data in each feature to have a standard deviation of 1. Another subtle question is the fourth question, are there too many features? If I have a fitness app, I might want to find different groups of individuals who have similar health and fitness needs. But suppose I add a bunch of useless features. Like, for every word in the English dictionary, I add a feature for how many times this particular person uses that word in the forums. It sounds silly, but a lot of times we have features in our data that aren't entirely useful to the purpose at hand. When we know which features aren't useful, because of some domain knowledge, we want to get rid of them beforehand. But sometimes we don't know which are the useful and useless features. In this case there are some pretty magical algorithms that let us reduce the dimension of our feature space. That is they let us reduce the number of features to just get the most important features out.\n\nProbably the most widely used algorithm of this sort is Principal Components Analysis, or PCA. As an example, imagine I have a spring and a ball is moving back and forth on that spring. And I record this motion with a bunch of different cameras. Maybe five cameras. Well, the motion of the spring is really just one-dimensional. So I effectively have four extra features. PCA will help me pick out the one feature that really matters, even though it doesn't even necessarily correspond to just one camera. So it's often a good idea to run PCA on your data before running K means. In this case we refer to PCA as a pre processing step for K means. Okay, a fifth and final question to ask is, are there any domain-specific reasons to change the features? As I mentioned earlier, it's best to get rid of any features you know in advance won't be helpful for the final problem.\n\nTypically you or a colleague might have this knowledge due to being an expert about the particular data you're using. Note that any domain-specific feature changes should be done before running PCA or K means or anything else for that matter. It's also best to change any features that might not satisfy the assumptions of K means. Princeton says we talked about in this video and the last video there might be transformations of your features to something that better fits the assumptions of K means. It's nice to automate everything about using machine learning and statistics. But if there's knowledge you can use specific to your problem area can sometimes make a big difference if you use it. In this video, we talked about preparing or pre processing your dataset so that you you can run the K means algorithm with the best possible results. In the past few videos, we started to dig deep into the grouping similarity and data assumptions that goes into K means clustering. In the next two videos, we'll explore further the assumption of a fixed number of clusters, K clusters, and clustering itself.",createdAt:ln,updatedAt:ln,publishedAt:"2023-03-16T01:25:49.719Z"}},{id:1244,attributes:{title:"Beyond K-Means: Big Data and Nonparametric Bayes",slug:"beyond-k-means-big-data-and-nonparametric-bayes",duration:"4:37",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience112BeyondKMeansBigDataandNonparametricBayes",objective:"You have learned a lot about a fixed number of clusters. Learn about why that not always be the case.",englishTranscript:"Welcome back. In the past few videos we've been examining some of the assumptions that go into K-Means clustering. And we've talked about what you can do if you want to cluster data but your application or your data don't quite fit those assumptions. In this video and the next, though, we're going to take a step back. After all, K-Means clustering is just one particular type of fixed K clustering. That is, clustering where we assume the number of clusters is finite. Even when we don't know the number of clusters in advance, we often still assume that the number of clusters in our data is some fixed, finite number.\n\nIn this video we'll talk about why that assumption might not always be right. Often big data doesn't just mean that we have a single very-large data set. We might have streaming data, where new data is being added to our data set all the time. For instance, English-language Wikipedia along averages around 800 new articles per day right now. If we build a new fitness app, we hope that it will not only get new users all the time, but that those users will interact with it everyday. So what changes when we have streaming data. To illustrate, I like to think about my own experiences with Wikipedia. I recently read this really neat book, A Long Trek Home, about a couple who travel 4,000 from Seattle to the Aleutian Islands in Alaska by hiking and rafting. So this makes me go on Wikipedia and look up the Aleutian Islands. And that leads me to an article on the Pacific Ring of Fire.\n\nAnd soon I'm reading about volcanos, earthquakes, and plate tectonics. And the thing about Wikipedia is, no matter how many articles I've read in the past, there are always new topics to read about and learn about. I like to think of this as some sort of Wikipedia phenomena. Suppose I have 100 articles from Wikipedia, I could cluster those article and find some topics to group articles together, but if I have a thousand Wikipedia articles, I expect to find new topics I didn't see in the first 100 articles. And as I read more and more Wikipedia, getting to millions and billions of articles, I expect to keep finding new topics. In this case it might be okay to run clustering with some fixed number of clusters for a hundred articles, or any fixed number of articles. But as the size of my dataset grows, I always expect to see more groups, more clusters.\n\nSo I want the number of clusters to grow with the size of my data. Or another way to think of this is, I want the complexity of my model to be able to grow with the size of my data. We see this in a lot of other types of data as well. Humans have been studying and discovering new species for a very long time now. And yet at any time you can do a search on Google News and be sure to find articles about all kinds of new species that were discovered in the past week. Humanity has discovered a ton of different species, but we're still discovering new ones all the time, and we're not done yet. Thinking back to our imaginary fitness app, we could imagine asking people about their exercise routines. And no matter how many people we ask, we might expect that we will keep discovering new and unusual exercises that we haven't seen among the previous users we've asked. In genetics, we might expect to find new and unknown ancestral populations, as we study more individuals' DNA. Or supposed we look at newborns in a hospital. As we study more and more newborns, we might expect to find more new and unique health issues among those newborns. And we want to be prepared for this new health issues so that we can best treat this newborns and not less in important diagnosis. We might consider a social network.\n\nAs more and more people joined the social network, we expect to see new friend groups and interests represented in the network. Or we can imagine image database, like the images on Instagram. As we examine more and more images, we expect to find more unique objects on those images. Objects we haven't seen before in previous images. In all of these cases we wouldn't want a fix number of clusters, K, for clustering. We want K to be able to grow as the size of the data grows. Once solution to this problem is provided by non-parametric Bayesian methods. Non-parametric makes it sound like there are no parameters, but in the context of the term non-parametric Bayes, non-parametric actually means many parameters, or infinitely many parameters. These methods let the number of instantiated parameters grow with the size of the data. While these methods are more complex than K-Means clustering. Some recent work on MAD-Bayes shows how to turn non-parametric bayesian methods into K-Means like problems, and algorithms for clustering in particular, and unsupervised learning in general. In this video, we've talked about why you might not always want a fixed number of clusters K. Even when K isn't known in advance, the assumption that K is the same throughout some streaming dataset might be problematic. In the next video we'll talk about cases where you want to find hidden patterns in your data, but clustering might not be quite the right type of pattern.",createdAt:lo,updatedAt:lo,publishedAt:"2023-03-16T01:25:48.138Z"}},{id:1243,attributes:{title:"Beyond Clustering",slug:"beyond-clustering",duration:"5:05",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience113BeyondClustering",objective:"All of the videos before have talked about clustering. Now learn why that may not always be the best method.",englishTranscript:"Welcome back. In the previous video, we started talking about cases where traditional clustering just doesn't cut it anymore. Last time, we looked specifically at the case where we can't assume there's a fixed number of clusters in our data set. We might want the complexity of our model to grow with the size of the data. In this video, we'll talk about cases where clustering doesn't quite represent the hidden patterns we want to find in our data. Let's think back to the image analysis example from last time. For the moment, suppose that I have a website where people post pictures of their pets. I can imagine that I might like to cluster these images. And I'd be very happy if my clustering algorithm put all of the images of cats in one cluster, all of the images of dogs in another cluster and all of the images of lizards in yet another cluster.\n\nOne way I could represent this clustering is in a table or a matrix. Each row of the table or matrix will correspond to one of my images. I'll put a one in the column that corresponds to the cluster this image belongs to. So here pictures one, two and seven belong to the same cluster, pictures three and five belong to the same cluster and picture four is in its own cluster. But it seems something's missing. What if someone post a picture of both a cat and a dog? Or a cat and a dog and a lizard? Or what if they post the picture that doesn't happen to have any animals then? The problem with clustering is that each data point has to belong to one and only one group or cluster. We might express uncertainty about which group that is, but ultimately we believe that the ground truth is that there's exactly one true group that the data point belongs to. So now, think back to our website and people's pet pictures. Here we might want our data points, the pictures, to belong to multiple groups simultaneously or no groups or just one group.\n\nAny of these options should be allowed. In this case, we call the groups features instead of clusters, and the underlying structure is a feature allocation instead of a clustering. Note that this is a different use of the word feature than we saw on previous videos. A similar idea is to say that the data points exhibit mixed membership. They can belong to multiple groups at the same time unlike clustering. Sometimes, clustering is called the mixture model, since each day the point comes from one of a bunch of different groups. The bunch of different groups forms the mixture.\n\nThen, the case where each data point can belong to multiple groups at the same time is called then admixture model. The word comes from the genetics literature. But essentially all of these terms, feature allocation, mixed membership, and admixture, capture the idea that data points can belong to multiple groups simultaneously. We see this sort of motif crop up in lots of different data. Suppose we're analyzing a corpus of documents, and we want to find the topics or themes that occur. We might look at all of the documents and past issues of the New York Times. So natural topics that we might find could include sports, arts and economics.\n\nBut then suppose we read a review of the movie Moneyball, based on the book by Michael Lewis. Well, it's about the arts because it's a movie review. The review is also about sports because the movie is about baseball players. But the review is also about economics because the movie is about using analytics and statistics to trade players and choose the best players. So, if we think of a document as a data point, we want our data points to be able to belong to multiple groups. Similarly, if we study genomics we often see that an individual's DNA may be comprised of parts from a number of different ancestral groups. If we study politics, we may see that individuals votes actually represent a number of different political ideologies. If we study social networks, we may see that an individual's interactions on a social network, represent various different personal identities such as a work identity, a family identity and a social identity separate from the first two identities.\n\nThere are number of different models and algorithms that let us go beyond clustering and capture this kind of mixed membership structure in data. Perhaps the most popular of these algorithms is Latent Dirichlet Allocation or LDA. LDA was originally designed for text data, and it's popularity maybe due in part to the rise in massive amounts of text data available online. But it could be applied much more widely to other types of categorical data, including genetics data. Other K-means like algorithms for this feature allocation problem, have been derived using MAD-Bayes. For instance, the DP-Means and BP-Means algorithms have been used to study tumor hydrogenated. In many tumors, multiple different types of cancer are present. It's important to identify all of the different types of cancer, so as to design effective treatments. In this submodule, we've studied clustering in general and the very popular K-Means algorithm for clustering in particular. In the past couple of videos, we got the taste of when and how you might start to go beyond the clustering paradigm. In the remaining videos of the submodule, we'll get some hands on experience with clustering and mixed membership. Not only will you know all about clustering and K-Means, and it's extensions, but you'll see them used on practical problems. But, it's worth emphasizing there are many other forms of unsupervised learning out there, beyond the ones we've seen so far. Clustering and mixed membership and feature allocations are just the beginning.",createdAt:lp,updatedAt:lp,publishedAt:"2023-03-16T01:25:47.059Z"}}]},instructors:[{id:642,name:lq,position:a,nationality:a,achievements:a,focus:a,bio:lr,learnMoreURL:a},{id:643,name:io,position:a,nationality:a,achievements:a,focus:a,bio:ip,learnMoreURL:a}]}},{id:143,attributes:{title:"Dimensionality Reduction & Spectral Techniques",slug:"dimensionality-reduction-and-spectral-techniques",description:"Understand how clustering occurs in networks and find meaningful clusters that respect hidden structure in the data.",prerequisite:a,duration:"40 min",objective:a,createdAt:"2023-10-18T21:49:04.704Z",updatedAt:"2023-10-18T21:49:45.626Z",publishedAt:"2023-03-16T13:13:32.720Z",chapters:{data:[{id:1242,attributes:{title:"Networks and Complex Data",slug:"networks-and-complex-data",duration:"4:07",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience21NetworksandComplexData",objective:"Learn about tools that tell us a whole lot about data.",englishTranscript:"In the past few sections, we have already seen many interesting and very useful methods to find grouping structure in data. For example, the k-means algorithm. In the next few sections, well see other very interesting tools that tell us a whole lot about our data. In particular, some of these work even if you can't apply k-means. So, let's quickly recap a few things. For clustering and many other tasks, our data comes in form of data points. Each data point is a feature vector. You can think of it as a string of numbers and each number describes a feature. For example, in the introductory email example, each email is a data point.\n\nIt is described by the words it contains. So, we have a huge vector and each entry in this vector corresponds to a word. The number for each word means how often this word occurs in the email. With such representations, we can use k-means, but then we are not always so lucky. Let's think about some examples. The feature vectors can contain too much useless information or noise. We may not have such vectors at all. In such cases, we may be able to construct new features from whatever we had. Sounds weird? Well, soon, we will see how to do that. First, let's look at some examples. Data can contain a lot of measurements. That is each data point is a huge feature vector with many entries. Think of a person being a data point and the descriptions, the variation in the person's genome.\n\nThat can be a lot or think of a collection of images that are face portraits. Each image is a data point and is described by a few hundred thousand pixels, but then some pixels will be more meaningful than others. If you look at the upper right corner of face board read images that is most likely going to be not so relevant, it is constant or varies randomly across images since it's background. So, there's not a whole lot of information. The important information is how the images deviate in the major way from some average image. For example, there will be variations in hair, glasses, beard and so on. These major axis of variations are what captures the data in a more meaningful way and maybe there are actually matched your axis and pixels. Why? Some groups of pixels tend to vary together, because they belong to some subregions like eye brows and chin.\n\nThese few trends can help compress the data. Similarly, user ratings can be much more easily understood with patterns. My ratings for movie can be understood more easily, if you know that I like comedies or action movies. Finding and recognizing such patterns can help reduce the complexity in the data to reduce noise, and bring out important trends, and also to compress it. We will see some important methods for finding such patterns and then sometimes we do not even have feature vectors. Think back about the social network of monks that Sampson recorded back in the 60s. The monks are our data points and we want to find groups of friends among them. Similarly, of course, for much larger networks like Facebook. Can we use k-means to find groups here? It's going to be hard. We don't have any feature vectors. All we know is who talks to whom.\n\nSo, we have placed the monks and we have relationships who talks to whom. This gives us a graph. We can draw all the monks on paper and draw lines between those who talk. The structure of points and lines is a graph. The lines are called edges. This graph tells us a whole lot about the patterns in our data. Groups of friends will have lots of edges between them and between such groups, there are not many edges. We will see how a little bit of math will magically find us the groups in the graph. In fact, for all of the problems I mentioned on a high-level, the same approach will work. We are going to create new features that represent our data point. These new features reveal a lot about the hidden structure in the data. Each data point can be represented by those new features. Where do they come from? We construct them by looking at the entire data. Sounds like magic? We will soon see how the magic works.",createdAt:ls,updatedAt:ls,publishedAt:"2023-03-16T01:25:45.893Z"}},{id:1241,attributes:{title:"Finding Principal Components in Data & Applications",slug:"finding-principal-components-in-data-and-applications",duration:"5:11",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience22FindingPrincipalComponentsinDataApplications",objective:"Now learn about finding major patterns in data using principal component analysis.",englishTranscript:"We will now look at one of the most widely used methods for finding major patterns in data, principal component analysis, or PCA. It is most often used when each data point contains a lot of measurements, and not all of those may be meaningful, or there's a lot of covariance in the measurements. In the last section, we came across some examples, genomic data, or images. PCA describes data by summarizing it in typical patterns. These are the principal components. Let's look at a concrete toy example that illustrates what kind of information we may extract.\n\nHere, we see a matrix of ratings for holiday destinations. Each of your friends has given a rating to four holiday places. We have first, a spa hotel in the Alps. Second, a Hawaiian beach. Third, a trekking tour in the Himalayas, and fourth, a deep sea scuba diving class. Anne really likes the spa in the Alps, Bill is up for Hawaii, while Maggie loves the Himalayas. Can be better understand this data? Are there underlying patterns? Each person here is a data point and their features are the ratings. We want to find patterns such that each person's rating can be explained as a combination of patterns.\n\nThese patterns are the principal components. Each principal component is a vector. Here, the two principal components are in order of importance and slightly rounded. Component one is minus 1, plus 1, minus 1, plus 1. And component two is plus 1, plus 1, minus 1, minus 1. Do these patterns tell us anything? Pattern one has high ratings for trekking and scuba and low ratings for beach and spa. We could say pattern one corresponds to the amount of adventure for each place. If I like adventure, pattern one tells my likings. Pattern two prefers Alps and Himalayas over beach and scuba. This pattern expresses a liking for mountains. Now, each person's ratings can be expressed by other two patterns and a mean rating. And ratings, for example, are approximately the mean rating minus 6.7 times the adventure pattern plus 2.2 times mountains. She has a negative coefficient for adventure. She tends to rate adventurous things low. And she likes mountains over sea. Indeed, she rates the Alpine spa the highest. We can do this with each person, and the coefficients for each pattern are telling. In fact, these two patterns pretty much explain people's ratings here. Instead of four features, one rating for each location, we now have only two, adventure and mountains.\n\nWe can now also visualize the data. For each person, we compute the two coefficients, one for each pattern, and plot each person as a data point on these two axis. Such visualizations can be tremendously helpful. Here, natural clusters emerge. Maggie and Jenn are really adventurous, whereas Chris and Bill want to relax at the beach. We just saw an example of principle component analysis, or PCA. It finds the major axis of variation in the data. These were our patterns. Each data point can be expressed as a linear combination of these patterns or components. Let's briefly look at two other famous real world examples. First, components of face images. We treat each image as a vector. We take the pixelized columns and stack them on top of each other. We can find a blurry average face. Equally, we can find a few principal components. We see each component looks like a ghostly face. These components have been termed Eigen faces. The Eigen comes from the verb eigenvector, as we will see in the next section. Each face image is then an overlay of rated versions of these components. For each image, we only need to know its coefficients. This is useful for compression, but also for understanding the space of face images. Our final example is a study from genetics. It asks, can you see someone's origin from their DNA? Researchers collected data from roughly 1,400 Europeans.\n\nEach person is described by his or her genetic variations. That is, 200,000 features. The researchers found the two principal components of the data and plotted each person, just like we did with our ratings. In the plot, the data points form clusters. In these clusters, we can see the map of Europe. The two principal components came only from genomic data and still, the map emerges. We see that indeed, the principal genetic variations in Europe are highly related to geography. PCA indicated that for us. Let us recap. The principal components capture major patterns, and each data point can be expressed by these components. We have created new features. Typically, a few components are enough. If each component describes a feature, we have reduced the number of features. This is called dimensionality reduction. Next, we will see how to compute the principal components.",createdAt:lt,updatedAt:lt,publishedAt:"2023-03-16T01:25:44.761Z"}},{id:1240,attributes:{title:"The Magic of Eigenvectors 1",slug:"the-magic-of-eigenvectors-1",duration:"4:50",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience23TheMagicofEigenvectors1",objective:"You just learned about PCA. Now learn how to compute it.",englishTranscript:"In the last section, we saw the usefulness of Principal Component Analysis, or PCA. Now we learn how we can actually compute it. The magic is called eigenvectors, and indeed, we will see many more benefits of eigenvectors. In PCA, we want to find principal patterns. Typically, these patterns do not affect a single feature, or in the holiday example, a single rating. They affect many ratings together. Ratings for several holiday locations go up and down together, governed by some latent factor like adventure. This going up and down together is the covariance of the ratings. They co-vary from the mean.\n\nWe want to find the major directions of covariance. To find those, a helpful tool is the covariance matrix of the data. Let's see how we construct it. For each holiday location, we compute the average rating. We then look how for each person, the rating is deviating from that average. For example, the deviation of Hawaii for Anne is Anne's rating for Hawaii minus the mean rating for Hawaii. The covariance of two locations-- say, Hawaii and the Himalayas-- measures how much the ratings both vary in the same direction.\n\nFor each person, we multiply the deviation for Hawaii and for Himalayas. If both ratings are larger than their mean, then the product is positive. If they vary from the mean in different directions, the product is negative. We do that for all people. The covariance of Hawaii and the Himalayas is then the sum of those. The covariance matrix contains these terms for all possible pairs of features. For D features, this is a D by D matrix, and the [? i-j-th ?] entry is the covariance or features I and J. In our matrix, for example, the relaxing Alpine location has negative covariance with deep sea diving. People tend to like either one or the other. The principal components that we are looking for are the eigenvectors of this coherence matrix. What are eigenvectors? We can define them by our matrix and vector products. We can multiply every length D vector by a D by D matrix.\n\nThe result is another vector. You can think of a vector as a string of numbers or as a direction or arrow in D dimensional space. When we multiply it by a matrix, the vector gets rotated and scaled. If the vector is an eigenvector, its direction stays the same. It only gets scaled. The amount by which it gets scaled is the corresponding eigenvalue. Each eigenvector has an eigenvalue. In some sense, the eigenvectors capture the major directions that are inherent in the matrix, and the larger the eigenvalue, the more important is the vector.\n\nThis can be written as an equation. We won't solve that. The computer can do that for us. Let's rather try to understand what this means for covariance matrices. Here is another data set. The data are points in 2D on a plane. Each data point has two features. In this data, either both features are low or both are high. In fact, there may be an exact relationship, and the rest is noise. The principal components here literally capture the major axis of variation. The first and most important eigenvector points in the direction in which the ellipse of the data extends most-- that is, the vector with the largest eigenvalue. The eigenvalue tells the amount of stretch. That's also the major relationship between the features. We found this automatically. The second eigenvector is perpendicular, and given that constraint, shows the next largest direction. This is what it looks like for another data set with clusters. If we encode every vector only by the first principal component, we shrink all the data onto this component.\n\nBut even though everything is now on one line, the clusters are still separated. We did not lose much. All of this can be done in high dimensions too. This last example actually points to another question. How many components do we need? Some of them may be useful, and the rest is noise. For that, we can look at the eigenvalues. If we sort them, there's often a gap after which all are small. We pick all the eigenvectors that correspond to large ones and ignore the rest. In summary, we have seen that we can compute the PCA from the covariance matrix. The eigenvectors with the largest eigenvalues are the principal components. PCA is very useful for a wide range of settings. We've seen a few. It can also be used to reduce the large feature vectors that come from deep neural networks, for example. Of course, it can't do everything. For example, it can only capture linear relationships in the data. Still, it is one of the most widely used data analysis tools.",createdAt:lu,updatedAt:lu,publishedAt:"2023-03-16T01:25:43.640Z"}},{id:1239,attributes:{title:"Clustering in Graphs and Networks",slug:"clustering-in-graphs-and-networks",duration:"4:33",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience24ClusteringinGraphsandNetworks",objective:"Learn how clustering occurs in graphs and networks.",englishTranscript:"So far, in our data, each data point was described by a set of features. Now, we no longer have that luxury. Take a social network. We may know nothing about the people in there, so we have no feature vectors. But we know who talks to whom, that is, who is connected. In the Internet, two websites are connected, if one links to the other. Then there are biological networks, computer networks, and data represented as networks, such as images. So our data is given as a network or a graph.\n\nEach data point, for example, a person in a social network, is a node in the graph. The connections are edges. We can draw this on paper by drawing dots for nodes and and lines for edges. We may have weights on the edges. The larger the weight of an edge, the stronger is the connection between the two nodes. While small graphs on paper may still be easy to get, making sense of a huge social or biological network is much harder. The first thing to understand may be the community structure of the network. Are there dense groups of friends for example? We may also want to partition other networks into well connected groups. We lot from cluster analysis and the graph. In social networks, the cluster structure affects how epidemics spread. Cluster analysis of dynamic social networks teaches us about the formation of friends and opinions. In Biology, clusters of proteins can be indicative of functionality. And in science clusters of coauthor networks indicate research topics. There's a lot of other examples. In fact, we'll see that we can even use graphs to cluster data that is given as feature vectors and even in cases for key means fails, this happens for example if the data forms some sheets or curls.\n\nLet's try to make this a bit more formal, what is a cluster in a graph? There are in fact many definitions, but many of them share some intuitive criteria. Let's have a look at those. Intuitively, a cluster consists of points that are well connected with each other. That is, there are lots of edges between them. The number of edges within a cluster is also called the volume. The volume per node is the density. We want this to be large, that's the first criteria. Second, between different clusters there should not be many edges. After all if there were many edges, why would you not declare this a single cluster? This separation of clusters is measured by the cut value. If we wanted to cut out the cluster from the graph, we would need to disconnect the edges that connect the cluster with the rest of the graph. The cut value is the number of edges we need to cut or their rates. So we could just find a group of nodes with a low cut value. Is this a good criterion? Certainly, most clusters have a low cut value. But there are clusters that have an even smaller cut. Look at this outline node, it has only one edge so it's cut value is one, that's super low. And hence, by our definition, makes a good cluster.\n\nMaybe that's not exactly what we were aiming for. Instead combining cut and volume strikes a balance between the size of the clusters and their separation. Popular examples of such combined criteria are conductance and normalized cut. Both divide the cut by a measure of volume. We want to minimize this criteria. If the cluster is small or has small volume then the denominator is small and the criterion is large. If the cluster is large then the other cluster, that is the remaining nodes, is small. By this measure, good clusters are not too small, internally well connected and separated from the rest of the nodes. Clusters can also be seen in the so called adjacency matrix. We can construct a matrix that has a row and a column for each node. The entry n row I and column J is one, if there is an edge between row I and J, and its there otherwise. If your other nodes, or rows and columns by cluster, then the matrix has a block structure.\n\nTypically we do not know the clusters, and the nodes are shuffled arbitrarily. So the structure is hidden in what looks like chaos. There are many other flavors of clustering. For example, modularity measure indicates how high the density of edges is within groups, compared to a baseline graph where edges occur randomly. Or, sometimes, if the graph is massive, we are only interested in very local clusters. For such local clustering, we don't need to look at the entire graph. Finally, in correlation clustering, we have information for pairs of points. If they are similar, they should be in the same cluster. If they should be separated, we draw a negative edge between them. We then cluster points to respect as many of these given relationships as possible. Here, we'll continue with the goal of finding dense, well separated clusters. Next, we'll see how to do it.",createdAt:lv,updatedAt:lv,publishedAt:"2023-03-16T01:25:42.626Z"}},{id:1238,attributes:{title:"The Magic of Eigenvectors 2",slug:"the-magic-of-eigenvectors-2",duration:"5:16",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience25TheMagicofEigenvectors2",objective:"See how useful eigenvectors can be when describing the connectivity structure of a large network",englishTranscript:"As we have seen graph clustering can be useful to understand community structure in a large network. But how do we compute that? What is important here is the global connectivity structure of the graph. To capture that connectivity structure, we will see that, once more, eigenvectors can be really useful. They encode an impressive amount of information about the structure of the graph. We almost just need to read it out to find the clusters. The resulting methods are called spectral methods, and more more precisely, spectral clustering.\n\nThe spectrum of a matrix is the set of its eigenvalues. We've seen eigenvectors for PCA, but what is the matrix for a graph that gives us the magic eigenvectors? The matrix is the so called Laplacian of the graph. Let's construct it step by step to see what it is. Here's an example graph for illustration. We number the nodes one through n. For a graph with n nodes, the Laplacian is an n by n matrix. It has one row and one column for each node. The entry in row i and column j corresponds to a potential edge between nodes i and j. If there's an edge, the entry is -1, otherwise it's 0. The entries on the diagonal are the degrees of the nodes. The degree of a node is the number of edges that it meets. Often this Laplacian is normalized by appropriately dividing by the degrees or the square root of the degrees. One can also define all of that for graphs.\n\nNow what is so special about this matrix? We'll see. Let's have a look at the eigenvectors of the Laplacian. Remember, each eigenvector comes with an eigenvalue. We'll sort them by the eigenvalues in increasing order. We will be interested in the largest and the smallest one. The smallest Eigen value is always 0. In fact, here's our first structural relation. The number of 0 eigenvalues is exactly the number of disconnected parts of the graph. These parts are called connected components. For example, this graph has three connected components and this one has one. Let's assume our graph is one component and let's have a look at the eigenvectors. The eigenvector for eigenvalue 0 is the all once vector. That's the same for all graphs, and hence pretty uninteresting.\n\nSo let's go on to the second smallest eigenvalue and vector. Each item vector has n entries, one for each node in the graph. Here are two graphs, and we color code the notes by the value in the item vector. Now this is interesting. The values in the eigenvector reflect the graph's vector. Close by nodes have similar colors. In fact, we could find a threshold to partition the nodes. All nodes with values above the threshold go in one group and all others in the other group. If we do this here, we actually find a meaningful clustering in the graph. This eigenvector has a lot of good information for us. What about others? The nest few eigenvectors complement that information. Here's a graph with four natural clusters.\n\nOur second eigenvector partitions this nicely into two groups, each consisting of two clusters. The color coding for the third eigenvector also reflects clusters in the graph, but partitions the graph differently. If we take this information for both eigenvectors together, we can easily find all four clusters. This seems extremely useful. The eigenvector is kept to the graph structure. In fact, they gave us more. Remember, our nodes do not have feature vectors. Now, we can give each node one feature value from each eigenvector and we suddenly have vectors. These new features are called an embedding. PCI also gave an embedding.\n\nFor PCI, we use the vectors with the largest eigenvalues. Here, the smallest ones will be important. What do these features mean? Just like we plotted the fetus from PCA, let's take the second and third smallest eigenvectors and use them as coordinates for the nodes. Here's what this looks like. Nodes that are connected are positioned close by on the plane. Our embedding here wants to minimize the stretch of the edges. And clusters in the graph become clusters in 2D. So far, we've used the eigenvectors with lowest eigenvalues. What about the ones with the largest eigenvalues? Here is what happens if we use these as an embedding. They have the opposite effect.\n\nThey maximize the stretch of the edges. Nodes that are connected are placed far away on the plane. This is the same graph, but looks so different. Is there an explanation? In fact, the eigenvectors minimize or maximize a different score. Remember, each node gets a value. For each edge, we take the difference of the adjacent node values and square that. We do this for all edges and sum it up. The eigenvectors with small eigenvalues minimize these differences. They give similar values to connected nodes.\n\nAnd the difference can be viewed as the stretch of the edge. The eigenvectors with large eigenvalues maximize the difference, they get very different values to connect the nodes. Overall, we see that the eigenvectors and eigenvalues contain a lot of valuable information about the graph structure, as we may suspect now, they are very useful for clustering. In fact, they contain even more information about the number of paths between any two nodes or about the importance of nodes and edges.",createdAt:lw,updatedAt:lw,publishedAt:"2023-03-16T01:25:41.104Z"}},{id:1237,attributes:{title:"Spectral Clustering",slug:"spectral-clustering",duration:"5:30",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience26SpectralClustering",objective:"In the past videos we have seen criteria for finding communities and we've seen that eigenvectors capture important properties of the network. Now we'll put everything together.",englishTranscript:"Communities and networks give us a much better understanding of network properties, friendships or functional groups. In the past sections, we've seen criteria for finding communities and we've seen that eigenvectors capture important properties of the network. Now we'll put everything together. Let's start with a graph where there are no connections between communities, and build this so called adjacency matrix. For every node, we put the strengths of the connection, or edge, to every other node in the matrix. If there is no connection, we'll put a 0. The Laplacian matrix that we've seen previously is closely related. Put the roll sums on the diagonal and negate the other numbers. Both matrices have the same block structure. And each block consists of edges of one community. Of course, in a real network, we don't know how to order the nodes to make this block pattern emerge. So, how do we find the blocks?\n\nLet's take a look at the eigenvector software of the matrix. The bottom three vectors, exactly indicate the block structure. And, they each, exactly indicate the nodes, of one community. The remaining eigenvectors are less important here. This works even if the nodes are scrambled. For each node, recreate a feature vector as we've seen before. One entry from each eigenvector. These feature vectors directly indicate which community the node belongs to. In reality of course, the communities are not separated so clearly. There are edges between communities, and edges within communities that are missing. But if there are not too many of these extra edges, the eigenvectors don't change too much, and things still work out. To see this, we can use our new features as an embedding, just like we've done previously.\n\nHere we have three features, so three dimensions. We plot each node on these three axis, and see that the graph communities emerge, as well as separated clusters. This suggests the following approach called spectral clustering. First, we write down the laplacian, which comes from the adjacent matrix. Then we compute the eigenvectors and keep the few bottom ones. The eigenvectors give new features. We use K-means clustering on those new features. This works because the new features magically separate the communities. To find K clusters, one typically uses K-eigenvectors. If the graph is completely connected, there's a Bohring-Eigen vector all at once that we discard right away. They are variants of spectral clustering, but the main idea is the same. Now recall our criteria from the very beginning where we wanted clusters of decent size with small cut between them. This sounds very different from our eigenvectors here, but in fact there's a closed connection.\n\nWe can give labels to nodes to indicate the optimal clusters for the criterion. For example, a special positive number for cluster one, and a special negative number for cluster two. The eigenvectors approximate those labels. Actually spectral clustering is widely used even for data that is not a network. Recall K means if you plug the data points, the clusters we find with K-means are round. So here's an example of data with clusters. But they are not round. The data seems to curve around, and intuitively, we'd like to follow that curvy structure. We run K-means, and the clusters look a bit odd. They disregard the shape of the data. Well, spectral clustering can help here.\n\nWe translate the data into the world of graphs. Concretely, if you build a graph from the data. For each data point, we create a node and connect it to what's closest or most similar points nearby. The graph now reflects local relationships between our data points. It follows the shape of the data. Now we used spectral clustering. We get clusters of points that are densely connected in the graph. These clusters do follow the shape. They look very different and more meaningful. In summary, to use spectral clustering on other data, we just add one step to our procedure. Create the so called neighborhood graph. A bit of care is needed when building this graph. An edge between two points is weighted by the similarity of the points.\n\nA common similarity function is the Gaussian similarity that decays exponentially with a distance. We connect each point to a fixed number of closest neighboring points. How many? Typically it should not be too many, but enough that the resulting graph has no, or very few, disconnected parts. What is the difference on spectral clustering and K-means here? The inter-point distances used by K-means don't capture the complicated shape of the data. In some sense, we don't want to jump across the gaps. A graph captures those gaps. Another interesting point, our Spectral Clustering also uses K-means but with an important difference. We created new feature factors and use K-Means with those new features.\n\nThen the clusters follow the inherent shape of the data. In other words,we found them embedding for our data points that captures delayed in structure much better. We've seen embedding with PCA, but this one is different. In PCA the new features are some combination of the existing features, even though we didn't explicitly write it that way. With a graph, there's no such relatively simple relation. Here we have a nonlinear embedding. In summary, by putting everything together we saw how to use the eigenvectors of the Laplacian matrix to find meaningful clusters that respect hidden structure in the data.",createdAt:lx,updatedAt:lx,publishedAt:"2023-03-16T01:25:39.852Z"}},{id:1236,attributes:{title:"Modularity Clustering",slug:"modularity-clustering",duration:"5:23",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience27ModularityClustering",objective:"In the previous video you saw how to explicitly use eigenvectors to recover hidden communities in a graph. Now learn new criterion that automatically determines the number of communities.",englishTranscript:"In the previous section we have seen how we can explicitly use eigenvectors to recover hidden communities in a graph, and in fairly complex data in general. Next, we will explore another very related criterion for finding communities. There is one difference, in the previous approach we needed to specify the number of communities. Our next method does that automatically. This method is called modularity clustering, and has been widely used to analyze all kinds of networks. For this method, we need to define a modularity score that we will aim to maximize. This score applies to a given clustering in two communities.\n\nThe characteristic idea of modularity, is to compare the communities to a random baseline graph that share some properties with our actual graph. Such as the number of edges, and roughly, the degree of the nodes. Good communities should have many more edges inside than expected by the baseline. So, let's say we have two candidate communities. How do we measure their deviation from a baseline? We compute the number of edges within a community, and then subtract the expected number of edges as per the baseline model. Concretely, for every pair of nodes, i and j, that are in the same community, we count AIJ minus PIJ. AIJ indicates an edge.\n\nIt is 1 if i and j are connected, and 0 otherwise. PIJ is the expected number of edges between i and j in the baseline graph. We sum this over all pairs. For each community, we get the actual total number of edges within the community, minus the expected number of edges within the community. If that is large, then the community is surprisingly dense. This score can be generalized to an arbitrary number of communities. What is the expected baseline number of ages? There are several ways to define that baseline model. Here's a common one. The expected number of edges between node i and node j is proportional to the degree of i times the degree of j.\n\nSo, nodes with many edges in our graph are expected to have many edges in the baseline too. The idea of modularity is that an arbitrary set of nodes, say people in a social network, may have just as many connections as the baseline expect. In that case, the modularity is close to zero. In the example here we have an arbitrary partition into meaningless communities. The modularity of this partition is low. But, if we pick exceptionally well connected communities, then the modularity is high. In fact, modularity can be negative too. This happens if the number of ages in our community is much lower than expected. It may look like this example. This looks like our edge stretch plots within Laplacian eigenvectors. That's not a coincidence. The phenomenon is closely related. Such communities can be useful, too. Say the nodes in this graph are words, and they have an edge if they co-occur in a collection of texts.\n\nVerbs and nouns typically co-occur, but not two verbs or two nouns. So, this partitioning would separate verbs and nouns. But, let's get back to dense communities and find communities that maximize modularity. Note that we do not need to specify the umber of communities. We pick whichever number is best for the score, it happens automatically. How do we find the a good partitioning? We could try all possibilities, but that's going to take really long, even with a very fast computer. One approach is to start with some partitioning and then move nodes between groups, if that improves the score. But it turns out, we can use eigenvectors. This time, we use a matrix that is related to the adjacency matrix and the Laplacian we have already seen, but it's slightly different. Remember our sum over Aij minus Pij to compute the modularity?\n\nOur matrix has one entry for each pair of nodes. And the entry in row i and column j is exactly Aij- Pij. Remember, Aij indicates if there's an edge, and Pij is the expected number of edges between i and j. We compute their eigenvectors of this matrix. Again, each eigenvector assigns a value to each node. For two communities, we use the eigenvector with the largest eigenvalue. We put all nodes with positive values in one community, and all nodes with negative values in the other. We can extend this to more eigenvectors and communities. The actual value given by the eigenvector shows how important or central a node is in the community.\n\nNow, there's one important difference to our previous vector clustering method. The eigenvalues here can be negative. A negative eigenvalue means that the associated eigenvector gives the node value that leads to low modularity. Let's conclude with a famous example. Zachary's karate club network. In this network here, each node is a member of the karate club at a US university in the 1970s. The edges indicate friendships between the members. Using modularity, we find two communities in this network. Now, the interesting fact is that, shortly after the data collection, there was an internal dispute and the karate club split into two groups. And guess what? The communities found by modularity clustering are exactly the two groups that resulted from the split.",createdAt:ly,updatedAt:ly,publishedAt:"2023-03-16T01:25:38.368Z"}},{id:1235,attributes:{title:"Embeddings and Components",slug:"embeddings-and-components",duration:lz,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience28EmbeddingsandComponents",objective:"All the methods learned so far involve new feature vectors for the data points. These are know as embeddings. Learn about different types and their uses.",englishTranscript:"In the previous sections we've seen PCAs, spectral embeddings and spectral clustering. All these methods find new feature vectors for the data points, in other words, an embedding. The idea of embeddings is very useful, so let's summarize some different types of embeddings and their uses. With vectors we already saw that different embeddings can look very different. We used embeddings to do dimensionality reduction.\n\nWe found short feature vectors since finding important information in high dimensions is like finding a needle in a haystack. Our two dimensional embeddings visualize graphs, news, or people's preferences. The new feature vectors bring out major trends, so it's easier to apply other methods. In spectral clustering, k-means give better clustering after spectral embedding. It's very easy to separate the clusters now. And embeddings can give us important components or themes, as with PCA. This points to a difference between the spectrograph embeddings and PCA. In PCA, each data point could be written as a weighted combination of the components.\n\nSuch embeddings are called linear. In spectral clustering, we first construct a graph and then point eigenvectors. Even if we start our feature vectors, our new representation is not linear. Such embeddings are non linear. The different types of embeddings are guided by different goals and criteria. In spectral clustering all we knew were pairs similarity relationships, like friendships in the social network or the related articles for given use article in a collection. Our goal is to find a vector representation that maintains these local relationships and puts related coins close to each other. This is what we saw in our visualizations. This goal of low dimensional features that preserve some pairs as relationships is shared by other methods. For example, sometimes we have site information available to guide the embeddings. Say, we have images of handwritten digits and we know the correct digits for some of them.\n\nWe'd like an embedding that places all two's together, all three's together and so on. We can guide our embedding by giving it example pairs that should be close because we know they are the same digit and example pairs that should not be close because those images are different digits. Apart from that we to preserve local relationships. Metric Learning is a method that takes outside information to guide the embedding. In methods like PCA we are not directly looking for a clustering, but for prominent patterns in our data and each data point may be associated with more than one pattern. The idea of finding prevalent patterns, components, topics or themes appears in several other methods. Independent component analysis or ICA recovers components that are statistically independent. That is, they have no information about of each other. This is stronger than the difference between the PCA components.\n\nThink of a microphone that records an overlay of noise from a dishwasher, a barking dog, and people speaking. ICA can be used to recover all these different sources of sounds. Dictionary Learning poses constraints on the data point associations. Each data point maybe associated with only very few components. In this case, we may need many components so we may not reduce the dimensionality but, we obtain a dictionary that can be used for example, to reconstruct corrupt images. Let's go into a bit more detail for one of the methods that is closely related to PCA, Singular Value Decomposition or SVD. In fact, it also gives us the principal components. Recall our PCA example, our friends' ratings of holiday places. Each row in our data matrix corresponds to one person and each column to a place.\n\nEach principal component is a pattern like adventure or mountains. And associates the places with those patterns. SVD will give us the association of both places and people for each pattern. How does it work? We take our data matrix and as for PCA, subtract the means, this gives us a matrix X. The covariance matrix in PCA is X times X transposed. Right here we keep X. The singular value decomposition writes our data matrix X as a product of three matrixes. These are usually called U, Sigma and V transpose. The matrix sigma is zero, except for the diagonal. The squares of the diagonal entries in sigma are the diagonal values we get in PCA.\n\nThe roles of V principals are the same as the eigenvectors in PCA. These are called the right singular vectors. The columns of the matrix are the left singular vectors. These three matrices describe our data via themes. For each theme we have a singular value, and a left and right singular vector. The singular value tells the importance of the theme. Higher is more important. The right singular vector shows the association of each place with a theme. Recall, scuba diving was associated with adventure, but not mountains. The corresponding left singular vector tells the preference of each person for the particular pattern. So we can simultaneously understand the likings of people and the characteristics of the places. The entries in the left vectors multiplied by the respective singular values are exactly the coordinates we use to visualize the data. The same kind of analysis is useful when the rows of our data are customers and the columns are products. Or if we describe drug interactions, the rows may be drugs and the columns are proteins or pathways in the body. So let's summarize. Embeddings give us meaningful dimensionality reductions, bring out hidden clusters in the data, and help us understand the data via major patterns, components or themes.",createdAt:lA,updatedAt:lA,publishedAt:"2023-03-16T01:25:36.801Z"}}]},instructors:[{id:641,name:io,position:a,nationality:a,achievements:a,focus:a,bio:ip,learnMoreURL:a}]}},{id:if0,attributes:{title:"Regression and Prediction",slug:"regression-and-prediction",description:"Learn the basics of regression for prediction and inferential purposes. Also, gain an understanding of modern linear and non-linear regression for predicition and iferential purposes. Finally, get practical experience of using classical and modern regression methods for prediction and inferential purposes.",prerequisite:a,duration:"1 hr",objective:a,createdAt:"2023-10-18T21:49:04.673Z",updatedAt:"2023-10-18T21:49:45.590Z",publishedAt:"2023-03-16T13:13:27.418Z",chapters:{data:[{id:1234,attributes:{title:"Introduction to Regression and Prediction",slug:jQ,duration:"2:22",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience31IntroductiontoRegressionandPrediction",objective:"Meet the instructor and learn about what will be taught in this course.",englishTranscript:"Hi, I'm Victor Chernozhukov, professor in the Department of Economics and Center for Statistics at MIT. My research in teaching for this econometrics and mathematical statistics was much of recent efforts due added to the use of machine learning methods for causal Inference. In this module, we will discuss regression in classical and modern versions. Regression analysis is about discovering correlations between the outcome, y and the set of regressors x, which sometimes are also called features. In this module, y is always the real random variable and x is the vector of random variables, with components denoted by X1 through Xp. Where we always assume that a concept of one is included as one of the components. We denote the transpose of a vector by the prime sign. For example, suppose our y is hourly wage which varies across workers, and x is the retro regressors which include variables that measure education, experience gender, and other job relevant characteristics.\n\nWhen we are interested in the effect of a particular component of x and y, we show partition axis for D and W, where D will be the target regressor and W will be called the controls of components. The purpose of the regression analysis is to characterize the statistical relation y to x. Namely, we want to answer two question. Question one, or the prediction question. How can we use x to predict y well? Question two, or the inference question. How does the predicted value of y change if we change a component of x, holding the rest of the components of x fixed? In our wage example, the predictions questions becomes how to use job relevant characteristics, such as education, experience, gender and others, to best predict the wage? An inference question becomes, for example, how gender affects the predicted wage? This allows us to uncover gender discrimination if it exists.\n\nIn this module, we will discuss classical and modern regression tools that answer these two questions. We'll now give a preview of some of the empirical findings from a case study in which we will examine our wage example using data from the US current population survey in 2012 for single workers. We will address the two questions post. Number one, we will construct a prediction rule for a hourly wage which depends on the job relevant characteristics linearly. We will assess the quality of the prediction rule using our sample prediction performance. Number two, we will find that an average women get about $2 less an hourly wage than men with the same experience and other recorded characteristics. This is called the gender wage gap in labor economics and measures in part gender pay discrimination.",createdAt:lB,updatedAt:lB,publishedAt:"2023-03-16T01:25:35.607Z"}},{id:1233,attributes:{title:"Linear Regression for Prediction",slug:"linear-regression-for-prediction",duration:"5:25",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience32LinearRegressionforPrediction",objective:"Define linear regression in the population and in the sample. Learn about the best linear predictor.",englishTranscript:"We'll start out by looking at the linear regression problem at the population level. This means that we have access to infinite amounts of data, and therefore we can compute theoretical expected value of population averages such as expected value, or expected value of Y times X. Later for estimation purposes, we will have to work with a finer sample of observations drawn from the population. But right now we focus on the population case to define the ideal quantities. We shall try to construct the best linear prediction rule for Y using a vector of X, with components denoted by Xj.\n\nSpecifically given such X, our predicted value of Y will be beta prime X, which is defined as the sum of beta js times Xjs. Here the primary vector beta consists of components beta js, and we commonly call this parameter the Regression Parameter. We define beta as any solution to the Best Linear Prediction problem, abbreviated as BLP in the population. Here we minimize the expected or mean squared error that results from predicting Y, using linear rule given by b prime X. Where b denotes a potential value for the parameter vector beta. The solution, beta prime X, is called the Best Linear Predictor of Y using X. In words, this solution is the best linear prediction rule among all the possible linear prediction rules. Next we can compute an optimal beta by solving the first order conditions of the BLP problem, called the Normal Equations, where we have expected value of (Y- beta prime X) = 0.\n\nHere we are setting to 0 the derivative of the objective function with respect to b that will minimize. Any optimal value of b satisfies the normal equations, and hence we can set beta to be any solution. Under some conditions such solution will be unique, but we don't require this in our analysis. Next, if we define the Regression Error as epsilon = (Y- beta X), then the normal equations in the population become expectation of epsilon times X = 0. This immediately gives us the decomposition Y = beta X + epsilon, where epsilon is uncorrelated to X. Thus beta prime X is the predicted or explained part of Y, and epsilon is the residual or unexplained part. In practice, we don't have access to the population data. Instead we observe a sample of observations of size n, where observations are denoted by Yi and Xi, and i runs from 1 to n.\n\nWe assume that observations are generated as a random sample, from a distribution F, which is the population distribution of R, Y, and X. Formally this means that the observations are obtained as realizations of independently and identically distributed copies of the random vector Y, X. We next construct the best linear prediction rule in sample for Y using X. Specifically, given X, our predicated value of Y will be hat beta prime X, which is the sum of hat beta js times Xjs. Here hat beta is a vector with components denoted by hat beta js. We call these components the Sample Regression Coefficients. Next we define the linear regression in the sample by analogy to the population problem. Specifically we define hat beta as any solution to the best linear prediction problem in the sample. Where we minimize the sample mean squared error for predicting Y using the linear rule b prime X. Here we denote the sample mean, or empirical expectation, by the symbol E sub n, which simply is the sample average notation. In words, the empirical expectation is simply the sample analog of the population expectation.\n\nWe can compute an optimal beta hat by solving the sample normal equations, where we set the empirical expectation of Xi(Yi- beta hat prime Xi) = 0. These equations are the First Order Conditions of the Best Linear Predictor problem in the sample. By defining the In-Sample Regression Error, hat epsilon, as (Yi- hat beta prime Xi), we obtained the decomposition of Yi into sum of two parts. Part one, given by hat beta prime Xi, represents the predicted or explained part of Yi. Part two, given by hat epsilon i, represents the residual or unexplained part of Yi. Next we examine the quality of prediction that the sample linear regression provides. We know that the best linear predictor of our sample is beta X. So the question really is, does the sample best linear predictor hat beta X adequately approximate the best linear predictor beta X? So let's think about it. Sample linear regression estimates P parameters, beta 1 through beta p, without imposing any restrictions on these parameters.\n\nAnd so intuitively, to estimate each of these parameters, we will need many observations per each such parameter. This means that the ratio of N \u002F P must be large, or P \u002F N must be small. This intuition is indeed supported by the following theoretical result, which reads. Under regularity conditions, the root of the expected square difference between the best linear predictor and the sample best linear predictor is bounded above by a constant, times the level of noise, times square root of the dimension p \u002F n. Here we are averaging over values of X, and the bound holds with probability close to 1 for large enough sample sizes.\n\nThe bound mainly reflects the estimation error in hat beta, since we are averaging over the values of X. In other words, if n is large and p is much smaller than n, for nearly all realizations of the sample, the sample linear regression gets really close to the population linear regression. So let us summarize. First, we define linear regression in the population and in the sample through the best linear prediction problems solved in the population and in the sample. Second, we argued that the sample linear regression, or best linear predictor, approximates the population linear regression, or best linear predictor in the population very well when the ratio p \u002F n is small. In the next segment we will discuss the assessment of prediction performance in practice.",createdAt:lC,updatedAt:lC,publishedAt:"2023-03-16T01:25:34.244Z"}},{id:1232,attributes:{title:"Assessment of Prediction Quality",slug:"assessment-of-prediction-quality",duration:"4:38",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience33AssessmentofPredictionQuality",objective:"Learn how to understand the analysis of variance, also known as ANOVA. Also, learn how to assess the output of sample predictive performance of the sample linear regression",englishTranscript:"In this segment, we have two learning goals. Goal 1 is to understand the analysis of variance or ANOVA in the population and in the sample. Goal 2 is to assess the out of sample predictive performance of the sample linear regression. We begin with the population case. Using the decomposition of Y into the explain and the residual part. And the normal equations that we derived in the previous segment, we can decompose variation of Y into the sum of explain variation and residual variation, as shown in the formula. We next define the population mean squared prediction error, or MSE, as the variance of the population residual.\n\nThe expectation of epsilon squared. We also define the population R squared as the ratio of explained variation to the total variation. In other words, the population R squared is the variation of Y explained by the BLP. And as such, it is bounded below by 0 and above by 1. The analysis of variants in the sample process analogous. We simply replace population expectations by the empirical expectations. Using the decomposition of Yi to explain and the residual part, and the normal equations for the sample. We can decompose the sample variation of Yi into the sum of explained variation and residual variation. The former is given by the sample variation of this sample best in a predictor. And the latter is given by the sample variation of the residual.\n\nWe can define the sample MSE as the sample variance of the residuals and we can define the sample R squared as the ratio of the X-plane to the total variation in the sample. We know that when P\u002FN is small, the sample linear predictor gets really close to the best linear predictor. Thus when P\u002FN is small, we expect that sample averages of y squared beta hat xi squared epsilon hat i squared to be close to the population averages of Y squared, beta X squared, and epsilon squared. So in this case, the sample R squared and the sample MSE will be close to the true quantities, the population R squared and MSE. When P \u002F N is not small, the discrepancy between the two sets of measures can be substantial. And the sample R squared and the sample MSE can be very poor measures of predictability. For example, when p = n, we can have sample MSE = 0 and sample R squared = 1 no matter what the population MSE or R squared are.\n\nThe following simulation example will support our reason. In this example, Y and X are statistically independent and generated from the normal distributions with mean need 0 and variance 1. The means that the true linear predictor of Y given X is simply 0 and the true R squared is also 0. Suppose the number of observations is N, the number of regressors is P. If p = n, then the typical sample R squared will be 1. Which is very far away from the true number of 0. This is an example of extreme over fitting. If p= n \u002F 2 then the typical sample r squared is about half which is still far off from the truth. If p = n \u002F 20, then the typical sample r squared is about 0.05 which is no longer far off from the truth.\n\nWe can now see that using sample R squared and MSE to judge predicted performance could be misleading when P over N is not small. So the question is, can we design better metrics for predicted performance, and the answer is yes. The first proposal is to use the adjusted R squared and MSE. We first define the adjusted MSE by rescaling the sample MSE by a factor of n over n- p. And then making the corresponding adjustment to the sample R squared. The re-scaling will correct for over-fitting but under rather strong assumptions. A more universal way to measure predictive performance is to perform data splitting.\n\nFirst, we use a random part of data, say half of data, for estimating or training the prediction rules. Second, we use the other part of data to evaluate the predictive performance or the rule, recording the out-of-sample MSE or R squared. Accordingly we call the first part of data, the training sample. And the second part, the testing or validation sample. Indeed suppose we use n observations for training and m for testing or validation. Let capital V denote the names of observations in the test sample. Then the out of sample or test means squared error is defined as the average squared prediction error, where we predict yk in the test sample by head beta xk. Where hat beta was computed on the training sample. The out of sample R squared is defined accordingly as 1- the ratio of the test MSE to the variation of the outcome in the test sample. So let us summarize. First we discussed the analysis of variance, in population and in the sample. Second, we define good measures of predictive performance, based on adjusted MSE and R squared, and based on data splitting. And next we will discuss a real application to predicting wages.",createdAt:lD,updatedAt:lD,publishedAt:"2023-03-16T01:25:32.959Z"}},{id:1231,attributes:{title:"Inference for Linear Regression",slug:"inference-for-linear-regression",duration:"5:36",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience34InferenceforLinearRegression",objective:"Learn the answer to the inference question posited in previous videos.",englishTranscript:"In this segment, we provide an answer to the inference question. To recall the question, we partition vector of regressors X into D and W, where D represents the target regressors of interest, and W represents other regressors, which we sometimes call the controls. We write Y = the predicted value which is beta 1D + beta 2W plus noise. And now we recall the inference question. Which is how does the predicted value of Y change if we increase D by a unit holding W fixed? For instance, in the wage example, the inference question can be stated as follows. What is the difference in the predicted wage for a man and woman with the same job-relevant characteristics? The answer to this question is the population regression coefficient beta 1 corresponding to the target regressor D.\n\nIn the wage example, D is the female indicator and beta 1 is called the gender wage gap. Next we attempt to understand better the meaning of beta 1 using a tool called partialling-out. In the population, we define the partialling-out operation as a procedure that takes a random variable V and creates a residual to the V by subtracting the part of V that is linearly predicted by W. So we can write tilde V as V- gamma V where gamma indexed by vw solves the problem of best linear prediction of V using W. Here we use notation argument that does not solution of the minimization problem. It can be shown that the partialling-out operation is linear. So if we have Y = V + U, then tilde Y = tilde V + tilde U. We now apply partialling-out to both sides of our regression equation to get the partialled-out version which implies that we obtain the decomposition which reads tilde Y = beta 1 tilde D + epsilon, where epsilon is uncorrelated with tilde D.\n\nThis decomposition follows because partialling out eliminates all the Ws, and also leaves epsilon untouched, since epsilon is linearly unpredictable by X, and therefore by W by definition. Moreover, since tilde D is a linear function of X, epsilon and tilde D are not correlated in our decomposition. Now recognize our decomposition as the normal equations for the population regression of tilde Y and tilde D. That is, we've just obtained the Frisch-Waugh-Lovell theorem, which states that the population linear regression coefficient beta 1 can be recovered from the population linear regression of tilde Y on tilde D. Beta 1 is a solution to the best linear prediction problem, where we predict tilde Y by a linear function of tilde D. We can also give an explicit formula for beta 1, which is given by the ratio of two averages that you see in this formula.\n\nWe also see that beta 1 is uniquely defined if D is not perfectly predicted by the Ws. So the tilde D has a non-zero variance. The Frisch-Waugh-Lovell theorem is a remarkable fact which is useful for both interpretation and estimation. It asserts that beta 1 can be interpreted as a regression coefficient of residualized Y on residualized D, where the residuals are defined by taking out or partialling-out the linear effect of W from Y and D. We next proceed to discuss estimation of beta 1. In the sample, we will mimic the partialling-out in the population. When p over n is small we can rely on the sample linear regression, that is on ordinary in these squares. When p over n is not small using sample linear regression for partialling-out is not a good idea. What we can do instead is do penalized regression or variables selection, which we will discuss later in this module. So let us assume that p over n is small.\n\nSo it is appropriate to use the sample linear regression for partialling-out. Of course, by the FrischWaughLovell theorem applied to the sample instead of the population, the sample linear regression of Y on D and W gives us an estimator hat beta 1, which is numerically equivalent to the estimator obtained via sample partialling-out. It is still useful to give the formula for hat beta 1 in terms of sample partialling-out where we use checked quantities to denote the residuals that are left after predicting the variables in the sample with the controls. The population partialling-out is replaced here by the sample partialling-out where V replace the population expectation by the empirical expectation. Using the formula, it can be shown that the following results calls. If p over n is small, then the estimation error in the estimated residualized quantities has a negligible effect on hat beta 1, and hat beta 1 is approximately distributed as normal variable with mean beta 1 and variance V over n where the expression of the variance appears over here. In words, we can say that the estimator hat beta 1 concentrates in a square root of V over n neighborhood of beta 1 with deviations controlled by the normal law.\n\nWe can now define the standard error for hat beta 1 as square root of hat V over n, where hat V is an estimator of V. This result implies that the interval given by the estimate +- 2 standard errors covers the true value of beta 1, for most realizations of the data sample. Or more precisely approximately 95% of realizations of the data sample. If we replace 2 by other constants, we get other coverage probabilities. In other words, if our data sample is not extremely unusual, the interval covers the truth. For this reason, this interval is called the confidence interval. For example, in the wage example, our estimate of gender hourly wage gap is about -2$ and then 95% confidence interval is about -1$ to -3$. Let us summarize, first we interpreted beta 1 as the regression coefficient in the univariate regression of the response variable and the target variable, after we have removed the linear effect of other variables. Second, we noted that this result is useful for interpretation and understanding of the regression coefficient. This result will also be super useful for setting up inference in modern high-dimensional settings which we will discussed later in this module. And next, we will carry out a case study for our wage example.",createdAt:lE,updatedAt:lE,publishedAt:"2023-03-16T01:25:31.457Z"}},{id:1230,attributes:{title:"Other Types of Regression",slug:"other-types-of-regression",duration:lj,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience35OtherTypesofRegression",objective:"Learn about nonlinear regression forms. Also learn about regressions that result from replacing the squared loss function by other loss functions.",englishTranscript:"In this segment, we'll briefly discuss other types of regression. First, we will introduce regressions introduced by using nonlinear functional forms, for example logistic regression. Second, we will mention regressions that result from replacing the squared loss function by other loss functions. For example, using the absolute deviation loss gives rise to median regression, and using the asymmetric absolute deviation laws gives rise to quantile regression.\n\nIn contrast to linear regression, in nonlinear regression we use a nonlinear function of parameters and regressors, say P of X and beta to predict Y. For instance, the logistic regression employs the logistic transformation of beta X given by the formula that you see. This logistic transformation is particularly attractive when the response variable, Y, is binary taking on values of zero or one so that the predicted values are naturally constrained between zero and one. The problem of predicting binary Y is a basic example of a classification problem. We can interpret the predicted values in this case as approximating probability that Y=1.\n\nIn nonlinear aggression, we estimate the parameters by solving the sample nonlinear least squares problem. Where we minimized the average squared error for predicting Yi by p(Xi, b). In the case of binary outcomes, we often estimate the parameters by maximizing the logistic log-likelihood function for the binary outcomes Yi conditional on Xi as you see in this formula. We previously used the squared error loss to set up the best linear predictor. Sometimes we call this approach the linear mean regression, because the best predictor of Y under squared loss is the conditional mean of Y given X. So what if we use the absolute deviation loss instead? Then we obtain the least absolute deviation regression or median regression, because the best predictor of Y under absolute deviation loss is the median value of Y conditional on X. We define the linear median regression by solving the best linear prediction problem under the absolute deviation loss. Where in the population we use the theoretical expectation and in the sample we use the empirical expectation instead. Just like mean regression, median regression can also be made nonlinear by replacing beta X with some nonlinear function p ( X, b ).\n\nRegarding the motivation, median regressions are especially great for cases when the outcomes have heavy tails or outliers. The median regressions have much better properties in this case than the mean regressions. If we use an asymmetric absolute deviation loss, then we end up with the asymmetric absolute deviation regression, or quantile regression. Specifically we define the linear Tao quantile regression by solving the best linear prediction problem under the asymmetric absolute deviation loss. In the population we use the population expectation and in the sample we use the empirical expectation instead. The weight Tao that appears in the definition of the absolute deviation loss specifies that Tao times 100-th percentile of Y given that we are trying to model linearly. Quantile regressions are great for determining the factors that affect the outcomes in the tails.\n\nFor example, in risk management, we might predict the extremal conditional percentiles of Y using the information X. This type of prediction is called the conditional value-at-risk analysis. In medicine, we could be interested in how smoking and other controllable factors X affect very low percentiles of infant birth weights Y. In supply chain management, we could try to predict the inventory level for a product that is able to meet the 90-th percentile of demand Y given the economic conditions described by X. In this segment, we have briefly overviewed non-linear regressions as well as quantile regressions and their uses. In the next block of our module, we will consider modern linear and nonlinear regressions which are motivated by high-dimensional data.",createdAt:lF,updatedAt:lF,publishedAt:"2023-03-16T01:25:30.299Z"}},{id:1229,attributes:{title:"Modern Linear Regression for High-Dimensional Data",slug:"modern-linear-regression-for-high-dimensional-data",duration:"4:47",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience36ModernLinearRegressionforHighDimensionalData",objective:"Learn the two motivations  for using high-dimensional regressors in prediction.",englishTranscript:"We begin a new block of segments where we consider modern linear regression for high dimensional data. We consider the linear regression model Y equals Beta X plus Epsilon where Beta X is the population best linear predictor, or BLP, of Y using X. Or simply the population linear regression function, and epsilon is uncorrelated with X. Here, the regressor X is p-dimensional, that is, there are P regressors and the sample size is N, and P is large, possibly larger than N. One reason for having high dimensional regressors in our model is the increasing availability of modern rich data, or quote unquote, big data.\n\nMany modern data sets are rich in that they have many recorded features or regressors. For example, in house pricing and appraisal analysis, we can make use of numerous housing characteristics. In demand analysis, we can rely on price and product characteristics, and in the analysis of court decisions, we can employ many of the judges' characteristics. Another reason for having high dimensional regressors in our model is the use of constructed regressors. By this we mean that if Z are raw regressors or features, then the constructed regressors, X, are given by the set of transformations, P(Z), whose components are Pj(Z). We sometimes call this set of transformations a dictionary.\n\nFor instance, in the wage example, we used quadratic and cubic transformations of experience, as well as interactions, or products, of these regressors with education and geographic indicators. The use of constructed regressors allows us to build more flexible and potentially better prediction rules than just linear rules that employ lower regressors. This is because we're using prediction rules Beta X equals Beta P(Z) that are allowed to be either linear or nonlinear in the raw regressor Z. We now know that we still call the prediction rule Beta X linear because it is linear with respect to the parameters Beta. And with respect to the constructed regressors X equals P(Z). We next ask the following related question. What is the best prediction rule for Y using Z? It turns out that the best prediction rule is the conditional expectation of Y given Z.\n\nWe denote this prediction rule by g of Z and we call it the regression function of Y on Z. This is the best prediction rule among all rules, and it is generally better than the best linear prediction rule. Indeed, it can be shown that g(Z) solves the best prediction problem where we minimize the mean squared prediction error, or MSE, among all prediction rules m(Z) linear or nonlinear in Z. Now by using beta P(Z) we are implicitly approximating the best predictor g(Z). Indeed, it can be shown for any parameter b shown the MSE for predicting Y with b(Z) was equal to the MSE for approximating G g(Z) with b'P(Z) plus the constant. That is, the mean squared prediction error is equal to the mean squared approximation error plus a constant that doesn't depend on B, so that minimizing the former is the same as minimizing the latter. We now conclude that the BLP beta P(Z) is the best linear approximation, or BLA, to the regression function g(Z). By using a richer and richer dictionary of transformations, P(Z), we can make the BLP beta P(Z) approximate g(Z) better and better.\n\nWe can illustrate this point by the following simulations. Suppose that is uniformly distributed on the unit interval, and the true regression function g(Z) is the exponential function of 4 times Z. Suppose we don't know this and we use the linear form beta P(Z) to provide approximations for g(Z)? Suppose we use P(Z) that consists of polynomial transformations of Z, consisting of the first P terms of the polynomial series 1, Z, Z squared, Z cubed, and so on. We use this dictionary to form the BLA or BLP beta P(Z). We now provide a sequence of figures that illustrate the approximation properties of the BLA or BLP corresponding to P(Z) equal to 1, 2, 3 and 4. With P equal 1, we get a constant approximation to the regression function g(Z), and as we can see, the quality of this approximation is very poor. With P = 2, we get a linear inside approximation for g(Z), and as the figure shows, the quality of this approximation is still very poor. With P = 3, we get a quadratic in Z approximation for g(Z), and now, the quality is quite good all of a sudden. This explains why using the linear transformations over [INAUDIBLE] regressors is a good idea. Now with P equals 4 we get a cubic in Z approximation for g(Z) and the quality of approximation becomes simply excellent.\n\nThis further stresses the motivation for using nonlinear transformations Over all regressors in linear regression analysis. In summary, we provided two motivations for using high-dimensional regressors in prediction. The first motivation is that modern data sets have high dimensional features That can be used as regressors. The second motivation is that we can use nonlinear transformations of features or raw regressors and their interactions to form constructed regressors. This allows us to better approximate the ultimate and best prediction rule, the conditional expectation of the outcome given raw regressors.",createdAt:lG,updatedAt:lG,publishedAt:"2023-03-16T01:25:28.694Z"}},{id:1228,attributes:{title:"High-Dimensional Sparse Models and Lasso",slug:"high-dimensional-sparse-models-and-lasso",duration:k$,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience37HighDimensionalSparseModelsandLasso",objective:"Learn about high-dimensional sparse models and a penalized regression method called the Lasso.",englishTranscript:"In this segment, we will talk about high-dimensional sparse models and a penalized regression method called the Lasso. Here, we consider the regression model Y = beta X + epsilon, where beta X is a population-best linear predictor of Y using X. Or simply the population linear regression function. The regresssor X is p-dimensional, with components denoted by Xj. That is, there are p regressors, and p is large, possibly much larger than N, where N is the sample size. In order to simplify the discussion, we assume that each Xj has a unit variance in the sample. Here, we are dealing with the high-dimensional setting, where p\u002FN is not small.\n\nClassical linear regression, or ordinary least squares fails in these settings, because it overfits the data, as we have seen in part one of our module. We need to make some assumptions and modify the classical regression method to deal with the high-dimensional case. One intuitive assumption is approximate sparsity, which informally means that there is a small group of regressors that have relatively large coefficients, that can be used to approximate the BLP beta 'X quite well. The rest of regressors have relatively small coefficients. An example of an approximately sparse model is the linear regression model with the regression coefficients beta j given by 1\u002Fj-squared as j ranges from 1 to p. The figure that you see here shows the graph of these coefficients. As we can see, the coefficients has decreased quite fast, with only three or four regression coefficients that appear to be large.\n\nFormally, the approximate sparsity means that the sorted absolute values of the coefficients decrease to zero fast enough. Namely, the js largest in absolute value coefficient is at most of size j into the power of minus a times a constant, where a is greater than one half. Here, the constant a measures the speed of decay. For estimation purposes, we have a random sample of Yi and Xi, where i ranges from 1 to n. We seek to construct a good linear predictor, head-beta X, which works well when p over N is not small. We can construct head-beta as a solution of the following penalized regression problem called Lasso. Here, we are minimizing the sample mean-squared error that results from predicting Yi with bX plus a penalty term, which penalizes the size of the coefficients, bjs, by their absolute values.\n\nWe control the degree of penalization by the penalty level lambda. A theoretically justified penalty level for Lasso is given by the formula that you see here. This penalty level ensures that the Lasso predictor, hat beta X, does not overfit the data and delivers good predictive performance under approximate sparsity. Another good way to pick penalty level is by cross-validation, which uses repeated splitting of data into training and testing samples to measure predictive performance. We will discuss cross-validation later in this module. Intuitively Lasso imposes the approximate sparsity on the coefficients hat beta, just like in the assumption. It presses down all of the coefficients to zero, as much as possible without sacrificing too much fit, and it ends up setting many of these coefficients exactly to zero. We can see this in the simulation example where our Xjs and epsilons are standard normal variables, and the true coefficients beta j are equal to 1 over j squared. Suppose also that n is equal to 300 and p is equal to 1000.\n\nThe following figure shows that hat beta, in blue, is indeed sparse and is close to the true coefficient vector beta, in black, indeed. Most of hat-beta js are set to 0, except for several coefficients that align quite well with the largest coefficient beta j, or the true coefficient vector. This really shows that Lasso is able to leverage approximate sparsity to deliver good approximation to the true coefficients. From the figure, we see that Lasso sets most of the regression coefficients to zero. It basically figures out approximately, though not perfectly, the right set of regressors. In practice, we often use the Lasso-selected set of regressors to refit the model by least squares. This method is called the least squares post Lasso, or simply post-Lasso. Post Lasso does not shrink large coefficients to zero as much as Lasso does. And it often improves over Lasso in terms of prediction.\n\nWe next discuss the quality of prediction that Lasso and Post-Lasso methods provide. In what follows, we will use the term Lasso to refer to either of these methods. By definition, the best linear prediction rule, out-of-sample, is beta X, so the question is, does hat-beta X provide a good approximation to beta X? We are trying to estimate p parameters, beta 1 through beta p, imposing the approximate sparsity via penalization. Under sparsity, only a few, say s, parameters will be important. And we can interpret s as the effective dimension. Lasso approximately figures out which parameters are important, and estimates them. Intuitively, to estimate each of the important parameters well, we need many observations per each size parameter. This means that n over s must be large, or equivalently, s over n must be small. This intuition is indeed supported by the following theoretical result, which reads, under regularity conditions, the root of the expected square difference between the best linear predictor and the Lasso predictor is bounded about by a constant times the level of noise, times square root of the effective dimension s times lower p n divided by n.\n\nHere, we are averaging over the values of x and the bond holds probability close to one for large enough sample sizes. The effective dimension s is equal to constant times n into the power of 1 over 2a, where a is the rate of decrease of coefficients in the approximate sparsity assumption. In other words, if n is large and the effective dimension s is much smaller than n over log(pn), for nearly all realizations of the sample, the Lasso predictor gets really close to the best linear predictor. Therefore, under approximate sparsity, Lasso and Post-Lasso will approximate the best linear predictor well. This means that Lasso and Post-Lasso won't overfit the data, and we can use the sample and adjusted R squared and MSE to assess out-of-sample predictive performance. Of course, it's always a good idea to verify out-of-sample predictive performance by using test or validation samples. So, let us summarize. We have discussed approximate sparsity as one assumption that makes it possible to perform estimation and prediction with high-dimensional data. We have introduced Lasso, which is a regression method that imposes approximate sparsity by penalization. Under approximate sparsity, Lasso is able to approximate the best linear predictor, and thus produces high quality prediction.\n",createdAt:lH,updatedAt:lH,publishedAt:"2023-03-16T01:25:27.290Z"}},{id:1227,attributes:{title:"Inference With Lasso",slug:"inference-with-lasso",duration:"3:22",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience38InferenceWithLasso",objective:"Find out how to use loss to answer the inference question",englishTranscript:"In this segment we discuss how to use loss to answer the inference question. Which is how the predictive value of y change if we increase the component of t(x) by holding the other components of x fixed. The answer is a population regression coefficient Beta 1 corresponding to the regressor D. And here would like to discuss how to use Lasso to estimate and construct confidence intervals for beta 1 in the high dimensional setting. We write the regression equation as Y equals beta 1D plus beta 2 w plus epsilon, where d is the target regressor and w's our d controls.\n\nWe recall from part one of our module that after portioning out we ended up with a simplified regression equation. Tilda Y = tilde d beta 1 + epsilon where epsilon is uncorrelated with tilda d. Here, tilde Y and tilde D, are the residuals that are left after parceling out the linear effect of W. As we argued in part one, this allows us to obtain beta 1 as a coefficient in the linear regression of tilde Y and tilde D. D in the population recovers the partialling-out procedure. For estimation purposes, we have a random sample of Yis and Xis.\n\nOur main idea here is that, we will mimic in the sample the partialling-out procedure in the population. Previously, when p over n was small, we employed least squares as the Prediction method in the partialling-out steps. Here p over n is not small, and we employed instead the Lasso method in the partialling-out steps. So let us explain this in more detail. First we round the lasso regression of Yi on Wi and of Di on Wi and keep the resulting residuals called check Wi and check Di. Second we run the list squares of check Wi on check Di. The resulting estimated regression confusion is our estimator check beta one. Our portioning out procedure was loss all. It relies on approximate sparsity in the two portioning out steps. Indeed, theoretically, the procedure will work well if the population regression coefficients and the two parceling out steps are approximately sparse, with a sufficiently high speed of degrees of assorted values, as shown in the conditions that you see here.\n\nNow we present the following theoretical result which reads, Under the stated approximate sparsity and other regularity conditions, the estimation error in check Di and check Yi has a negligible effect on check beta 1. And check beta 1 is approximately distributed as normal variable with mean beta 1 and variance V over n where expression of the variance appears here. That is we can say that the estimator check beta 1 concentrates in a square root of V \u002F n neighborhood of beta 1 with deviations controlled by the normal law. We can use this result just like we used the analogous result for the low dimensional case in block one. We can define the standard error of check beta one as square root of hat v over n. Where hat v is an estimator of v.\n\nThis is our quantification of uncertainty about that one. We then provide the approximate 95% confidence interval for beta 1, which is given by the estimate check beta 1 + or- two standard errors. So let's give a summary. In this segment, we learnt how to estimate the target regression estimate, beta one, in the high dimensional regression problem. We use Lasso to obtain estimates of the outcome Y and target regression D net of the effect of the effect of other regressors W and then run least squares of one on the other. Then we argued that the resulting estimator check beta1 is high quality estimator beta1 and we constructed a confidence interval for beta1.",createdAt:lI,updatedAt:lI,publishedAt:"2023-03-16T01:25:25.280Z"}},{id:1226,attributes:{title:"Other Penalized Regression Methods: Cross-Validation",slug:"other-penalized-regression-methods-cross-validation",duration:"6:15",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience39OtherPenalizedRegressionMethodsCrossValidation",objective:"Learn other penalized regression methods. Also learn what cross-validation is.",englishTranscript:"In this segment, we overview other penalized regression methods and also discuss cross-validation, which is a method to choose tuning parameters for the prediction rules. We are interested in prediction in the linear model Y = beta X + epsilon, where epsilon isn't correlated with X. And we have random sample of Yi and Xis. Our generic predictor will take the linear form hat f of x = hat beta x. The idea of penalized regression is to choose the coefficients hat beta, to avoid overfitting in the sample. This is achieved by penalizing the size of the coefficients by various penalty functions.\n\nIdeally, we should choose the level of penalization to minimize the out-of-sample mean squared prediction error. We first consider the Ridge method. The Ridge method estimates coefficients by penalized least squares, where we minimize the sum of squared prediction error plus the penalty term given by the sum of the squared values of the coefficients times a penalty level, lambda. We can see this in the formula given here. If we look at the formula, we notice that analogous to the Lasso, Ridge penalty presses down or penalizes the coefficients without sacrificing too much fit. In contrast to Lasso, Ridge penalizes the large values of coefficients much more aggressively and small values much less aggressively.\n\nBecause Ridge penalizes small coefficients very lightly, the Ridge fit is never sparse. And unlike Lasso, Ridge does not set estimated coefficients to zero, and so it does not do variable selection. Because of this property, Ridge predictor hat beta X is especially well suited for prediction in the dense models, where the beta js are all small without necessarily being approximately sparse. In this case, it can easily outperform the Lasso predictor. Finally we noted in practice, we can choose the penalty level lambda in Ridge, by cross-validation which we will discuss later in this segment. A Ridge and Lasso have other useful modifications or hybrids. One popular modification is the method called the elastic net. Here we estimate the coefficients by penalized least squares with penalty given by the linear combination of the Lasso and the Ridge penalties as you see in this formula. In the formula, we see that the penalty function has two penalty levels, lambda 1 and lambda 2, which could be chosen by cross-validation in practice.\n\nNow, let us look at the formula carefully. We see that the elastic net penalizes large coefficients as aggressively as Ridge. And we also see that it penalizes small coefficients as aggressively as Lasso. By selecting different values of penalty levels, lambda 1 and lambda 2, we could have more flexibility for building a good prediction rule than with just Ridge or with just Lasso. We also note that the elastic net doesn't perform variable selection, unless we completely shutdown the Lasso penalty by setting penalty level lambda 2 equals zero. Elastic net works well in regression models, where regression coefficients are either approximately sparse or dense. Another useful modification of Lasso and Ridge is the lava method. In the lava method, we estimate the coefficients by the penalized least squares as shown in this form. If we look at the formula carefully, we see that, just like previously, we are minimizing the sum of squared prediction errors from predicting outcome observations Yi with a linear and Xi prediction rule plus a penalty term.\n\nHowever, unlike previously, we are splitting the parameter components into gamma j, + delta j, and penalize gamma j like in Ridge and delta j like in Lasso. There are two corresponding penalty levels, lambda 1 and lambda 2, which can be chosen by cross-validation in practice. Now, because of the splitting, lava penalizes coefficients least aggressively compared to Ridge, Lasso or elastic net, because it penalizes large coefficients like in Lasso and small coefficients like in Ridge. Lava never sets estimated coefficients to zero, and so it doesn't do variable selection.\n\nThe lava method works really well in sparse + dense models, where there are several large coefficients and many small coefficients, which are not necessarily sparse. In these types of models, lava significantly outperforms Lasso, Ridge or elastic net. We have all ready mentioned cross-validation several times during the course of the segment. Cross-validation is an important and common practical tool that provides a way to choose tuning parameters such as the penalty levels. The idea of cross-validation is to rely on the repeated splitting of the training data to estimate the potential out-of-sample predictive performance. Cross-validation proceeds in three steps, which we've first described in words as follows. In step 1, we divide the data into K blocks called folds. For example was, K equal to five, we split the data into five parts. In step 2, we begin by leaving one block out. We fit the prediction rule on all other blocks, we then predict outcome observations in the left out block and record the empirical mean squared prediction error. We repeat this procedure for each block. In step 3, we average the empirical mean squared prediction errors over blocks.\n\nWe do these steps for several or many values of the tuning parameters, and we choose the best tuning parameter to minimize the average mean squared prediction error. Let us now consider more formal description of cross-validation. We randomly select equal sized blocks B1 through Bk to randomly partition the observation indices one through M. We then fit a prediction rule denoted by hat f sub -k or X and theta. Where theta denotes tuning parameters such as penalty levels, and hat f sub -k depends only on observations that are not in the block Bk. The empirical mean squared error for the block of observations Bk is given by the average squared prediction error for this block, as shown in this formula. In this formula M is the size of the block. The cross validated MSE is the average of MSEs for each block as shown again, in this formula. Finally, the best tuning parameter theta is chosen by minimizing the cross validated MSE. We now provide some concluding remarks.\n\nFirst, we note that in contrast to Lasso, the theoretical properties of Ridge and other penalized procedures are less well understood in the high-dimensional case, yet. So it is a good idea to rely on test data to assess their predictive performance. Second, we note that cross validation is a good way to choose penalty levels, but its theoretical properties are not completely understood in high-dimensional case yet.\n\nSo again, it is a good idea to rely on task data to assess the predictive performance of cross-validated rules. Finally, we may want to ask a question here. How do the penalize regression methods work in practice? In the next part of our module we will assess the predictive performance of these methods in a real example, where we will also compare these methods to modern nonlinear regression methods.",createdAt:lJ,updatedAt:lJ,publishedAt:"2023-03-16T01:25:23.834Z"}},{id:1225,attributes:{title:"Trees, Random Forests, and Boosted Trees",slug:"trees-random-forests-and-boosted-trees",duration:"7:33",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience310TreesRandomForestsandBoostedTrees",objective:"Learn how to use trees as a predictor for data.",englishTranscript:"We begin a new block of our module devoted modern nonlinear regression. Here we are interested in predicting the outcome y using the raw regressor Zed which are k-dimensional. Recall that the best prediction rule g(Z) is the conditional expectation of y given Z. In this module so far, we have used best linear prediction rules to approximate g(Z) and linear regression or lost regression for estimation. Now we consider nonlinear prediction rules to approximate g(Z), including tree-based methods and neural networks. In this segment we discussed tree-based methods. The idea of regression trees is to partition the regressor space where Z takes values into a set of rectangles, and then for each rectangle provide a predicted value. Suppose we have n observations, Zi, Yi for i ranging from 1 to n.\n\nGiven the partition of the space into M rectangles, R1 through Rm, which will be determined by data. The regression rule is a prediction of the four. Hat g(Z) is equal to the sum over m from 1 to m of hat beta m times the indicator that Z belongs to the rectangle Rm. The estimated coefficients are obtained by minimizing the sample MSE, as shown in this formula. From the formula we can conclude that hat Beta m is set equal to average of Yi with Zi falling in the rectangle Rm, as shown in the formula that you see. The rectangles or regions Rm are called nodes and each nodes has predicted value had Rm associated with it. And nice thing about regression trees is that you get to draw cool pictures like this one. Here we show a tree based prediction rule for our wage example where Y is wage and Z are experience, geographic, and educational characteristics.\n\nAs you can see from looking at the terminal nodes of the tree, in this tree, the predicted hourly wage for college graduates with more than 9.5 years of experience is $24, and otherwise it is 17. The predicted wage for non-college graduates with more than 14 years of experience is 14, and otherwise it is 12. Now how do we grow this tree? To make computation tractable, we use the recursive binary partitioning or splitting of the regressor space. First, we cut that regressor space into two regions by choosing a regressor and a split point that achieve the best improvement in the sample MSE. This gives us the tree of depth one, that you see in the figure\n\nThe best variable to split on here is the indicator of college degree and it takes values of 0 or 1, so the natural split point is 0.5. This gives us a starting tree-based prediction rule, which predicts a $20 hourly wage for college graduates and 13 for all others. Second we repeat this procedure over two resulting regions or nodes, college graduates and non-college graduates, obtaining in this step four new nodes that we see in this figure. For college graduates the splitting rule that minimizes MSE is the experience regressor at 9.5 years. This refines the prediction rule for graduates to $24 if experience is greater than 9.5 years and $12 otherwise. For non-graduates the procedure works similarly.\n\nThird, we repeat this procedure over each of the four nodes. The tree of that three now has eight nodes. We see that in the final level we are now splitting our gender indicator, high school graduate indicator, and the south indicator. Finally, we stop growing the tree when the desired depths of the tree is reached or when the minimal number of observations per node, called minimal node, size is reached. We've now made several observations. First, the deeper we grow the tree, the better is our approximation to the regression function g(Z). On the other hand, the deeper the tree, the more noisy our estimate of g(Z) becomes, since there are fewer observations per terminal node to estimate the predicted value for this node. From a prediction point of view, we can try to find the right depth or the right structure of the tree by cross-validation.\n\nFor example, in the wage example the tree of depth 2 performs better in terms of cross-validated MSE than the trees of depths 3 or 1. The process of cutting down the branches of the tree to improve predictive performance is called Pruning The Tree. However in practice pruning the tree often doesn't give satisfactory performance because a single prune tree provides a very crude approximation to the regression function g(Z). A much more powerful and one use approach to improve simple regression trees is to build the Random Forest. The idea of Random Forest is to grow many different deep trees and then average prediction rules based on them. The trees are grown over different artificial data samples generated by sampling randomly with replacement from the original data. This way of creating artificial samples is called the bootstrap statistics.\n\nThe trees are growing deep to keep the approximation error low and averaging is meant to reduce the noisiness of the individual trees. Let us define the bootstrap method. Each bootstrap sample is created by sampling from our data on pairs (Yi, Zi) randomly with replacement, so some observations get drawn multiple times and some don't get redrawn at all. Given a bootstrap sample numbered by numbered by numeral b, we build a tree-based prediction rule hat gb (Z). We repeat the procedure capital B times in total and then average the prediction rules, that result from each of the bootstrap samples. So here we have hat g random forest (Z) equals 1 over B times the sum of hat g sub B of Z over B, where b runs from one to capital B.\n\nUsing bootstrap here is an intuitive idea. If we could have many independent copies of the data, we could average the prediction rules obtained over these copies to reduce the noisiness. Since we don't have such copies, we rely on bootstrap copies of the data. The key underlying idea here is that the trees are grown deep to keep the approximation error low and averaging is meant to reduce the noisiness of the individual trees. The procedure of averaging noisy prediction rules over the bootstrap samples is called Bootstrap Aggregation or Bagging.\n\nFinally I would like to know that what we discuss here is the simplest version of the random forest, and there are many different modifications aimed at improving predictive performance that we didn't discuss. But I'm encouraging you to look at the course materials for additional references that discuss random force and more detail. Another approach to improve simple regression tree is by boosting. The idea of boosting is that of request of fatigue where estimated tree-based prediction rule, then we take the residuals and estimate another shallow tree-based prediction rule for these residuals and so on. Summing up the prediction rules for the residuals gives us the prediction rule for the outcome.\n\nUnlike in a random forest, we use shallow trees, rather than deep trees, to keep the noise low and each step of boosting aims to reduce the approximation error. In order to avoid overfitting and boosting, we can stop the procedure once we don't improve the cross-validated mean square error. Formerly, the boosting algorithm looks as follows. In step 1, we initialize the residuals Ri = Yi, for i that runs from 1 to n. In step 2, we perform the following operation over index j running from 1 to capital J. We fit a tree-based prediction rule, gj(Z) to the data (Zi,Ri) with i from 1 to n and we have data residuals as R1 equals previous Ri minus lambda times hat g sub j (Zi). Finally in step 3, we output the boosted prediction rule. Hat g(Z) equals sum over J of lambda hat g sub j(Z). The capital J and lambda that you see here are the tuning parameters. Representing the number of boosting steps, and the degree of updating the residuals. In particular, lambda equals 1, gives us the full update.\n\nWe can choose j and lambda by cross validation. So let us summarize. We discussed the tree based prediction rules, and ways to improve them by bagging or boosting. Bagging is bootstrap aggregation of the prediction rules. Bootstrap aggregation of deep regression trees gives us random forests. Boosting uses recursive fitting of residuals by a sequence of tree-based prediction models. The sum of these prediction rules gives us the prediction for outcome.",createdAt:lK,updatedAt:lK,publishedAt:"2023-03-16T01:25:22.140Z"}},{id:1224,attributes:{title:"Neural Networks",slug:"neural-networks",duration:iq,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience311NeuralNetworks",objective:"Find out what a neural network is.",englishTranscript:"In this segment, we consider a non linear regression method, based on neural networks. As before, we are interested in predicting the outcome Y using the raw regressors Z, which are K-dimensional. Recall that the best prediction rule of Y given Z is the function g(Z), the conditional expectation of Y given Z. The idea of the neural network is to use parameterized nonlinear transformations of linear combinations of the raw regressors as constructed regressors, called neurons. And produce the predicted value as a linear function of these regressors. The prediction rule is g(Z) is nonlinear in some parameters and with respect to raw regressors. With sufficient many neurons neurons, g(Z) can approximate the best prediction rule g(Z). In part 2 of our module, we already saw that many constructed regressors are useful in the high-dimensional linear setting to approximate g(Z).\n\nNeural networks also rely on many constructed regressors to approximate g(Z). The method and the name neural networks were loosely inspired by the mode of operation of the human brain, and developed by the scientists working on Artificial Intelligence. Accordingly, neural networks can be represented by cool graphs and diagrams that we will discuss shortly, so please stay tuned. To discuss the idea in more detail, we focus on the single layer neural network. The estimated prediction rule will take the form hat g(Z) equals sum over M, running from 1 to capital M of hat betaM times Xm of hat alphaM. Where the Xm of hat alpha m's are constructed regressors called neurons. The capital M neurons in total.\n\nThe neurons are generated by the formula Xm of alpha m = sigma of alpha mZ. Where alpha m's are neuron-specific vectors of parameters called weights and sigma is the activation function. For example, sigma can be the sigmoid function given by this formula. Or it can be the so called rectified linear unit function or ReLU given by this formula. The following figure shows the two graphs of the two Activation Functions. The sigmoid function and the rectified linear unit function. The horizontal axis shows the value of the argument. And the vertical axis the value of the function. The estimators had alpha m, and beta m for each M are obtained as the solution to the penalized nonlinear least squares problem shown by this formula.\n\nHere, we are minimizing the sum of squared prediction errors in the sample, plus a penalty term given by the sum of the absolute values of components of alpha m and beta m, multiplied by the penalty level, lambda. In this formula, we use the lasso type penalty but we can also use the ridge, another type of penalties. The estimates are computed using sophisticated gradient descent algorithms. Where sophistication is needed because nonlinear least squares optimization problem is generally not a convex problem. Making the computation a difficult task. The procedural fetching renewal network model has tuning parameters and in practice we can choose them by cross-validation.\n\nThe most important choices concern the number of neurons and the number of neuron layers. Having more neurons gives us more flexibility, just like having more constructed regressors gave us more flexibility with high-dimensional linear models. To prevent overfitting, we can rely on penalization as in the case of linear regression. In order to visualize working of the neural network, we rely on the resource called Playground.TensorFlow.org using which we produce a prediction model given by simple single layer neural network model. We now see the graphical representation of this network. Here we have a regression problem and the network depicts the process of taking row regressors and transforming them into predicted values. In the second column on the left, we see the inputs are features. These features are our two row regressors. The third column shows eight neurons. The neurons are constructed as linear combinations of the row regressors transformed by an activation function. That is, the neurons are along linear transformations of the row regressors. Here, we set the activation function, to be the rectified linear unit function, RE of U.\n\nThe neurons are connected to the inputs and the connections represent the coefficients hat alpha M, which are coefficients of the neuron specific linear transformations of raw regressors. The coloring represents the sign or the coefficients, orange negative and the blue positive. And with all the connections represents the size of the coefficients. The neurons are then linearly combined to produce the output, the prediction rule. In the diagram we see the connections going outwards from the neurons to the output. These connections represent the coefficients hat beta M, or the linear combination of the neurons that produce the final output. The coloring and the widths represent the sign and the size of these coefficients. In the diagram, the prediction rule is shown by the heatmap in the box on the right. On the horizontal and vertical axis, we see the values of the two inputs. The color and its intensity in the heatmap represent the predicted value.\n\nWe also see on the top that the penalty function is L1, which stands for the lasso type penalty. Another option is to use L2, which stands for the rich type penalty. The penalty level is called here the regularization rate. In this example we are using a single layer neural network. If we add one or two additional layers of neurons constructed from the previous layer of neurons, we get a different network, which we show in the following diagram. Prediction methods based on neural networks with several layers of neurons called the deep learning methods. This diagrams showcases that one of the major benefits of doing prediction with neural networks is that you can adapt with pretty cool artwork. Let us summarize, in this segment we have discussed neural networks that have recently gathered much attention under the umbrella of deep learning. Neural networks represent a powerful and all-purpose method for prediction and regression analysis. Using many neurons and multiple layers gives rise to networks that are very flexible and can approximate the best prediction rules quiet well.",createdAt:lL,updatedAt:lL,publishedAt:"2023-03-16T01:25:20.142Z"}},{id:1223,attributes:{title:"Causality: Can It Be Established From Regression?",slug:"causality-can-it-be-established-from-regression",duration:ld,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience312CausalityCanItBeEstablishedFromRegression",objective:"Learn about causality. Find out when you can and cannot establish it from regression.",englishTranscript:"In the previous lectures, we learned about the classical and modern regression methods and their uses for prediction and inference. This segment is about the use of regression to infer causal relations and answer causal questions. The causal questions are important and they arise in many real-world problems, especially in determining the efficacy of medical treatments, social programs and government policy, and in business applications. For example, does a particular drug cure an illness? Or, do more lenient gun laws increase murder rates and other types of crime? Or, does the introduction of a new product raise profits of a company? We begin by noting that regression uncovers correlation or association between outcome variables and regressors.\n\nHowever, correlation or association does not necessarily imply a causal relation unless certain assumptions hold. The association between outcome variable Y and a regressor D formally means that D predicts Y, namely the coefficient in the linear regression of Y when D is not zero or that, more generally, conditional expectation of Y given D is not constant. This is what the regression analysis gives us. So why doesn't association between Y and D necessarily imply that D causes Y? Here is a contrived example. Suppose we conduct an observational study on people's use of pain-killers and pain. We record the outcome variable Y, which is whether a person is in pain, and a regressor variable D, which is whether the person has taken a pain-killer medicine.\n\nIt's likely that people will take these pills when they are in fact in pain and so we are likely to find that D indeed predicts Y so that the regression coefficient of Y on D is positive. Y and D are associated but D does not cause Y as we know from the clinical trials in medicine that establish that pain killers actually work to remove pain. An ideal way to establish a causal relation from the regression analysis is to rely on data from a randomized control trial. And in the mass control trials we have the so called random assignment or. Remember the treatment variable T is a sign or generic D that is independent of specific outcomes.\n\nSpecifically in randomized control trials or experiments we have a group of treated participants and the untreated ones. The letter called the control group, and there are no systematic differences between the two groups when the trial starts. After the trial is completed we record the outcomes of interest for participants in the two groups, and carry out the regression analysis to estimate the regression equation Y = alpha + beta D + u. Here the regression coefficient beta D measures the causal input, or the treatment, on average outcomes. And so beta is called the average treatment effect. Under random assignment we don't need to use any additional regressors or controls. However we may use additional controls to improve the precision of estimating the average treatment effect better. This is called the Co variate adjustment method. Specifically we may set up a linear or partially linear model Y equals beta G plus G of zed plus epsilon and carry out inference variable using the inference methods that we have learned in this module.\n\nThe randomized control trials represent the golden standard in proving that medical treatments and social programs work, and for this reason they are very widely used in medicine and in policy analysis. And randomized control trials are also widely used in business applications, under the name of AIB testing. To evaluate whether new products and services raise profits. But if we don't have access to the data from the randomized controlled trials. What if we work a data set that is pure observation. Under what conditions can we claim that we've established a causal relationship. A sufficient condition is that D, variable of interest is generated as if randomly assigned, conditional on the set of control Z. This is called the conditional random assignment, or conditional exogeneity. There is condition on Z, variation in D is as if it were generated through some experiment. As in a randomized control trial. In this case, we can also apply the partial linear regression model, Y = beta D + g(Z) + epsilon, and beta D indeed measures the causal effect of D on average outcome controlling for Z. We then apply the inference method that we have learned to beta, and construct confidence intervals.\n\nOne note that we would like to make here is that under conditional random assignment, or conditional unlike under pure random assignment, controls must be included to ensure that we measure the causal effect and not something else. Under pure random assignment, we do measure the causal effect and not something else whether or not we include the controls. So on the conditional random assignment or conditional exogeneity, regression doesn't cover causal effects. For example, recall our case study of the impact of gun ownership on predicted gun homicide rates.\n\nWe did find that there that increases in gun ownership rates lead to higher predicted gun homicide rates, after controlling for the demographic and economic characteristics of various counties. If we believe that the variation in gun ownership rates across counties was as good as randomly assigned, after controlling for these characteristics, then we should conclude that the predictive effect is a causal effect. If we don't believe that this variation is as good as randomly assigned, even after controlling for all these characteristics, then we shouldn't conclude that the predictive effect we've found Is a causal effect. Similarly, in another case study, we studied the impact of being female on the predicted wage. We did find that females get paid on average two dollars less per hour than men, controlling for education, experience, and geographical location.\n\nIf we believe that the variation of gender conditional on these controls is as good as randomly assigned, then we should conclude that the predictive effect is a causal effect, which is the discrimination effect in this context. However, if we don't believe that this variation is as good as randomly assigned, then we should not claim that this effect is due to discrimination. As you can see, making causal claims from the observational data is difficult and the success really depends on being convincing at persuading ourselves and others that the assumption of conditional random assignment or conditional exogeneity holds here, the burden of persuasion lies entirely on the data scientist, if indeed she or he is willing to make the causal claim.\n\nI would like to conclude this module with some parting remarks. In this module we studied the use of the classical and modern linear and non-linear regression for purposes of prediction and inference, including causal inference. The material in this module is very difficult and if you've managed to navigate through it you have done extremely well. If you are interested in having additional reading and in replicating the case studies yourself, I have the following materials for you. First I have a complete set of slides. That accompany the video lectures. And second, I have data sets and programs written in R that carry out the studies. I encourage you to check and try those out. With this, let me say goodbye, and wish you luck in your data science adventures.",createdAt:lM,updatedAt:lM,publishedAt:"2023-03-16T01:25:17.805Z"}}]},instructors:[{id:kU,name:lN,position:a,nationality:a,achievements:a,focus:a,bio:lO,learnMoreURL:a}]}},{id:bn,attributes:{title:"Hypothesis Testing and Classification",slug:"hypothesis-testing-and-classification",description:"This module will cover the basics of anomaly detection and classification: for these tasks there are methods coming from either statistics or machine learning that are builton different principles. As well as the fundamentals of hypothesis testing, which is the formalization of scientific inquiry. This delicate statistical setup obeys a certain set of rules that will be explained and put in context with classification.",prerequisite:a,duration:"1 hr 50 min",objective:a,createdAt:"2023-10-18T21:49:04.643Z",updatedAt:"2023-10-18T21:49:45.555Z",publishedAt:"2023-03-16T13:13:19.865Z",chapters:{data:[{id:1222,attributes:{title:"Introduction to Hypothesis Testing and Classification",slug:"introduction-to-hypothesis-testing-and-classification",duration:"1:05",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience41IntroductionandHypothesisTestingandClassification",objective:"Meet the instructors and learn what classification is.",englishTranscript:"Hello and welcome to the module on classification and hypothesis testing. I'm David Garmanik from Sloan School of Management at MIT. I'm also a member of Center for Statistics at IDSS. My background is in applied probability, optimization, and algorithms. And I regularly teach probability statistics and optimization methods to Masters of Business Administration students. I will teach this module jointly with Jon Kelner.\n\n\u003E\u003E Hi, I'm Jonathan Kelner and I'm a professor in the Math Department and Computer Science and Artificial Intelligence Lab at MIT. My work also focuses on algorithms, optimization, and applied probability. And I regularly teach classes on mathematical foundations of probability, as well as on the algorithmic and computational techniques required to manipulate, understand, and extract meaning from data. At the beginning, we gave just one example of a concept commonly called classification in the field of statistics and machine learning.\n\nThe goal of classification is to be able to place a given object into the appropriate category based on its attributes, typically given by data associated with this object. In the credit card example, the object is the credit card transaction nd there are two assigned categories, legitimate and fraud.\n",createdAt:lP,updatedAt:lP,publishedAt:"2023-03-16T01:25:16.405Z"}},{id:1221,attributes:{title:"What Are Anomalies? What Is Fraud? Spams? Part 1",slug:"what-are-anomalies-what-is-fraud-spams-part-1",duration:"1:51",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience42WhatAreAnomaliesWhatIsFraudSpamsPart1",objective:"Learn about a common classification problem.",englishTranscript:"It's a beautiful sunny day and you just arrived at Schiphol Airport in Amsterdam. A sparkling European airport suggests the start of a great three day vacation. Tired from a long overseas flight you walk into a cafe and order a fresh cup of coffee with an apple Danish but your Visa card is declined. The credit card company suspects a fraudulent transaction. And in order to prevent the fraud it stops it. You walk to the nearest ATM to get some local currency. Perhaps, only to discover that your cash withdrawal is declined as well.\n\n\u003E\u003E A few months later, during a business trip to Toronto, you receive a phone call from your credit card company. The operator wishes to inquire about the legitimacy of a transaction on your credit card that was attempted the previous evening, in which someone tried to use the card to purchase some liquor in New Jersey. Yesterday evening you were en route to Toronto, and you certainly did not intend to buy any liquor. Your card was in your possession at all times, but the operator informs you that these days it's not hard for criminals to copy the information on a magnetic strip to a different card, which was probably what occurred. Luckily, the bank figured out that the purchase was almost certainly fraudulent, and it stopped the transaction.\n\n\u003E\u003E The two examples above are really two faces of the same coin. Your credit card company constantly needs to check the legitimacy of the transaction credit cards. The verification needs to be done rapidly in a matter of a split second. It needs to approve the transactions the company deems to legitimate, and decline the ones it suspects to be fraud.\n\n\u003E\u003E In the first example, a legitimate transaction was declined by your bank, creating a major inconvenience on your end.\n\n\u003E\u003E But in the second example, an illegitimate transaction was thankfully declined by the bank, saving you from the frustration of dealing with a fraud and possibility of even a financial loss.\n\n\u003E\u003E Credit card companies constantly deal with assessing the likelihood that a given transaction is illegitimate. Need to minimize the risk of either declining legitimate transaction or allowing a fraudulent one. In this module, we'll introduce statistical methods that assist companies in assessing these types of risks. It'll be our first example of a very common category of questions which we'll refer to as binary classification.",createdAt:lQ,updatedAt:lQ,publishedAt:"2023-03-16T01:25:14.961Z"}},{id:1220,attributes:{title:"What Are Anomalies? What Is Fraud? Spams? Part 2",slug:"what-are-anomalies-what-is-fraud-spams-part-2",duration:"3:28",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience43WhatAreAnomaliesWhatIsFraudSpamsPart2",objective:"Gain more understanding of binary classification.",englishTranscript:"The example above, for the credit card transaction and fraud detection, is one of the many, many examples where a classification problem needs to be solved based on the availability of imperfect information usually in the form of some data. Another very similar example of fraud detection is medical fraud. A medical provider sends an invoice to a health insurance company for some services provided to a patient. Sometimes unfortunately, the services are fake and possibly even the patient was nonexistent.\n\nThe insurance company needs to come up with a quick judgement on the validity of the claim, honoring the claims it deems valid and declining the claims that look suspicious. A set up very similar to the one with the credit card example. A slight difference is the timing of the decision and the actual process. If the claim is dimmed to be fraud, it is sometimes turned to a human expert who can make a further determination regarding the validity of the claim, and only upon further investigation make a final determination. Whereas in the credit card example, the determination is conducted immediately. As before the problem is classifying the claim as legitimate versus fraud. \u003E\u003E Let's consider a different example from the healthcare area.\n\nA patient is rushed to a hospital emergency center with severe symptoms including chest pain, shortness of breath, heart palpitations, sweat and some other symptoms usually associated with a heart attack. The doctors needs to make a quick determination of whether this is indeed a case of a heart attack and take the appropriate measures. Again this is the example of classification. Heart attack versus not heart attack. The determination needs to be made rapidly in potently life threatening situation. All of the examples above fall into the category of so called binary classification, where the goal is to place the given object into one of two possible classification groups. Other examples of binary classification are determining the gender, male or female of a photograph, spam versus not spam for emails, impact versus no impact for a potential drug, healthy versus cancerous for tissue in a tumor, global warming versus normal temperature fluctuations and geoscience among many others.\n\n\u003E\u003E The history of humans facing various kinds of classification problem is very old. Think about the questions enemy versus friend, murderer versus not a murderer. Think about any civil or criminal trial. The goal of the jury is to solve a binary classification problem. Guilty versus not guilty, based on the data, evidence. For good or for bad, the binary classification problem in the field of criminal justice is typically not solved using statistical methods per se, but using different methods such as witness testimony. But one can argue that indirectly, historical data plays an important role here. A person possessing a weapon of murder is far more likely to be a perpetrator than the one without a weapon in similar circumstances. At the same time, of course, other factors play an important role, such as witness testimony or simply prejudice. Think about the binary classification outcome of Salem witch trials.\n\n\u003E\u003E Even animals routinely solve binary classification problems when they face a potential threat. Upon seeing a creature, they need to make an instant termination of the kind, predator versus not a predator, and react rapidly and accordingly. The faster and more accurately this classification problem is solved the better the chances of survival are for the animal. In nature this classification problem is solved by the brain processing the given data associated with potential threat. Including the visual image, smell, speed of movement, etc. One artificial model based on how animals solve such a binary classification problem is called the neural network. It's intended to loosely model brain activity and it will be discussed later in this module.",createdAt:lR,updatedAt:lR,publishedAt:"2023-03-16T01:25:13.708Z"}},{id:1219,attributes:{title:"What Are Anomalies? What Is Fraud? Spams? Part 3",slug:"what-are-anomalies-what-is-fraud-spams-part-3",duration:"5:53",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience44WhatAreAnomaliesWhatIsFraudSpamsPart3",objective:"Learn about errors in classification analysis.",englishTranscript:"The classification problems we will discuss in this module will be the kind sold by in statistical methods based on your medical data. This applies to most, but not all of the examples above. For example, the criminal justice system example relies more on witness testimony rather than hard data and such examples will not be considered in this module. Depending on the context, one might deal with situations where the data is in abundance, or conversely, is very scarce. For example, in trying to design a classifier for spam, lots of data is typically available in the form of prior emails already categorized as spam versus not spam.\n\nConversely, in clinical trials the data comes from experiments conducted on human volunteers. Such things are very expensive both financially and otherwise. As a result, are based on much smaller amounts of experimental data.\n\n\u003E\u003E But even in the case where the data in abundant, one may face a huge imbalance of the data availability in the different categories. For example, typically only a very small proportion of credit card transactions are fraud. Thus, while the amount of data is huge, those that fall into the fraud category constitute a much smaller amount. Similarly only a tinny number of say images taken from a street camera correspond to wanted people such a criminals, terrorists or lost children. Furthermore, in trying to correctly classify the type of object the implications of misclassification actually may have very different consequences with different levels of severity. This brings us to the subject of type one versus type two errors. When solving a binary classification problem, we encounter two types of errors.\n\nConsider again a medical fraud example. The two categories associated with this example are legitimate and fraud. The two types of errors we're about to discuss relate the ultimate classification decision made by a classifier to the crack but unknown to the classifier category. Suppose for example an insurance company receives a claim which happens to be a legitimate one.\n\nThough of course company has no way of knowing that for sure. Suppose further that an automated system designed to make a preliminary determination declares the claim to be a fraud clearly making a mistake, somewhat arbitrary and somewhat non-creatively. We call this type 1 error to contrast it with the following mistake naturally called type 2 error. A claim which happens to be a fraud though unknown to the classifier is received but the classifier determines it to be a legitimate one.\n\n\u003E\u003E Consider our example of a patient with severe symptoms who is rushed to an emergency center. Suppose this patient is not having a heart attack, even though he has severe symptoms that make the heart attack hypothesis very plausible to doctors. Again, this is clearly not something the doctors know a priori. Suppose they make an incorrect inference and decide that the patient is having the heart attack, and they conduct the treatment appropriate for a heart attack. This is a type 1 classification error. Conversely if a patient is indeed suffering a heart attack but the medical team fails to see this we're dealing with a type 2 error.\n\n\u003E\u003E Deciding which type of the classification is type 1 versus type 2 is clearly somewhat arbitrary. And depends on the category which is declared to be the first category and which one is the second one. This is really up to the classifier, and simply is a matter of labeling the classes. Typically, in statistics the classes are called null hypothesis and alternative hypothesis, and we will talk about this in our future lectures. What is important to keep in mind though, is the potential difference in severity of making type one versus type two error.\n\n\u003E\u003E For example, in the case of medical fraud discussed earlier, a type one error leads to an inconvenience for the medical provider who needs to waste time proving the validity of his claim. If this happens to often the provider may stop excepting this type of insurance which could cost the insurance company to loose customers. On the other hand, a type 2 error, where the insurance company pays for a fraudulent claim, leads to a financial loss coming from paying for a false claim. It may also lead medical providers to believe they can get away with fraud and embolden them to commit further fraud in the future. It's a judgment call as to what type of error is more severe, and thus, which one needs to be mitigated more stringently. But clearly a balance needs to be struck.\n\nContrast this with the implications of the two types of errors in the heart attack example. Type 1 error leads to a healthy patient or a patient suffering from something much less severe then a heart attack, receiving the treatment for a heart attack. Conversely, type two error results in a patient suffering from a heart attack Not receiving the right treatments. The potential implications of a type 2 error are very severe, possibly leading to death. Whereas the implication of a type 1 error, while possibly very unpleasant, are usually not life-threatening.\n\n\u003E\u003E The potential difference of severity of two types of error is important to keep in mind since, as we will see later, there is a fundamental trade off between the likelihood of two types of errors. With a fixed amount of data, one can typically make type one errors small only at the expense of increasing type two error and vice versa. Balancing the two types of errors should depend ultimately on the severity of the implications of these errors. Think about the implications of type one and type two errors for our other examples. Guilty versus not guilty. Predator versus not a predator.\n\n\u003E\u003E A parting thought we would like to discuss in this introductory lecture is the importance of a solid scientific approach in applying a variety of classification methods, such as the ones that we are going to be discussing in future lectures. While these methods are based on rigorous statistical foundations, it's easy to misuse them and to obtain wrong conclusions by misapplying them as such. Such misuse is particularly likely in instances where the party conducting the classification analysis has a stake in a particular outcome.\n\nFor example, a pharmaceutical company testing the effectiveness of a particular drug or scientists testing a particular scientific hypothesis typically wish for one of the two particular outcomes. Understanding how to recognize and avoid such bias is a crucial yet surprisingly subtle question. At the end of this module we'll discuss some frequently occurring examples of the misuse of classification methods. And we'll discuss ways to prevent them.\n",createdAt:lS,updatedAt:lS,publishedAt:"2023-03-16T01:25:12.046Z"}},{id:1218,attributes:{title:"False Positive\u002FNegative, Precision\u002FRecall, F-Score",slug:"false-positive-negative-precision-recall-f-score",duration:"7:13",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience45FalsePositiveNegativePrecisionRecallFScore",objective:"Learn how to evaluate a classifier using the metrics presented.",englishTranscript:"In the previous section we introduced binary classification, in which the goal is to put an object into one of two possible categories. Our aim later in this module will be to design effective methods for performing this task. But in order to do this, it's important to have a well defined notion of what it means for a classifier to be effective. In this video we'll examine some of the considerations that go into evaluating the quality of a classifier and I'll present a few of the metrics that are frequently used to do so. As a concrete example, let's consider the case we described in the last video where a credit card company is trying to identify fraud. When presented with a transaction, the classifier has to either label it fraud or not fraud. We discussed two different ways that the classifier could make a mistake.\n\nA type I error, where we identify a legitimate transaction as fraud, and a type II error, where we label a fraudulent transaction as legitimate. It's useful to think of the classification problem as answering the yes or no question, is this transaction fraud? So that we can then describe the categories as positive, corresponding to the answer, yes, this is fraud, and negative, corresponding to the answer, no, this is not fraud. It's often helpful to arrange the possibilities as a table, where the columns correspond to the answer the classifier gives, and the rows correspond to the true underlying answer. We thus have four possibilities.\n\nThe first is that the transaction was fraud, and the classifier correctly labelled it as fraud. In this case the true category was positive and the label was positive as well. We'll call this a true positive. The next possibility is a type I error. Here, the transaction was not fraud but the classifier incorrectly said that it was. In this case, the true category was negative, but the classifier falsely labeled it as positive. So we call these false positives. The third possibility is a type II error, where the transaction was fraud, so the right answer was positive, but the classifier falsely labeled it as negative. So we call these false negatives. The final possibility, which hopefully is the most common, is that the transaction was legitimate. So in this case, the true answer was negative, and the classifier correctly labelled it as such. We call these true negatives. We can then visualize the performance of the classifier by writing in each box the number of times that the corresponding possibility occurred, forming what is sometimes referred to as the confusion matrix. Note that the upper left and lower right numbers correspond to correct answers and the upper right and lower left ones correspond to mistakes.\n\nNow, how should we evaluate the quality of our classifier? A natural first guess would be to just look at what fraction of the time it gives the right answer. However, this number can be misleading, particularly in settings like this one, where one of the categories is much more common than the other. To see the problem, suppose that 1% of the transactions are fraudulent. And suppose that the credit card company's very lazy fraud detection department just wrote a computer program that labeled everything as legitimate. This is clearly not a very helpful classifier, since it never says there's any fraud. But it's correct 99% of the time.\n\nTo get a better evaluation, we need an approach that provides more nuanced information by the different ways in which the classifier can be right or wrong. A common way to do this is to look at two numbers, precision and recall. Roughly speaking, precision will tell us how often the classifier is right when it says something is fraud. And recall will tell us how much of the actual fraud that occurs we correctly detect. Let's now define these mathematically. We define the precision to be the number of true positives divided by the number of true positives plus the number of false positives. Note that the denominator equals the total number of examples that the classifier labeled as positive. This number tells us what percentage of the time an instance labelled positive was actually positive. In our fraud example, the classifier labeled 2 plus 8 equals 10 instances as fraud. Of these, 2 were actually fraud, so the precision was 2\u002F10 = 0.2. We then define the recall to be the number of true positives divided by the number of true positives plus the number of false negatives. Here, note that the denominator equals the total number of examples in which fraud actually occurred. This number tells us what percentage of the actual positive examples our classifier detects. In the fraud example, there were 2 plus 4 equals 6 instances of fraud, and the classifier correctly caught two of them. So the recall here was 2\u002F6, which is 1\u002F3. This number measures how sensitive our classifier is to indicators of possible fraud, so it's sometimes called the sensitivity. Note, this would actually catch the problem with the lazy fraud detection algorithm that we described before. The one that describes everything as legitimate, since in this cases the sensitivity would be zero.\n\nKnow that there's often a trade-off between these two quantities. A more aggressive fraud department that flags a huge number of things as fraud, sometimes incorrectly, would have a high recall, since it would catch a lot of the fraud that occurs. But it would have a low precision, since it would also flag a lot of legitimate transactions as fraud. On the other hand, a very conservative department that only flags the most obvious cases would probably have high precision. But it would miss the subtler cases of fraud, and would thus have lower recall.\n\nHow one trades off between these two numbers is a judgement call, based on the specifics of the situation. It is often most informative to look at both of these numbers separately, since they measure different properties of the classifier. However, if you want a single number to evaluate the quality of a classifier, there's a statistic called the F-score, or sometimes called the F1 score, that combines the two. Our first attempt at combining the two numbers would be to just take their average. However, this often isn't quite what we want. In particular, suppose we are in the case where the classifier labels almost everything as positive. Here, the recall would be great, that is very close to one, but the precision would be quite small, and thus close to zero. This would probably not be a very good classifier, but averaging the two numbers produces something close to a half. Instead, we would like a way of combining the two numbers that puts more emphasis on the smaller value.\n\nA good choice for this turns out to be what's known as the harmonic mean. The harmonic mean of two numbers, x and y, is given by 1 over the average of 1\u002Fx and 1\u002Fy. This will always be in between x and y, but it will be small if either of the two numbers is small. So it's closer to the smaller of the two numbers, in the arithmetic means. The F-score is then defined to be the harmonic mean of the precision and the recall. So it equals 1 over the average of 1 over the precision, and 1 over the recall. Simplifying this gives a slightly nicer expression. F1 = 2 x precision x recall over precision + recall.\n\nSo in summary, we now have a couple of measures for evaluating how good a job a classifier does. We defined precision and recall, two numbers that capture different facets of the quality of classifier. And then we showed how to combine them into an F1 score. This gives us a set of statistics we can use so that later we can decide, when we have some classifier, how good a job it's doing. In the next section, we're gonna switch a little bit and talk about hypothesis testing. And then we'll come back to how to build effective methods for constructing specific classifiers.\n",createdAt:lT,updatedAt:lT,publishedAt:"2023-03-16T01:25:10.773Z"}},{id:1217,attributes:{title:"Hypothesis Testing",slug:"hypothesis-testing",duration:"1:20",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience46HypothesisTesting",objective:"Learn what a hypothesis test is.",englishTranscript:"Today, we will introduce a very important statistical concept. Hypothesis testing. The meaning of the term hypothesis testing is precisely what it says. Using a data observed from a distribution with unknown parameters, we hypothesize that the parameters of this distribution take particular value and test the validity of this hypothesis using statistical methods. We test a hypothesis.\n\nThe method of hypothesis testing has many applications. Say a new drug for lowering blood pressure was just produced by a certain pharmaceutical company. Does the drug has a meaningful effect on lowering the blood pressure as the company claims? Is the impact larger than that of the competitive drugs. A community organization claims that the police in a certain precinct is prejudiced and tends to stop disproportionately many people of color during traffic patrols. They have produced some data to support their claim showing much larger number of people of color stopped than say, on average, across US.\n\nA car manufacturer claims to have achieved a significantly lower emission rate than those of its competitors and has provided data to support their claim. The data supporting these kinds of claims is naturally subject to a lot of uncertainty. And one can never be completely certain whether or not the claim has any validity to it. One has to rely on statistical methods to draw such conclusions. Today, we introduced some of this methods.",createdAt:lU,updatedAt:lU,publishedAt:"2023-03-16T01:25:09.456Z"}},{id:1216,attributes:{title:"Confidence Intervals",slug:"confidence-intervals",duration:"6:46",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience47ConfidenceIntervals",objective:"Learn about confidence intervals and how they relate to hypothesis testing.",englishTranscript:"To get started, we should review some key concepts related to the, so called, confidence intervals. The confidence intervals are intended to provide us with probabilistic level of certainty about our conclusions, regarding parameters of some probability distribution. Let us introduce it somewhat more formally. Suppose you have several observations, X1, X2, and so on, Xn from the probability distribution. Suppose you have a prior knowledge that this is a normal distribution, but with some unknown mean value, mu. Supposed at the same time you happen to know the standard deviation, sigma, for this normal distribution.\n\nThis is not as unreasonable as it may sound. As you may recall, we denote such distribution as follows. Our goal is to estimate mu, a natural estimation for mu is simply the average of our observations, which we denote by x bar, like this. Naturally, the actual value of X bar, practically, will never equal to the mean value mu. Our hope is, though, that the estimation is not too far. But, how far is too far? This is where the concept of confidence interval helps. Let us use the property of the normal distribution, which says, that in fact, X bar itself also has a normal distribution, with the same mean value, mu, and standard deviation, equal to sigma over square root of n. Namely, the standard deviation of X bar is this. Using our notational convention, X bar then has the following distribution. It is very important to understand, that while X bar is intended to estimate the unknown mean value, mu, it is actually a random variable with this distribution.\n\nFor such a random variable, we can compute the probability that it falls in a certain range. For example, we can compute the probability that it does not exceed the actual, but unknown to us, mean value of mu by, say, two units. Namely, our estimation is now two units greater than the actual mean value. Here's how we compute this probability. First we write it like this. And now we rewrite it like this, and now we rewrite it further, in the following, admittedly, weird way. Why in the world we would want to do something like this? The answer is that, what we have on the left-hand side, namely this expression, is actually a standard normal random variable. That is, it is the normal random variable, with mean 0 and standard deviation equal to unit. Using our notation, this is, to convince yourself of this, you need to recall some of the basic properties of the normal random variable. Like, what happens to it when you multiply it by some number, and observe that the mean of X bar is mu, and the standard deviation is sigma divided by the square root of n?\n\nNamely, recall that X bar has distribution as follows. The second part of the magic, is that the expression, which you see here, magically does not involve the unknown value mu. It involves sigma, which we know, and the sample size, n, which we certainly know, since this is just a sample size. So we just need to compute the probability that a standard normal random variable does not exceed the following value. But this we can do, either by consulting the so-called standard normal distribution table. Search Google for this, for example, or applying any standard statistical packages.\n\nLet us do an example. Say you have collected a data set consisting of 40 observations. In this case, the sample size, n, is 40. Suppose you happen to know that the standard deviation of this distribution, from which the sample is generated, is 6.8. From this, we can immediately find that, from the standard normal distribution table, we'll find that the probability that the standard random variable does not exceed this value, is approximately 0.968. You have computed the average of these 40 observations, and let us say you found it to be 4.7. Then according to our derivation, the true, but unknown, mean value mu, is at least 4.7 minus two, which is 2.7, with probability 0.968. We also say that our confidence level of this statement is 0.968, or roughly 96%. In this example, we have assumed the knowledge of the standard deviation sigma. Usually in practice, we do not have it, but we can still estimate it from data using the following expression, which we denote by sigma bar. Here, X1, X2, and so on, Xn are your observations. Just like X bar is perhaps the most reasonable estimation of the actual, but unknown, mean value mu. This formula gives perhaps the most reasonable estimation for true, but unknowns standard deviation value, Why?\n\nThis is outside of the scope of this lecture, but I invite you read about this in one of the statistics books. This substitution appears to rely on circular reasoning. Shouldn't we have confidence intervals for sigma bar first, before we use it for our computation of confidence intervals for the mean value mu? The answer is that, when the sample sizes are reasonably large, usually at least 30, the estimation for sigma bar is far more accurate than the level of error we will encounter for mu. So, this substitution is a reasonable one. There is a deeper theory behind this, which relies on the so-called, t-distribution, which we'll skip today.\n\nIn the example above, we have obtained an estimation from mu, which is only one sided, namely, it is at least 2.7. Ideally, we would like to have an estimated from above and from below. Additionally, often we want to have a target level of confidence level, such as 0.95 or 0.99, rather then whatever comes out here, like 0.968. This can be done by, sort of, reverse engineering our interval, so that the resulting certainty equals the target we need. Say we want to have confidence level of 0.95. Here's how we can find the corresponding confidence interval. The derivation here is terse, and I invite you to take some time to parse it, or consult a statistics book for a more detailed explanation. But it is worth trying to parse it yourself first. So our goal is to find a value c, such that the following is true. We can rewrite this equivalently as follows. And, if you find the right value for c, for this to be the case, our 95% confidence interval would be of the form X minus c, to X plus c.\n\nNow saying that this relationship is true, is the same as the following being true, which is the same as the following being true. From the standard normal distribution table, I can find the magic number, 1.96, which satisfies the following property. We obtain that the following must be the case, from which we find the formula for c as follows. Since we know the standard deviation, which in our example was 6.8, and since we know the sample size, which in our case, is 40. We can compute c as follows, we conclude that the true, but unknown, mean value mu lies within approximately 2.1 units from X bar, which is 4.7 with confidence 0.95, namely 95%. This interval is in fact defined by 2 values, the lower value here, and the upper value here. We can summarize our conclusions as follows, the true, but unknown, mean value mu is at least 2.6, and at most 6.8, with 95% confidence.\n\nThe interval length might appear as too wide. This is perhaps true, but sometimes even a very wide interval can provide an extremely useful information. When, for example, you want to figure out whether the mean value equals 0 or not. That is, you hypothesize that the mean value is 0 and you want to test it. In other words, you want to conduct a hypothesis testing.",createdAt:lV,updatedAt:lV,publishedAt:"2023-03-16T01:25:08.185Z"}},{id:1215,attributes:{title:"Validity of Binomial Distribution",slug:"validity-of-binomial-distribution",duration:lz,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience48ValidityofBinomialDistribution",objective:"Learn about how hypothesis testing works with a binomial distribution.",englishTranscript:"In the discussion of hypothesis testing, we have considered only one example involving normally distributed random variables. But the hypothesis testing method is much broader than that, and allows you to test the validity of the probability distribution itself. Let us illustrate this in a special case when the underlying distribution is binomial. In the beginning of the lecture, we mentioned an example of using hypothesis testing for the analysis of police racial profiling and possibly testing the hypothesis that the police is particularly biased against people of color when they pull over drivers on the road. Let us say that in the year of 2015 among the drivers stopped by the police 35% were black drivers. The actual figures are somewhat different and you can find some of these figures on the Bureau of Justice Statistics website.\n\nOne way to think about the statistics is that if one draws a random driver from the list of all drivers stopped by the police in the year of 2015, theres a 35% chance that this driver is African American. Since this percentage is higher than the percentage of black drivers among all the roughly 200 million drivers in the US, this already suggests a bias. But let us imagine the following situation. Community organizers in several towns across several states, claim that in fact the situation is worse in their communities than it is in the US on average, however disturbing these averages are on their own. They claim that even if one accepts for the sake of argument that racial profiling is non-existing from a statistical point of view, in the US on average the police precincts in their towns do rely on racial profiling.\n\nTo support their claim, the data was provided for two precincts, which we just call Precinct A and Precinct B for simplicity. The data shows that during the year of 2015, out of approximately 12,567 drivers stopped by the police, in fact 4,513 were black. 4,513 divided by 12,567 is approximately 0.359, which is larger than 35% and hence the claim of racial profiling. For the second Precinct B among 15,687 drivers stopped 5,562 were black. Again, 5,562 divided by 15,687 is approximately 0.354, which is also larger than 35% and the claim of racial profiling is made again. How can we asses the statistical validity of these claims?\n\nWe do this by conducting the hypotheses test. We take as our null hypothesis, namely the hypothesis to be tested, that the drivers pulled over by police in these precincts are as likely to be black as for the statistics across US suggest. Namely, a driver pulled over by police in these precincts is black with 35% likelihood, as is the national figure. We set Type 1 error threshold alpha to be 5%, this is up to the person conducting the test, of course and different error rates may be considered, but let us accept if for now. Assuming the validity of this hypothesis, we will compute the likelihood that among 12,567 drivers in Precinct A at least 4,513 were black. Namely at least as many as was the number of black drivers actually pulled over. If this probability is greater than 5%, we accept the hypothesis as valid with 5% error rate and say that the exceedance of 35% rate in this precinct is just a statistical fluctuation. If it is less than 5%, we will reject this hypothesis and say that the number of black drivers actually pulled over was unusually high. We will conduct a similar analysis for Precinct B. How can we compute these probabilities?\n\nIf the 35% hypothesis is true, then according to the binomial distribution, the likelihood that out of 12,567 drivers at least 4,513 were black can be computed using the following formula for binomial distribution as follows. Take some time to convince yourself that we have applied the formula for the binomial distribution correctly. This might look horrendous but in fact most statistical softwares and many other non-statistical packages such as MathLab will find and compute this expression in a split sec. The answer in this case is 0.016 or about 1.6%. We conclude that under the null hypothesis, no profiling. The likelihood that out of 12,567 stopped drivers 4,513 were black is approximately only 1.6%. Since this is below 5% selected for Type 1 error tolerance, we must reject the hypothesis. Thus, according to the data, it is unlikely, only 1.6% chance, that the black drivers were stopped at the same rate as in the US on average. This of course does not legally prove the presence of racial profiling, just indicates the statistical evidence for it.\n\nWhat about Precinct B? Using a similar analysis the likelihood that out of 15,687 stopped drivers at least 5,562 are black is given by the following expression, which is evaluated to be approximately 11.4%. Since 11.4% is higher than 5%, we must accept the null hypothesis. Namely, according to the statistical analysis, there is no evidence of racial profiling in Precinct B. Of course again, this is just a statistical analysis. In the reality, such an analysis might suggest a further investigation where profiling appears to take place according to the statistical evidence. As far as the analysis can either confirm the presence of profiling or not find any evidence of it. The statistical tools can simply assist in the identifying, relatively quickly, precincts where there is some statistical evidence of profiling.\n\nFrom the methodological perspective, we just gave an example of testing your hypothesis regarding the validity of a certain distributional assumption, binomial distribution being the case. One can naturally apply it to other types of distributions, such as uniform, geometric, Poisson distribution and many others. The method is very general. It is also very useful, I hope I have convinced you of that.",createdAt:lW,updatedAt:lW,publishedAt:"2023-03-16T01:25:07.113Z"}},{id:1214,attributes:{title:"Misuses of Statistics",slug:"misuses-of-statistics",duration:"9:06",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience49MisusesofStatistics",objective:"Learn how to not misuse your statistical output.",englishTranscript:"Let us say you are managing a manufacturing firm which produces a certain electronic device. And the industry standard for the lifetime of this device is to have let's say, lifetime of at least nine life years. Your engineer claims the device produced by your company fits the standard and in fact has average lifetime of 9.4 years. And a standard deviation of 1.43 years. To support his claim he gives you sample of lifetimes for 50 devices with various actual lifetimes. In practice, one does not need to wait for ten years to test the lifetime of the device, but instead there is a procedure to estimate its projected lifetime using a certain manufacturing process. The details of this process are besides the point in our skit. You have computed the average of the projected lifetimes for the sample provided by your engineer, and found it to be 9.6 years, which is above the industry standard of 9 years, which is good.\n\nYour goal is to test the following hypothesis. The lifetime of the device follows a normal distribution with mean equal to 9.4 and standard deviation equal to 1.43 years. Since the mean value in this hypothesis is 9.4 which is greater than 9, confirming this hypothesis would be good news as far as meeting the industry standard is concerned. So, our hypothesis is as follows. How can you test this hypothesis? First some notation. When you introduce and test some hypothesis, a convention is to call it a null hypothesis, and denote it as follows. Now let's do actual testing. This is done by first assuming that the hypothesis is true, selecting a certain measurement error value C. And computing the probability that the outcome, namely the average of these 50 observations falls within C units of the claimed value of the mean value of mu. Which in our case, was claimed to be 9.4.\n\nIf we choose our measurement error to be, say, 0.3 lifetime years, then we need to conclude the following probability. How can we compute this probability? Well again, we can use the trick of manipulating x bar in the standard, normal random variable as follows. As in our discussion of confidence intervals, the random variable x bar minus mu over sigma divided by square root of n, is actually a standard normal random variable. Let's denote it by Z for convenience. Then we have to compute the following. And we have everything we need to compute this probability. We have n, which in our case is 50. We have sigma, which in our case is claimed to be 1.43. And we have c, which we have chosen to be 0.3. So we need to compute the following probability which we can find in the table for standard normal distribution. It is approximately 0.861 or roughly 86%. Let's summarize what we found.\n\nWe have computed that when a sample of 50 observations from normally distributed random variable with mean value equal to 9.4 years, and standard deviation equal to 1.43 years is drawn. The probability that this sample falls within 0.3 units from the mean value of 9.4 equals 0.861, namely about 86%. Wait a minute. Where did we use in this computation that our actual observed average of 50 estimated lifetimes was 9.6? This is the where we can use it to test the validity of the hypothesis. Falling within 0.3 units of the claimed mean value of 9.4 occurs with 86% likelihood if the hypothesis is true. Our observed average is 9.6, which is in fact within 0.2 units from the claimed mean value of 9.4. Since 0.2 is in fact smaller than 0.3, we'll say that we accept this hypothesis and we do so with 95% confidence. If you or your engineer collected a different sample with, say, average 9.05, and would want to use this average to test the same hypothesis, you would do the same thing. Compute the difference of 9.05 and the claimed mean value of 9.4, which is negative 0.35. Since negative 0.35 in absolute terms is larger than 0.3, the hypothesis should be rejected. Notice that you have rejected the hypothesis even though your observed average was 9.05, which is higher than the industry standard of 9 years.\n\nYou still reject the hypothesis because the hypothesis was about the mean of the distribution, and not meeting the industry standard per se. There's something unnerving about this method. Whether we accept or reject the hypothesis depends on the data we observe. For different samples, we may either accept or reject the same hypothesis. But the samples come from data and this is the only thing we have access to. We can only draw our conclusion from observed data. There is no crystal ball to tell us whether the claimed value of 9.4 years of lifetime is correct or not. In fact, there is no such thing as the correct mean value. Since the concept of the distribution underlined observed values is entirely in our head, and it is used as a convenient conceptual device to test the validity of our conclusions.\n\nThere is something else which is perhaps annoying in this example. The confidence level in our hypothesis testing example was computed to be approximately 86%. This was an implication of choosing somewhat arbitrarily the error value of 0.3 years for the lifetime duration of the device. Well this is something we can deal with. We can choose the target level of confidence say 95% and compute the implied error value. This is very similar to what we have encountered in our discussion of confidence levels. To achieve the target value of 95%, Namely the error rate of 5%. Let us first denote this error rate by alpha. Now we can compute the implied error value c as follows. From the table, as we recall, there is a magic number of 1.96 for the following idea.\n\nWe conclude that the following is true. Looks familiar? It should. In our case, we use the fact that sigma is hypothesized to be 1.43 and the sample size is 50, and find that the value of c is 0.396 like this. We summarize this as follows. We accept our hypothesis of mean value of 9.4, and standard deviation 1.43 with 95% confidence level if our observed sample average falls within 0.396 units from the claimed value of 9.4. Recall that the sample brought to you by your engineer had average value of 9.6 units, which is within 0.2 units of the claimed mean value. 0.2 is smaller than 0.396 so the hypothesis should be accepted.\n\nIf the observed average was 9.05, as in our second example, then since the difference of 9.05 and 9.4, which is negative 0.35 is still smaller than 0.396, the hypothesis should still be accepted. It was though, rejected before. What has changed? You've guessed it. The error value of 0.3 chosen by us was more stringent that the error value of 0.396 implied by 95% confidence level. Thus, there is something else here which is perhaps unnerving in the discussion above. The same hypothesis with the same data and thus the same average value might be accepted or rejected depending on perhaps somewhat arbitrarily chosen confidence level. But maybe this is not so bad since it allows for choice of confidence level in different areas in different industries. For example, when dealing with drug manufacturing or anything else which relates to human lives and well being, for example airline safety.\n\nOne might want to desire a very high level of confidence of let's say, 99% or even 99.9%. Industry experts in the government do about worry about setting such standards. At the same time, say in retail industry if your goal is test the hypothesis of mean value of the number of online customers in order to see if it say significantly exceeds the mean value of the number of customers visiting your brick and mortar stores. Making a mistake in testing this hypothesis is at worst would lead to poorer management decision. Say closing your chain of physical stores and focusing entirely on online industry, but not loss of human lifes.\n\nUnless as a company manager, you feel suicidal about your poor management judgement which obviously you should not. We have mentioned earlier the convention of calling the hypothesis, which is being tested, a null hypothesis. It is time to discuss another related concept called type 1 error, which has implicitly already surfaced in our discussion. Note than even when the hypothesis is true, say in our example the mean value was indeed 9.4 and standard deviation was indeed 1.43. There is a chance that the hypothesis is rejected simply because the sampled average was unusually far from the true but unknown to us mean value of 9.4. In fact, we have set up our testing framework in such a way that the likelihood of this rejection of the true hypothesis equals 5%. The error of rejecting a true hypothesis is called type 1 error, which in our case is 5%. This is contrasted with type 2 error. Type 2 error corresponds to the case of accepting by mistake a hypothesis when it is wrong. For example, accepting the hypothesis that the mean lifetime of the device 9.4, when in fact, it is not. To estimate the probability for such an error, one naturally has to formulate the meaning of something like, 9.4 is not the right mean. Namely, one has to formulate what is called an alternative hypothesis, for example something like this. The lifetime mean value of the device is at least 10.5. To contrast it with null hypothesis, such a hypothesis is called alternative hypothesis, and it is denoted like this.\n",createdAt:lX,updatedAt:lX,publishedAt:"2023-03-16T01:25:05.309Z"}},{id:je,attributes:{title:"P-Value",slug:"p-value",duration:"9:39",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience410PValue",objective:"Learn about the p-value and how it works.",englishTranscript:"In discussing the hypothesis testing method, we have introduced in particular, a method for accepting or rejecting a hypothesis which is selected to be the null hypothesis. The method was based on introducing some measurement of the observed sample such as its sample mean and computing the probability that this measurement falls within some declared region such as for example, an interval around the mean which is claimed by the null hypothesis.\n\nThe life of the interval was engineered so that the probability of falling outside of this interval, which as you may recall is called type 1 error, achieves the target value alpha. Usually taken to be, for example, 5% or 1%. In a case that this measurement indeed falls outside of the interval, the hypothesis is rejected otherwise, it is accepted. Thus, if the hypothesis is true the likelihood it is rejected equals value alpha. There is a certain limitation of this method. This mechanism provides an acceptance rejection rule for the hypothesis, but does not provide any sense of the strengths of the measurement in either supporting or rejecting the hypothesis.\n\nIf we were to have two measurements of the sample and say based both of them the hypothesis should have been rejected, which one has stronger evidence for rejection? To gauge the strength of evidence for either supporting or rejecting a hypothesis we introduce now a new and important concept of p-value. A formal definition of the p-value sounds something like this. The p-value is the probability of observing an outcome which is at least as hostile or adversarial to the null hypothesis as the one observed. Did you get this? Probably not. So let us discuss an example.\n\nLet us recall our example of a manufacturing device lifetime. Recall, that the null hypothesis in this particular example stated that the mean lifetime of the manufacturing device is 9.4 years. Then we computed a magic number 0.396 which had the property that if the computed sample mean is within 0.396 lifetime units from 9.4, then the hypothesis is accepted. Otherwise, it is rejected. Suppose you have actually gathered two samples, one with 50 elements with a sample mean equal to 8.96, and another one with 60 elements, with sample mean equal to 9.81.\nFirst note that according to each of this measurement the null hypothesis should be rejected. Verify this on your own. Each of this measurements will be associated with some p-value which we now describe how to compute. Take first 8.96 sample average. What is the probability that when we generate a different and independent sample average of 50 observations we get the value less than 8.96 if the null hypothesis is true.\n\nBefore we answer this let us discuss why we want to compute something like this? We want to have a sense of how unlikely it is to obtain a sample average of 8.96. Now the probability for observing exactly 8.96 is practically zero. So instead, we're asking to compute the likelihood of observing something even worse than 8.96. And this is our approximate measurement of the unlikelihood of seeing the number as far from the claim mean of 9.4 as 8.96. Now worse can mean two things. One is natural getting a number smaller than 8.96. But there is a complimentary worse, the difference between 8.96 and 9.4 is negative 0.44. Then getting a sample of 9.4 plus 0.44, namely 9.84 is as bad as the one of 8.96. So getting a measurement above 9.84 in this sense is also worse than getting the number 8.96. Now let us compute the probability of getting a measurement either below 8.96 or above 9.84.\n\nI say that this probability is the probability that the standard normal variable which we denote by capital Z here. Takes at most the following value for the sigma equals to 1.43 is a standard deviation of the device under the null hypothesis. I'll let you figure out why this is the case. On your own we have done similar deviations already several times, so you should be able to do that yourself. From the standard normal distribution table we find that the probability that the standard normal rounding variable x value below a negative 2.175 is about 0.015 which is about 1.5%. So our answer is twice that much namely 0.03 or 3%. We conclude that the p-value for our sample is 3%. Inwards, the likelihood of getting a sample which is as far from the hypothesis as the one observed is 3%. Since the p-value of 3% is kind of small, kind of. We will be inclined to reject this hypothesis. Notice that there is a simple relationship between the p-value and our acceptance, rejection rule say based on the type one error alpha equal to 5%. The hypothesis is accepted if the p-value exceeds 5% and rejected if it stays below 5%.\n\nA similar rule applies if the value of the type one error is said to be 1%. Often the p-value is defined as probability of being worse than observed only in one direction, in this case being below 8.96. So in our example because the normal distribution is symmetric about its mean, the actual p-value would be only half of the 3% corresponding to this probability namely only 1.5%. The one-sided version of the p-value is particularly useful when the distribution is not symmetric around its mean as we are about to see in our next example.\n\nIt is important to be explicit in what the p value stands for when conducting a statistical test. So far it seems like we have not done anything new here. But wait a minute, we have the second sample of 60 observations with average 9.81. The difference of 9.81 and 9.4 is 0.41. As a result, the p-value is computing using the following formula. From the standard normal distribution, we find the answer to be twice 0.013, namely 0.026, that is 2.6%. We conclude that the p-value for the second sample is only 2.6%. It is still below the 5% cutoff so the rejection is called for, but its value of 2.6% is even lower than the value of 3% for the value of the first sample. So the p-value of the second sample provides, so to speak, even stronger evidence against a null hypothesis. If we had two samples, one with say, p-value of 11% and one with say, 18%, the null hypothesis would be accepted based on both.\n\nBut arguably, the second p-value gives us stronger evidence supporting the hypothesis. So it is natural to call p-value the strength of the test, a sample mean in our case. The larger the p-value, the stronger is the evidence supporting the hypothesis. The smaller is the p-value, the stronger is the evidence against the hypothesis. Now let us turn to our other example, testing the binomial distribution in the context of racial profiling. Recall that in precinct A, from 12,567 drivers stopped by the police, 4,513 were black. Whereas the national figure is approximately 35%. We have computed that under the null hypothesis stating that the likelihood that a random driver stopped by the police has 35% chance of being a black driver, the probability of having at least 4,513 black drivers is 1.6%. That was below 5% selected value for alpha as a type one error and the hypothesis was rejected.\n\nSince the binomial distribution is not symmetric. So it is approximately symmetric since it can be approximated by the normal distribution. Let us compute the one sided p-value namely the probability of observing an outcome even more hostile to the null hypothesis than the one observed. Guess what? We have already computed it. It is 1.6%. Indeed, since the observed value was 4,513, then the p-value is the probability of having at least 4,513 stopped drivers to be black, which we have computed to be 1.6%. For the precinct B, the one-sided p-value is the probability that at lease 5,562 drivers out of 15,687 were black, which we have computed to be approximately 11.4%.\n\nThen the p-value for this experiment is 11.4%. There is a lot of discussion as to what extent one should rely on p-values of say 5% or 1%. Type 1 errors for accepting or rejecting hypothesis in various fields. These are just useful tools and they need to be used with extreme care and never applied blindly. The context of the underlying statistical estimation should almost invariably guide the application of the maths. In our last lecture in this module, we will show examples of how the application of methods such as hypothesis testing and p-value can be misused, and lead to wrong conclusions. And I finish this part of the lecture by offering you the following challenge. The p-value is naturally a random variable, as it is the probability of a observing a value smaller than the observed sample average. Since the simple average is a random variable so is the probability of seeing the value below it.\n\nWhat is the distribution of this random variable? Well, there is a magic answer. The distribution of the p-value is uniform in the interval between 0 & 1 when the distribution corresponding to the measurement is continuous. This is not obvious at all. But try to derive this fact. It is a good brain challenge.",createdAt:lY,updatedAt:lY,publishedAt:"2023-03-16T01:25:03.118Z"}},{id:he,attributes:{title:"Methods of Estimating Likelihood",slug:"methods-of-estimating-likelihood",duration:"6:42",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience411MethodsofEstimatingLikelihood",objective:"Learn about estimating likelihood.",englishTranscript:"On January 28, 1986, NASA's space shuttle Challenger was scheduled to be launched from Cape Canaveral, Florida. The event was highly publicized, and President Reagan was expected to make an announcement about the launch on news channels. The temperature was near freezing on this day, which was unusual for this geographic location. Several engineers expressed concerns about the safety of the launch. Specifically, they were concerned about the reliability of so-called o-rings at low temperatures, rising the possibility of failure, which could lead to a catastrophe. However, there were five such rings. And all five of them would need to fail for the destruction of the Challenger.\n\nThe managers at NASA, who were strongly against the postponement of the launch, noted that among the o-rings failures observed, a few happened at high temperature, thereby conjecturing that the threat for low temperature was not worthy of postponement. After intense discussion, they prevailed, and NASA went ahead with the launch. Tragically, soon after the launch, the Challenger disintegrated and came apart, killing everybody on board. The post-mortem analysis revealed that a failure of all five rings of the shuttle were the cause of the disaster. Was the disaster avoidable? Was there enough evidence for the engineers to convince managers at NASA that the performance of o-rings at low temperature was a serious vulnerability? In this lecture, we discuss the methods of estimating the likelihood, namely the probability of a particular event, given the data at hand. The method has many, many applications across fields.\n\nAnd to illustrate the idea, let us go back to some of the examples we have introduced in our first lecture. For the credit card example, we might want to estimate the probability that the particular transaction is a fraud, given some information about this transaction-- for example, the distance between the place where the sale took place and the residence of the credit card owner; or for example, the likelihood that the claim submitted to the insurance company is a fraud, given the number of particular procedures conducted by the medical provider; or the likelihood that the patient will develop a serious heart condition given his weight and average blood pressure; or finally, going back to the beginning of this class, the likelihood that all five o-rings fail, given the current temperature. This last example will be the subject of a special case discussion to be done after we introduce the basics in this lecture.\n\nThe method of logistic regression allows us to answer these types of questions by building the model of the following form. Let us discuss the details of this model. Here, the symbol e stands for the hopefully familiar Euler's Constant, which roughly equals x corresponds to an independent or explanatory variable, such as for example the distance between the sale and home location for the credit card example, or the weight of the patient for our second example. Letter e corresponds to the event of the interest, such as transaction is a fraud, or that a given patient develops diabetes. The expression, which we read as the probability of the event e condition on the value x, is the quantity of interest.\n\nThis is what we want to estimate. Finally, the coefficients beta 0 and beta are called the regression coefficients, and are to be estimated from the data. For now, don't worry about how this estimation is done. We will explain it later. Consider our diabetes example. Suppose based on the past medical data, the coefficients beta 0 and beta are estimated to be as follows. Then according to this model, a patient with a body weight of say 135 pounds-- which is, by the way, my weight-- has a chance of developing diabetes equal to namely approximately 13%. Incidentally, approximately 9% of the US population are suffering from some form of diabetes. So actually this number is not too unreasonable. On the other hand, for a significantly obese patient with weight, say, 275 pounds, the corresponding likelihood is estimated to be as follows-- namely approximately 23%. According to this model, obesity is a significant risk factor for diabetes. One has to be extremely careful in interpreting this model. A more accurate way to express this conclusion of the model would be to say that within a group of people with otherwise similar characteristics, a patient with body weight 275 pounds has roughly 23% chances of having diabetes.\n\nThis is very similar to the care needed in discussion of conclusions of the usual linear regression model. In fact, as you probably see, the logistic regression has also strong similarity with linear regression model. Revealing a linear regression model is probably a good idea before continuing. But strictly speaking, it is not necessary at this point. Let us go back to the expression here. Since the numerator is smaller than the denominator, the ratio takes [? squarely ?] between 0 and 1. And therefore, it is a suitable expression for a probability value. The function of the form that you see here is called logistic function. And this is where the logistic regression derives its name. In this example, we have used only one independent variable-- body weight-- in to estimate the probability of developing diabetes. Naturally, some other factors might play a role here. For example, genetics. Consider, for example, the following logistic regression model. In this example, we have two independent variables-- x, which is before stands for the body weight, and y, which counts the number of parents with diabetes, and which takes value of either 0 or 1 or 2. The coefficients beta w is what we denoted by simply beta in the previous example. Though its actual value might be different.\n\nAnd the coefficient beta g is a new coefficient corresponding to the newly introduced explanatory variable y, corresponding to the genetic predisposition for diabetes. Let us assume that using past data, the coefficients are estimated to be as follows. Then a patient with a weight of 215 pounds and none of the parents suffering from diabetes has likelihood of developing diabetes equal to roughly 18%. At the same time, a patient with the same weight of 215 pounds who has both parents suffering from diabetes, according to this model, has likelihood of roughly 25% for developing diabetes over the lifetime. We see that according to this model, the genetics plays a significant role in the likelihood of developing this sickness.\n\nOf course, this is only likelihood. While we cannot change our genetics-- well, not yet-- we can certainly impact other important risk factors such as weight, diet, lifestyles, to name a few, which are known to play a role in the likelihood of developing a sickness. Again, if one has access to further data, just as in the case of linear regression, additional independent variables can be used for likelihood estimation with better accuracy.",createdAt:lZ,updatedAt:lZ,publishedAt:"2023-03-16T01:25:01.530Z"}},{id:ib,attributes:{title:"Support Vector Machine: Non-Statistical Classifier",slug:"support-vector-machine-non-statistical-classifier",duration:"8:33",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience412SupportVectorMachineNonStatisticalClassifier",objective:"Find out how non-statistical classifiers work.",englishTranscript:"The methods for binary classification that we focused on so far, work in what I'd like to call a statistical setting. Given some input to classify, they postulate some class of probabilistic models, fit various parameters. And using these they give us an estimate of the probability that the input falls into one category or the other. If we think back to the description of binary classification, we were just required to pick one of the two categories. So trying to give a numerical estimate of the probability for each category is actually a little more than we were originally asked for. Of course these problems are very related to each other and it can't hurt to know this extra information. But finding it sometimes comes with a cost. For instance, it may require stronger assumptions about the distribution of the data, like normality or independence. And it may limit your flexibility in choosing your methods, or require you to solve more complicated, computational questions. What I'd like to talk about now are some non statistical methods.\n\nBut just try to answer the original question of choosing one of the two categories. And don't try to fit a statistical model or assigned probabilities. I should mention that sometimes these procedures can be turned into statistical ones. But it's not always obvious how to do so, and it often involves making further assumptions. Perhaps most importantly, the non statistical perspective suggests methods that we might not come up with from a statistical point of view. And it's led to some techniques that have been extremely effective in practice. Before launching into the methods, it's helpful to spend a little time carefully defining our question. We'll set the problem up in the framework of supervised learning that was discussed earlier in the course. The first step here is to describe our data. To do this, we'll represent each object be classified with a list of numbers that describe it which we'll call a feature vector.\n\nFor instance, if we're trying to classify an image, say trying to determine whether it includes a human face. The feature vector may be red, green, and blue values for each pixels. If we're trying to determine if the patient is having a heart attack, the vector may comprise their blood pressure, their heart rate. A zero or one indicating if they're conscious, and a number between one and ten indicating their self assessed level of chest pain. Now there's a lot of discretion in choosing this representation. I could give you many different ways of describing an image, or many sets of numbers that would describe a patient. And we'll see that actually how we choose this representation is crucially important in determining the quality of our classifier. It's easy to see intuitively why a good representation should be important. Imagine that you're trying to decide, without a computer, if a picture contained a face. Now suppose that instead of giving you the picture in its original form, I gave it to you with the pixels reshuffled in some arbitrary order. If I told you of the order that the pixels were in, this would contain the same information as the original image.\n\nBut you would obviously find the problem a lot harder. For now, you should think of the good representation as being one where the number has captured the important information in the problem. Encode it in a nice way, and don't include too much extraneous junk. Once we choose how to represent objects with feature vectors, we can think of the objects to be classified as points in some possibly high dimensional space. The classifier is then just a division of the points in the space into two pieces according to the category it assigns them. We can think of this as a function that takes points as input and produces a category label as its output. In supervised learning, the process is that we are given some training data which comes in the form of a collection of points that are correctly labeled as positive or negative. Using these, we try to construct a good classification function. We are then given new points, and we classified them by applying our function. The question then comes down to how we choose this function, which is what distinguishes the different techniques for classification. There's usually an important trade-off involved here. If the class of functions we look at is too simple, we won't be able to find the function in this class that does a good job describing the right way to map feature vectors to category labels. If we look at classes of functions that are too complicated we'll need immense amounts of data and possibly computation to figure out which one to use.\n\nThe first method we'll look at is called support vector machines, or SVMs. The category of functions they use will seem very simple, perhaps even too simple. But they're often very effective maps. Moreover, we'll see that there is a beautiful and simple technique called the Kernel Trick. That lets them handle a much more complex classes of functions with very little additional work. In addition, they'll be the fundamental building block out of which we'll construct deep networks, which for many problems give the president state of the art. To specify SVMs we'll need to specify two things. The collection of allowable classification functions that we could end up producing. And how we choose which one to use based on the training data. The classification functions we produced will be what we call linear classifiers. They'll work by applying a linear function and thresholding the result. More precisely, we will specify a classifier by giving a vector w of weights, w1through wn, where n is the number of coordinates in the future vectors. And a threshold b, given a data point x, with coordinates x1 through xm.\n\nThey first compute the dot product, w.x. Which you'll recall means that we add up w1 times x1, plus w2 times x2, etc., up to wn times xm. They'll then compare the resulting number to the threshold b. If it's larger than b, they'll put the point in the positive category, otherwise they'll put it in the negative category. This has a nice geometric interpretation. If say the feature vectors are two dimensional, instead of points with w.x exactly equal to b, it's just the line in the plane. The points with positive dot product lie on one side and the points with negative dot product lie on the other. The classifier is thus just cutting the plane into two pieces with a straight line. In three dimensions, the set of points with w.x equal to b is a two-dimensional plane, and ou classifier uses it to divide the three-dimensional space into two regions. When the dimension is larger, it's a little harder to visualize, but the picture is analogous. The points with w.x = b form an n minus 1 dimensional hyperplane which divides un-dimensional space into two regions.\n\nThe classifier calls something in one region positive, and something in the other region negative. Now, let's see how we should choose such a hyperplane given a set of label points of training data. It's not always possible to separate the positive and negative points for the hyperplane. For instance, we can clearly see that there's no way to do it with the points showing here. However, let's suppose for now that we're in the good case where we can find such a hyperplane that perfectly separates the positive and negative training points. In this case, we will call the points linearly separable, and it means that there is some linear classifier that correctly classifies all of the training data. If you look at the picture, you'll see that there could be many such hyperplanes. While they all give the same answers on the training data, it would correspond to very different rules for classifying new objects. For instance, the two hyperplanes shown here both match the training data exactly. However, they would give very different answers when asked to classify any of the points shown here.\n\nTo choose among them, SVMs look for what's known as the hyperplane of maximum margin. Given linearly separable training data, we can see how far we can move this hyperplane until it stops classifying the data correctly. In the picture, this means that we'll shift in one direction until it hits something in the set of positive points. And then we'll shift it in the other direction until it hits something in the set of negative points. This gives us two new hyperplanes, both parallel to our original one. And they both correctly classify our training data. Intuitively, when these hyperplanes are very far apart, this means that the original hyperplane very robustly divides the two groups in the training data.\n\nBecause it worked with room to spare so to speak, this means that it's reasonable to guess that if we had some new training data the hyperplane would probably still work fairly well. We'll call the distance between these two parallel hyperplanes the margin. Now support vector machines work by looking at all the hyperplanes that divide the two groups in the training data. And then they try to find the one with the largest possible margin. If we imagine shifting and rotating the hyperplane around to see what possibilities are, you'll see that it can hit the points that lie nearest to it, shown here in red. And that these are the ones that determine everything we need to know. Whereas the ones that are farther away, shown in blue, don't really matter. And moving them around a bit doesn't change the answer.\n\nThe red points are called support vectors, which is why we call these support vector machines. Here are a few examples of the maximum margin hyperplanes for different training sets. You can see that they nicely match our intuition about the right way to divide the two classes. Going forward, there are two very natural questions that we'll need to answer. The first is whether and how we can find this optimal hyperplane? And the second is, what we should do when the points are not linearly separable? Answering these will be the primary goal of our next two sections.",createdAt:l_,updatedAt:l_,publishedAt:"2023-03-16T01:25:00.147Z"}},{id:jN,attributes:{title:"Perceptron",slug:"perceptron",duration:"10:27",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience413Perceptron",objective:"Learn about a simple classifier with an elegant interpretation.",englishTranscript:"In the previous section, we looked at how to frame binary classification problem in the setting of supervised learning. Let's briefly review where we left off, and define some notations to use going forward. We described each object to be classified as a list of d real numbers, which we call the feature vector. We can think of these as points in d-dimensional space, which we'll call Rd. A binary classifier is then a function that maps each point in Rd to one of the two possible categories. If we label the positive examples with plus 1 and the negative examples with minus 1, we can thus succinctly describe the binary classifier as a function from Rd to the set containing plus and minus 1. We're then given a bunch of training examples, which consist of points x1 through xn, in Rd, along with our correct labels, l1 through ln.\n\nWe use these to decide what function f from Rd to plus or minus 1 to use as our classifier. Then, given a new point to classify, we get our answer by just applying the function f. Deciding what class of functions to choose the classifier from is one of the crucial design choices to be made when setting up our classifier. And we decided to start with the case of linear classifiers, which choose a category by dividing space up into two categories using a hyperplane. We can describe the hyperplane as the set of points with w .x equals b. Where w equals W1 through Wd is a vector in Rd and B as a number. If we use plus or minus 1 to label our categories, we can write or classify very cleanly. We want to return plus 1 is w .x is bigger than b, and minus 1 if it's smaller. So, this just says that f of x is equal to the sign of w .x minus b. We then looked at the case where there was a hyperplane that correctly classifies all of our training data. In which case, we said that the points were linearly separable.\n\nWe define the margin of such a hyperplane to be the amount we can shift in either direction, while still correctly classifying all the training data. Given linearly separable training data, there'll often will be many hyperplanes that separate the positive and negative examples, so we have some freedom in how to construct our linear classifiers. One role people often use to make this decision is to choose a hyperplane with the largest possible margin. The resulting linear classifiers called the Support Vector Machine. We thus gave a reasonable proposal for how to perform binary classification when the training examples are linear acceptable. But it's only useful if we can actually implement it reasonably efficiently. There are actually some problems that don't look too different from this. But for which the running time grows exponentially in the number of dimensions, which would be prohibitive, even for fairly moderate D.\n\nSo, it's not obvious apreari that we can do this. However, luckily, we can. Let's see how. Our classifier is specified by the vector w and the threshold b. For it to correctly classify the ith training example, we need w .x of i minus b to be greater than 0, if l sub i equals 1, and less than 0 if l sub i equals minus 1. Keeping the vector b around ends up being a little bit annoying. It'd be cleaner if we were in the case where b equals 0. It turns out that there's a nice trick that lets us reduce to this case. And the idea is just that we're going to add the dummy coordinate and set it equal to 1 for all of our training points. So, we now think of our training points as vectors, x sub 1 prime through x sub n prime, where x sub i is the d plus 1 dimensional vector X sub i comma one. We then pull b into our weight vector by defining w prime to be the vector w comma negative b. This makes it so that w prime dot x sub i prime equals w dot x minus b.\n\nSo, finding a vector w such that w dot x sub i minus b is greater than or less than 0 is the same as finding a vector w minus prime, such that w minus prime .x sub i minus prime is greater than, or less than 0. And now we don't have to worry about b. We can thus assume from now on that b is 0 without loss of generality. With this pre-processing step, the algorithmic problem of finding a classifier that works for all of the training data comes down to finding a vector w that meets all the constraints coming from the x of i. That is we want to find a vector w, such that w .x of i is greater than zero, for all i such that l sub i equals one.\n\nAnd w .x sub i is less than 0 for all i such that l sub i equals negative 1. Let s be the set of w that meet these constraints. To find the support vector machine, we wanna find the w in s with maximum margin. If we start with the hyper plane w .x equals 0, we can get shifted versions of it by just changing the right hand side. How quickly the hyperplane moves when we change the right hand side depends on the norm of w. And one can check if the hyper plane w dot x equals delta, corresponds to shifting the original one by a distance of delta over the norm of w. If we shift the hyperplane so that it hits a point xi, which corresponds to the margin of the hyperplane with respect to the point xi, this will be the hyperplane with delta equal to w .xi. Which means that we shifted it by W. Xi over the norm of W.\n\nBy scaling these constraints, we can write the problem of finding the hyperplane of maximum margin as just minimizing the norm of W. Now, there are a few ways one can find the W we're looking for. In other sections of this course, both before and after this one, an important theme has been that we have these powerful algorithmic techniques available for solving convex optimization problems. In practice, the most common approach for finding linear classifiers in general, and FPNs in particular, is to use convex optimization. The basic idea is that one can rewrite the problem of finding the hyperplane of maximum margin as optimizing a convex function over the set S. And we can then apply the convex optimization machinery. This machinery has been carefully tuned for SVMs, and there are excellent libraries available for doing this. However, rather than just running through some algebra to write the problem in a convex form, and then referencing the existing general algorithms for these problems, I'd like to instead talk about a beautifully simple way of solving the problem, that I think actually might be a bit more insightful. It's called the perceptron algorithm.\n\nAnd it's not quite as efficient as the convex optimization routines, and I'm just gonna talk about it for finding any linear classifier, rather than to find the one with with the largest margin. But it has some beautiful ideas in it. And it will let me give you a full, self contained algorithm. Moreover, it solves a more general version of the problem called online learning. Before we talk about the algorithm, there's one final modification that it will be convenient to make. Since we've gotten rid of B, all of our constraints look at whether w .x sub i is positive or negative. Note that scaling Xi by a positive content won't change that. For instance, saying that W .Xi is positive is the same as saying that W.5xi is positive. We can best scale all of the Xi however we want.\n\nBy doing this, we can assume that each Xi has norm, which is just another word for length equal to one. A basic property of dot products says that the square of the norm of a vector is given by the dot product of the vector with itself. So, we can write this as norm of x squared equals xi .xi equals 1. We can now finally describe the output. The idea is as follows. Instead of being given all of the training data, we'll imagine that somebody hands us the points from it one at a time. Possibly giving us the same point more than once. Each time we're given a point, we have to assign it to a category. After we do, we find out what the actual label was, and we can use this information for the later points.\n\nNow, the difference here is that we have to make our decisions online. That is, we don't get to see the whole training stat before we have to classify some points. Our algorithm for doing this is going to be very simple. We'll maintain a vector w that will be our guess for the weights, which is initially set to zero. Given a point, X, to classify, we'll then use your current guess for W. So, we'll guess positive if W .X is greater than 0, and negative otherwise. Now, if we guessed right, we'll leave our guess for W unchanged. However, if you make a mistake, we'll update our guess to try to fix it. Specifically, if we guess negative, and the answer was positive, we'll replace w with w plus x. This increases w .x, so it pushes w in the direction of answering positive for x. Similarly, if we guess positive when we should have said negative, we'll replace w with w minus x. Now, if we wanna solve the original problem, we can just pick our points from the training set, say randomly, and feed them to the algorithm and repeat. Thats actually the whole algorithm, which Ive written a bit more formally here. We can actually show a remarkably strong guarantee about this algorithm. It will say that if there is any linear classifier that correctly classifies all of the points that were given with some margin gamma, then the total number of mistakes we'll make over the entire series of points will just be one over gamma squared.\n\nTo give you a feel for it, here's an animation of the algorithm being run on some two dimensional data. Here are the data points, and here is the maximum margin hyper blade, which we notice does not pass through the origin. We start by adding a dummy coordinate to let us work with hyper planes through the origin, as shown here. And by initializing w1 to be 0, we'll choose each x sub t randomly from the data points, allowing reads. Now, one mild technicality is that w1 is 0. So, for the first point, the dot product, w sub 1.x sub 1 is actually 0. And we need to decide whether to call that. Positive or negative. Which one we choose doesn't really matter, so we'll just pick one. Say, negative, if the dot product is exactly zero.\n\nNow, in this case, the correct label for x1 was positive, so we made a mistake. And we update our weights by setting w2 equal to 0 plus x1. We now have a nonzero weight vector, so we can think of it as specifying a hyperplane, which we then use to classify the points. We'll then repeatedly compute W sub .Xt, use that to see which side of the hyperplane the point lies on, and then use this to compute the sign, which we'll use to compute a prediction for l sub t. We'll then use this to update our weight vector accordingly, and obtain W sub t plus 1. The rest of this section is an animation of the perception algorithm run on our data. It steps through the first ten iterations slowly, so you can see exactly what's happened. And then it quickly steps through the next hundred, so that you can get a feel for the overall behavior of the algorithm. For the first ten iterations, you may find it helpful to pause the video on each and make sure you understand how I got from one step to the next.",createdAt:l$,updatedAt:l$,publishedAt:"2023-03-16T01:24:58.846Z"}},{id:jL,attributes:{title:"Perceptron Proof",slug:"perceptron-proof",duration:"8:10",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience414PerceptronProof",objective:"Learn about the proof behind our claim in the previous video",englishTranscript:"In this section, we'll prove our previous claim that the perceptron algorithm makes at most one over gamma squared total mistakes, where gamma is the margin. I should mention that this section may be slightly more mathematically involved than the others. I think it provides a lot of insight into why the perceptron algorithm works, so I decided to include it. But nothing we do later will rely on the details of this proof. So if you skip this or you don't fully follow the details, it won't cause any problems with the later material. Okay, so let's now see the proof. Our starting assumption is that there's some linear classifier that classifies all of the x sub i correctly with margin gamma. With some gamma greater than zero. We can describe this classifier by giving its weight vector w star, which we'll normalize to have norm one. Because of this normalization, the distance of a point x sub i from the hyperplane is just given by the absolute value of w star dot x sub i. The margin assumption says that w star dot x sub i is greater than or equal to gamma whenever l sub i is one and w star dot x sub i is less than or equal to negative gamma whenever l sub i is negative one.\n\nNow, let w sub tb the way vector after the algorithm has gone on for t iterations. The idea of the proof is that we're gonna keep track of the dot product w sub t dot w star which corresponds geometrically to the component of w sub t in the direction of w star. If w sub t perp is the component of w sub t that is perpendicular to w star, as shown here, then the length of w is given by the Pythagorean theorem. Which tells us that the norm of w sub t squared equals the sums of the squares of these two components. So the norm of w sub t squared = (w sub t dot w star) squared + norm w sub t perp squared. Since the norm of w sub t perp squared is always at least 0, this tells us in particular that the norm of w sub t squared is always at least w sub t dot w star squared. So the norm of w sub t is always at least w sub t dot w star. Geometrically, this is just saying that the length of the whole vector, w sub t is at least the length of the component in the direction of w star. Now the idea behind our analysis is to show that w sub t dot w star will go up a lot every time we make a mistake. If we then show that the total length of w sub t doesn't go up too much overall this will mean that we couldn't have made too many mistakes. Let's make this precise. In step t we see the point x sub t and predict the label by looking at the sign of w sub t dot x sub t. If our prediction is correct, we don't change our weight vector.\n\nSo, w sub t plus one equals w sub t. And the dot product of our weight vector with w star doesn't change. The interesting case is when we make a mistake. Our first claim is that this will substantially increase the dot product with w star by making w sub t plus 1 dot w star greater than or equal to w sub t plus gamma. To show this, let's first suppose that we have a false negative in step t. This means that we predict negative, but l sub t equals positive 1. So we then update our weights by setting w sub t plus one to equal w sub t plus x sub t. So w sub t+1. w star equals w sub t. w star which equals, w sub t. w star + x sub t w star. Our margin assumption has that x of i. w star is greater or equal to gamma. Whenever l sub i is 1. So the right had side is at least w sub t dot w star plus gamma.\n\nGeometrically, this says that the assumption that w star correctly classifies the point of margin gamma, means that x sub t has to have a component of at least gamma in the direction of w star. So adding x sub t increases the component in the w star direction by at least gamma. The case of a false positive is similar, just with some signs flipped. A false positive means that we predict positive when l sub i = -1. And we update our weights by setting w sub t +1 = w sub t- x sub t. So we get that w sub t + 1 dot w-star = (w sub t- x sub t) dot w-star. Our margin assumption says that x sub i dot w-star is less than or equal to negative gamma whenever l sub i = -1. So we get that the right side is at least w sub t, dot w star, minus negative gamma. Which again equals w sub t plus gamma. So in both cases, we got that the dot product, increases by at least gamma, as we claimed.\n\nNow our second claim is that the total length of the weight vector, doesn't increase too much overall. Again, nothing changes when we classify the point correctly, so we really only have to worry about the case where we make a mistake. In this case, we'll show that when we make a mistake, the squared length of the wave factor never increases by more than 1. That is, that the norm squared of w sub t plus 1 is less than or equal to the norm squared of w sub t plus 1. Showing this just takes a few lines of algebra. Let's first do the case of a false negative. In this case, we classify x sub t as negative, so w sub t is less than or equal to 0. And we set w sub t +1 to equal w sub t +x sub t. The norm squared of the vector is just the dot product of the vector with itself.\n\nSo the norm of w sub t +1 squared = w sub t+1 dot w sub t + 1. Which we can expand out as w sub t dot w sub t + x sub t dot x sub t + 2w sub t dot x sub t. The first term is just the norm squared of w sub t and the second is just the norm squared of x sub t. So this equals the norm squared of w sub t + the norm squared of x sub t + the last term, 2 w sub t dot x sub t. Now, remember that we normalized x sub t to have a norm 1. So the norm of x sub t squared just equals 1. And w sub t dot x sub t is negative. So we gather the norm of w sub t plus 1 squared equals the norm of w sub t squared plus 1 plus something that's less than or equal to 0. So it's at most the norm of w sub t squared plus 1 as we've claimed.\n\nWe can interpret this geometrically as well. The fact that we classify x sub t as negative means that the dot product between x sub t and w sub t is less than or equal to 0. Which means that the angle between these two vectors is at least 90 degrees. And x sub t has length 1. If the dot product is 0, the angle's exactly 90 degree. And the norm of w sub t + x sub t squared is exactly equal to the norm of w sub t squared + 1, by the Pythagorean theorem. And this is the worst case, since the vector only gets shorter if the angle is more than 90 degrees. The case of a false positive is again the same, with some signs flipped. This time, w sub t dot x sub t is positive, and the norm of w sub t + 1 squared = (w sub t- x sub t) dotted with itself. Which expands out to the norm of w sub t squared + 1- 2 times the positive number w sub t dot x sub t. Which again, is at most the norm of w sub t squared +1 as we want it.\n\nNow, we can put these two claims together to bound the total number of mistakes made by the algorithm. Suppose that we run our algorithm for say m steps and we make a total of k errors along the way. At the beginning of the algorithm, we start with a weight of 0. So the initial dot product, w sub 1 dot w star equals 0. Claim one tells us that the dot product increases by at least gamma every time we make a mistake. So if we make k mistakes, we know that the product at the end, w sub m dot w star, is at least k times gamma. On the other hand, claim two tells us the norm of w sub m squared. Goes up by, at most, 1, each time you make a mistake. So the norm of w sub m squared is, at most, k. And therefore, the norm of w sub m, is, at most, the square root of k. Now, as we said earlier, since w star has norm 1, the dot product, w sub m.w star is the length of w sub n's component in the direction of w star.\n\nAnd this can't be bigger than the length of the whole vector w sub m. So, w sub m dot w star is less than or equal to the norm of w sub m. Putting these all together, the yet that k gamma is less than or equal to w sub m dot w star which is less than or equal to the norm of w sub m which is less than or equal to the square root of k. So k gamma is less than or equal to the square root of k, which means that k is at most 1\u002Fgamma squared, which is what we originally claimed.",createdAt:ma,updatedAt:ma,publishedAt:"2023-03-16T01:24:57.273Z"}},{id:1208,attributes:{title:"Perceptron & Data That Is Not Linearly Separable",slug:"perceptron-and-data-that-is-not-linearly-separable",duration:"10:16",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience415PerceptronDataThatIsNotLinearlySeparable",objective:"Apply some simple modifications to allow SVMs to be used in more complicated settings",englishTranscript:"We've seen that we can use support vector machines for binary classification, as long as our training data is linearly separable. However, as you might guess, not all data sets are linearly separable. And we'd really substantially limit the applicability of our techniques if these were the only classification problems that we could handle. For instance, we could get data from some experiments and these experiments maybe should lead to nice linearly separable training data. But possibly they have experimental errors that lead to a few errant data points in our training data. Or we could be in a setting where there's a good classifier but is just isn't the hyperplane. In this section, we'll give a brief overview of some of the simple modifications that allow us to apply support vector machines in these more complicated settings.\n\nIn the first case, where we want a linear classifier, but there's no hyperplane that perfectly classifies the training data, the idea is just to slightly modify our goal. Instead of requiring our hyperplane to cross by all of our points in some given margin, we'll make a penalty function that zero for a point that is classified by the hyperplane with this margin and that penalizes points that violate the constraint. And we'll make the size of the penalty depend on how bad the violation is. Then given some margin, we can look for the hyperplane that has the smallest penalty. Note that there's a tradeoff here. If we demand a larger margin, we should expect more violations and thus a larger penalty so we have to somehow decide how to balance the two. There's also some discretion in how we choose our penalty function.\n\nResearchers have studied how to make these choices, and there's some simple, commonly used penalty functions that work very well in practice. And in fact, for these penalty functions, it turns out we can actually find the optimal hyperplane using algorithms that are almost the same as the ones we used in maximum margin setup. Let's now talk about the second problem I mentioned which actually seems to be a bit more problematic. This is the problem where there is a right classifier, but it's just fundamentally not a hyperplane. For instance, it could be that the best way to classify the data is by looking at some higher degree polynomial instead of a linear function. To see how this could happen, let's look more carefully at the example I showed before. In this case, any linear separator that we try will miss-classify a significant faction of the trait data. Here's a picture of what this would look like.\n\nHowever, we would correctly classify all the training data and get what visually seems to be the right classifier by looking at the degree two polynomial x squared + y squared, labeling a point as positive when it's bigger than one and negative when it's smaller than one. Unfortunately, our whole approach was tuned into finding a hyper plan so seems like we need to start from the scratch and try with all of the theory and all the algorithm that we have. Before completely anew if we wanna get to classifier that has some other shape. Like the ones we got to be higher. Our priority we don't even know if we can do this on a reasonable way. Our algorithms, like the perceptron algorithm we described, or the convex programming methods that I mentioned, were all really about linear separators.\n\nPriori, it actually seems quite possible that finding the best classifiers with say degree two polynomials could just be fundamentally harder, and end up being too hard a problem, one that we can't solve with any efficient algorithm. However, as you may have guessed, this isn't the case. There is a beautiful trick that actually lets up use what we know about support machines directly to find a whole bunch of different types of non linear classifiers without having to start from scratch. The basic idea is really simple. Remember that when we set up the problem, we have to choose how to encode our data as feature vectors. But this was a design decision, and there are a lot fo ways we could've made this choice. For instance, instead of making the coordinates of the future vector be x and y, I could have taken them to be out of 100x squared and 74cos y, or x + y and xy. Or I could have even put them in three dimensions, with coordinates say xy and x cubed times y. In general, we can apply any function that we want to our data points, and use the results as our feature vector instead. Now, how we make this choice has a huge effect on what a linear classifier looks like, what its margin is, and how well it performs and different choices can actually lead to very different answers. In fact, we already took advantage of this freedom a little bit when we were describing the perceptron algorithm.\n\nRight at the beginning of the algorithm we decided that it was easier to only work with hyperplanes through the origin. The only way were able to get away with this was by adjoining a dummy coordinate of one. That is by applying a function that takes a vector x to a new vector x,one in a space that's one dimension larger. This then broadens the types of classifiers we could get by taking hyperplanes through the origin. Any classifier we could have gotten in the original setup by using some arbitrary hyperplane that might not have gone through the origin, can now be expressed as a hyperplane through the origin in this higher dimensional space. One thing to note here is that by lifting the points to a higher dimensional space, we actually gave ourselves the ability to describe more classifiers as hyperplanes through the origin. Intuitively, we can describe a hyper plane through the origin in d dimensions, by giving a weight vector with d coordinates.\n\nBy throwing in a dummy coordinate, we put our vectors into d plus one dimensions. So we now describe a hyper plane with d + 1 numbers. So, by lifting our points up into a space, with one more dimension, we gave ourselves one more degree of freedom, to use in describing our hyper plane. But that just seems like a little technical convenience to avoid working with numbers other than 0 on the right hand side of our equations. The idea of the kernel trick is to really exploit this freedom. If we wanna look for classifiers whose values are delineated by something other than a hyperplane, we'll apply a function f that maps our original points to some new points, so that these more complicated classifiers on original points are just linear classifiers applied to the new one. To see how this could work, lets go back to the example we had before. If we had somehow encoded our points differently, say like this.\n\nThen we could have gotten a much better result with a linear classifier, as shown. Now this is pretty adhoc, but there's actually a more systematic way that we can proceed. Instead of describing a point with two numbers, X and Y Let's describe it as six numbers. One, X, Y, X squared, Y squared, and XY. Formally, we can think of this as a function that takes X, Y to a new vector with six coordinates. Let's call them Z one through Z six. By applying the function f written here. This didn't really add any information. We're still just giving a more complicated way of encoding the original numbers X and Y. But now we hae six degrees of freedom for our hyper planes through the origin which we can describe by saying the linear function c1z1 plus c2z2 up to c6z6 equal 0. But if we applied this linear function to f of xy for one of our original points we get c1 plus c2x plus c3y plus c4x2 plus c5y2 plus c6xy. But this is the general form of a polynomial of degree at most two applied to x comma y. So, by choosing c one through c six, we can describe and quadratic polynomial. And the linear classifiers applied to z give quadratic classifiers applied to x comma y In particular if we let c1 = c4 = c6 = 1 and then set the rest to zero, we get the classifier that returns positives if 1+z4+z5 which equals 1+x squared+ y squared is positive and then negative otherwise, which was exactly the one that looked like the right choice for our sample.\n\nThis is actually kind of remarkable if we want quadratic classifiers, we don't need to start over. We can get them by just applying a function that lift our points up into some higher dimensional space, and then look for linear classifiers. We actually could've done this more generally. If we wanted degree functions, we just need to include the degree three. X cubed, XY squared, X squared Y, and Y cubed. In general, if we want degree K classifiers, we just need to apply the function that writes a point by listing all the monomials of degree at most K. We can then just apply our SBM algorithm to the results. So we don't need to do any additional thinking grees. All we have to do is just apply a function and use the SVM machinery that we already have. And there was nothing that forced us to use polynomials.\n\nIf we wanted to describe some other sort of classifiers, we could have just applied some other function to our training data instead. Now you might worry that the computational problems is getting a lot harder, since were getting a lot more coordinates.For example they are on the order of d to the k mono nomials of degree k in d variables, so this gets big in a hurry. However, it turns out that we don't always need to explicitly work with these really high conventional representations. If we look carefully at the perceptron algorithm or at a lot of the other algorithms we're finding FEM's. It turned out that we can actually describe in a way that doesn't specifically need to know the coordinates of the data themselves. Instead it just needs to be able to compute dot products, f(xi) and f(xj). So if we want to find linear separators after applying the function f, we just need to be able to compute dot products of the forum f of x sub i dot f of x sub j. Often, there's a way to do this that doesn't require us to actually compute all the coordinates. In particular, for the function f that takes a point to all monomials of degree up to k If we scale the coordinates by some appropriate constants, the dot product of f(x sub i) and f(x sub j) turns out to just be given by (1 + x dot y) raised to the dth power.\n\nTo find this we just need to compute the dot product x dot y. We never actually need to write out this long list of variables. This is obviously a lot faster, and it lets us handle cases that would be way too complicated otherwise. So, to abstract a little and sum everything up, the idea was that we would get more complicated classifiers by applying a function F to our feature vectors, and then using SVMS on the results. Instead of computing F of x for all points, we actually just need to compute the dot products, K of x of i comma x of j, which we'll say equals F of x of i dot and F of x of J. And we call this the kernel trick. This kernel is all we actually need to be able to run our algorithms.\n\nThis idea is called the kernel trick and it lets us apply our SVM machinery to get non linear classifiers. This is an extremely powerful tool. But I just like to conclude with one. One quick caveat, remember that when you have more parameters, you need more data to properly fit them, otherwise there's a risk of so called overfitting where you get a really complicated function to describe your data, but this function is fairly useless on new data points as shown here. When using these kernels, it actually gets easier to do this since we're introducing a lot more possibilities for our classifiers. One thus has to use some care in applying them and they have to be used judiciously to avoid overfitting. There's a beautiful theory that studies the tradeoffs in doing this. But it's slightly beyond the scope of this course. In general, you should think of the goal as trying to work with as few degrees of freedom as you can, while still being able to describe a classifier.",createdAt:mb,updatedAt:mb,publishedAt:"2023-03-16T01:24:55.148Z"}},{id:1207,attributes:{title:"Estimating the Parameters of Logistic Regression",slug:"estimating-the-parameters-of-logistic-regression",duration:"7:49",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience416EstimatingtheParametersofLogisticRegression",objective:"Find out how you can estimate the parameters of logistic regression.",englishTranscript:"It is now time to discuss methods for estimating the parameters of logistic regression. Namely the parameters such as beta 0, beta, beta w, and beta g. As we noted earlier, these parameters are estimated using historical data. In particular, the data will be used in combination with a so called maximum likelihood estimation. Let us first describe the general principle in abstract terms and then illustrate it with a simple example. Suppose, we have a collection of the data in the following form. Here X1, X2 and all the way to Xn are observations for independent variables. And Y1, Y2 and all the way to Yn indicate whether the event of interest takes place. In particular, Ys only take values 1 and 0. Here, the convention is that Yi takes value 1, if the event took place for the ith element of the sample, and takes value 0, otherwise. Now, remember, that according to the logistic regression model with one independent variable, we have the following relationship between the observations and data. Say, for example, that Y1 equals 1, then we have the following relationship. On the other hand, say, for example, that Y3 equals 0. Then according to our model, we have the following. So, that we don't have to indicate what case we are dealing with, observe that we can combine both cases as follows.\n\nPlease take some time to understand the meaning of this expression and convince yourself that the trick works regardless of whether the value of Y equals 1 or equals 0. One crucial assumption is that the observations are independent, just like it is a common assumption in the linear regression case. This is often the case in practice. Say, once again, X corresponds to individual's weight and Y indicates whether the patient has diabetes. It stands to reason that these observations are independent. If this is the case, the probability of observing a particular outcome for all individuals from 1 to n simultaneously is the product of probabilities for individual outcomes. And can be represented by the following expression. Now recall that the values of Xs and Ys are actually known to us. These are what we have collected in our data sample. The only unknowns are beta 0 and beta coefficients, which we want to estimate.\n\nThe maximum likelihood estimation is a principle which suggests that the best estimation of beta0 and beta are those which makes the expression on the right hand side of the equation as large as possible. In words, one could say, the best estimate for beta 0 and beta are those which make the likelihood of this particular outcome as large as possible. If this was too abstract, no worries. Let us turn to a concrete example to illustrate this idea. Imagine, that we have the following data on some individuals. Their body weight and whether or not they have developed diabetes. For simplicity, we will consider the case of a small data size, consisting of only ten data points. But naturally, in practice, you would want to use methods with a larger number of data samples for better accuracy. The left column simply gives the list of body weights for the ten people. The right column indicates whether the corresponding individual has diabetes. The value 1 stands for the case when the answer is yes, does have diabetes. And the value 0 stands for the case when the answer is no, does not have diabetes.\n\nFor example, the individual with weight 186 pounds corresponding to the first row in the table does not have diabetes. And the individual with a weight 156 pounds corresponding to the third row of the table does have diabetes. Then according to our formula, we need to find beta 0 and beta, which maximize the following expression. Take some time to study this expression, and convince yourself that we have applied the formula correctly. Now ignoring the term where we multiply by 0s, we can simplify the formula as follows. So, we need to find beta 0 and beta which make this expression as large as possible. How can we do this? Well, one straightforward approach is to try all possible values of beta 0 and beta within some range, by say, iterating over small increments.\n\nFor example, try all beta 0 and beta between -10 and 10 with increments say 0.01 and select the pair which gives the largest value. This method however, has several important limitations. Number one, we cannot be sure that our range between -10 and 10 includes optimal values. Number two, our increments can be too crude. Select and say, increment of 0.01 for beta 0, might be too refined and at the same time too crude for the second coefficient beta. Finally, when we have more than one independent variable, recall that in our second example, we had two independent variables, body weight and the number of parents with diabetes. The time to try all values within their fixed range might be too large for practical purposes. So, how do we find the best values? This brings us to the third and last part of this lecture. Algorithms for estimating regression coefficients.\n\nThere is a short answer to the task of finding the best regression coefficients, beta 0 and beta. Simply use one of the standard statistical packages. Most of them have the capability to do so. In fact, I would recommend to use one of them until you become a sophisticated practitioner of statistics. But we would like to leave the lead, so to speak, and share with you the main ideas for the method. This method will be discussed in some of the later lectures, so it is worth discussing it now as well. The secret is that the expression we'll need to maximize in terms of beta 0 and beta is actually very nice if we instead take the logarithm of this expression, and as a result, get the following formula. It turns out that this new function of beta 0 and beta, even though it involves a complicated combination of logarithms and exponents, falls into a very nice class of functions called concave functions. Let us imagine we have the function of just one variable. Now the expression we need to maximize has two variables, namely beta 0 and beta. But let us imagine for now that we're dealing with the function of only one variable. Such a function is called concave, if it has the following shape.\n\nThe definition of a concave function is that whenever we draw a horizontal line between any two points on the curve, the line stays entirely below the curve, like this. Or like this. The concave function is particularly nice when it comes to finding its largest value, namely solving the maximization problem. There is a variety of methods, including the so called Newton's method or the gradient descent method, which find the maximum values for such functions. These methods work by finding larger and larger values of function by small incremental improvements, sort of like what you see on this picture illustrated by small arrows. Until the value of x corresponding to the largest value of the function f of x is found, like this. The crucial concavity property guarantees that these steps will converge to the point which is the maximum point for the function f. There is a very similar nice class of functions called convex functions, which look like this. And which are good for finding a minimum point on the function instead.\n\nLet us imagine, we have access to the statistical software, which applies one of these methods, and indeed finds the values of beta 0 and beta which maximize the expression of interest here. I have done it myself, and found that the best values after some rounding are as follows. To summarize, according to this model, the logistic regression model prediction of the impact of body weight on the likelihood of developing the diabetes is given by the following formula. Naturally, the numerical values for beta 0, and beta, we have derived above, depend on the data we have used for the estimation. When the data changes, the estimation values for the parameters change as well. There are many additional questions worth discussing regarding logistic regression model, such as confidence intervals for the regression coefficient, statistical significance, relevant ranges, dealing with outliers for which we simply do not have time in this course. For now though, let us turn to a very important real life application of the method to the reliability analysis of O-rings for Challenger space shuttle. Was there enough evidence to convince NASA managers that the launch was too risky?",createdAt:mc,updatedAt:mc,publishedAt:"2023-03-16T01:24:53.461Z"}},{id:1206,attributes:{title:"Misapplications of Statistical Techniques",slug:"misapplications-of-statistical-techniques",duration:"6:27",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience417MisapplicationsofStatisticalTechniques",objective:"Learn what not to do with these statistical methods.",englishTranscript:"For some other subtle ways that misapplication of statistical techniques can lead to an incorrect conclusion lets turn to an example in the setting of scientific research. If you read the paper or browse the Internet, there's no shortage of advice, supposedly backed by some scientific study about the health effects of some food or behavior. You could find say, red wine prevents heart disease, or antioxidants make you live longer, cigarettes cause cancer, coffee causes cancer, coffee prevents cancer, or maybe, some obscure berry causes weight loss. Some of these assertions like the link between cigarettes and cancer, stand the test of time, get confirmed by other scientific experiments and become acknowledged as true scientific facts. For others, new studies don't find the same effect or maybe they even find the opposite one. And a literature goes back and forth with seemingly conflictory claims. This isn't just a problem with nutrition literature, you can find similar examples in the literature discussing the best treatment for medical problem, the genetic basis for disease, or the existence of telepathy.\n\nYou can find such conflicting evidence, even in clinical trials published in top scientific journals. Researchers who have studied this phenomenon have found surprisingly high rates of nonreplicability for scientific results. And some have even proposed the possibility that in certain fields, a majority of the published research findings could actually be false. This should be pretty surprising, since these studies are rigorously refereed. And they're usually accompanied by p-values and confidence intervals and tables of data that provide a veneer of the epistemological comfort conveyed by modern science. What I'd like to do now is give a few examples of the commonly made mistakes that can cause these problems. Consider the following scenario. Suppose we get frustrated with all the conflicting claims that we're reading. And we want to decide once and for all which foods cause or prevent heart disease.\n\nTo do this, we collect data on the 100 most common foods that people eat and their incidence of heart disease. Of course, designing this study is complicated. People lie about their eating habits. It's not always clear whether someone does or doesn't have heart disease. People eat foods in different quantities, etc. For simplicity, let's just set this aside and assume that we're able to successfully assign p values to hypotheses of the form, food x causes heart disease. We then take all the foods that have p greater than 95% and we'll report them as causing heart disease with 95% confidence.\n\nNow, here's the problem. Say that something has a p value of 95% means that under the null hypothesis that there's no effect, the chance of the observed outcome is at most 5%. Let's suppose that none of the foods that you test actually have any effect on heart disease. This means that for each of the hundred foods, the null hypothesis that the food does not cause heart disease holds. However, if you get false positives for each with a probability of 5%, you'd actually expect to find an apparent effect for five of the hundred foods. Even worse, if the false positives for different foods are independent, the probability that at least one food appears to have an effect is actually very high. It turns out to be about 99.4%. So even though the 95% constant seem quite good, the chance of you reporting a spurious effect is extremely high.\n\nThe problem here is what you might call hypothesis shopping. Instead of fixing a hypothesis and testing it, we looked at our data and tried to find a hypothesis that would pass the test. This probabilistic phenomenon, where we're simultaneously testing a large number of hypotheses, can occur fairly easily under less contrived conditions. For instance, imagine testing each of five variants of a drug on five different types of cancer and then looking at how the effects breakdown by age, gender and ethnicity. It's easy to find even more extreme examples in scientific settings. For example in genomics, scientists often want to figure out what genes cause the disease. A way to start looking for these is with what's known as a genome-wide association study, where they cross-reference the incidence of the disease with tens of thousands of genes, looking for the handful of genes that cause it. If they call some correlation significant and it clears the 95% confidence bar, you'd expect thousands of false positives, that is genes that exhibit a seemingly statistically significant correlation but don't actually have anything to do with disease.\n\nCompared to just a handful of true positives, corresponding to the actual genes they are looking for. Of course, good researchers carefully correct for this, by for example, requiring much higher probabilities or just using these studies to identify candidates and then carefully testing them by other means. But it's easy to miss this and to make mistakes. Alternatively, suppose you set up an observational experiment where you set something up and just look to see if anything interesting happens. There are a lot of potentially interesting behaviors. So you might actually expect something interesting to happen just by chance.\n\nThis last one actually leads to a very subtle way that these biases can creep into the literature. In general, it's pretty hard to get a good journal to accept a paper that says I did the following experiment and nothing interesting happened. People are much more likely to publish papers that describe some new and exciting phenomena. If one uses 95% as the cut off for something for being significant, you'd get something significant by chance 5% of the time. But people are often much more likely to report their findings when they see something significant. So if people don't properly correct for this, you'd expect a lot more than 5% of the interesting phenomena that are submitted for publication to have occurred by chance. This is particularly acute if you have a bunch of groups of people studying the same phenomenon. For instance, suppose that some food has no effect on whether you get cancer or not in actuality, but people think that there are some reason that it might.\n\nSuppose that this results in 50 groups of people independently performing experiments to test this hypothesis. Of these, you would expect that few groups seeing an effect that clears the 95% confidence bar, maybe two or three. These groups don't know about the others, so they excitedly submit their work for publication. The other 40 something groups don't see anything interesting and many of them don't even bother do publish this sort of finding. This means that all anybody ever sees is that two or three groups independently observe the phenomenon with 95% confidence, which if they were the only ones looking at the question, seems extremely unlikely to happen by chance. This isn't even a mistake on their part, if they don't know about the other groups, they might not have any obvious reason to worry about this sort of problem. One reason that we're mentioning these sorts of issues is that they can become particularly common in the context of so called the big data, if you're not careful. There's really a very thin line between data mining and hypothesis shopping.\n\nAs illustrated by the genome wide associations study example, bigger data sets make it possible to look for increasingly large numbers of patterns. Moreover, the complex machine learning techniques are looking for increasingly complex patterns, which makes it even easier to find something seemingly significant that occurs just by chance. The techniques we've described are powerful, and their math is sound, but it's important to make sure that you're using them correctly. When used correctly, they let people do things that otherwise wouldn't be possible. But if you don't combine them with the appropriate skepticism and common sense, it's easy to make mistakes and reach the wrong conclusions.",createdAt:md,updatedAt:md,publishedAt:"2023-03-16T01:24:52.301Z"}}]},instructors:[{id:jv,name:me,position:a,nationality:a,achievements:a,focus:a,bio:mf,learnMoreURL:a},{id:kV,name:mg,position:a,nationality:a,achievements:a,focus:a,bio:mh,learnMoreURL:a}]}},{id:140,attributes:{title:"Deep Learning",slug:"deep-learning",description:"Deep learning: this recent technique has been the driving force behind the rise of artificial intelligence. Professor Ankur Moitra will demystify this method by describing its underpinnings and limitations.",prerequisite:a,duration:k,objective:a,createdAt:"2023-10-18T21:49:04.611Z",updatedAt:"2023-10-18T21:49:45.520Z",publishedAt:"2023-03-16T13:13:09.778Z",chapters:{data:[{id:1205,attributes:{title:"Introduction to Deep Learning",slug:"introduction-to-deep-learning",duration:iq,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience51IntroductiontoDeepLearning",objective:"Learn about a new approach to classification: deep learning.",englishTranscript:"In this unit, we're gonna study a revolutionary, exciting, new approach to classification, called, deep learning. In contrast to the approaches we've talked about so far, which are, for the most part, well understood, no one really knows when, or why, deep learning works so well. In fact, truth be told, there's a lot of hype around it, and it's sometimes difficult to separate the facts from the fiction. What I hope that you'll get out of this unit, is a better sense of what new applications deep learning enables, what makes it so exciting, but also what are the caveats and dangers that come along with it.\n\nDeep learning is a major driving force behind many of the recent advances in machine learning, and it's arguably the reason why today truly is the golden age for machine learning. But there are also many subtle issues lurking underneath the facade. I think it makes sense to start with the excitement, so why is everyone so excited about deep learning?\n\nWell it's not shallow learning, so thats good, but even better, it's let to some remarkable progress on some of the central problems in machine learning. In fact, its even made its way to into the popular press with attention grabbing headlines like, deep neural networks revolutionized the field of speech recognition. Google AI algorithm masters ancient game of Go. Microsoft says its new computer vision system can outperform humans. Deep learning seen as the key to self-driving cars. There is substance behind each one of these.\n\nIf you have a smart phone, then chances are, the software on it that translates your voice into words uses deep learning. It's not that speech recognition didn't have good solutions before deep learning came around. It's just that the solutions that came from deep learning are markedly more accurate, and are fast enough that they can even be done in real-time, so that you don't have to wait seconds to have your voice transcribed.\n\nAnother major achievement of deep learning, was mastering Go. Go was long thought to be a much harder game for computers to play competitively than either checkers, chess, or backgammon. In each of those games, we've had terrific computer programs for many years. That can beat, or at least complete with, the world's best players. But until just this year, our best computer programs for Go were nowhere near the top players in the world. And now the crown belongs to the machines. We'll talk more about this example later in the unit. But, what I really wanna talk about are deep learning's applications to computer vision. Sociologically, that's the example that seem to get everyone's attention.\n\nSee, machine learning is often driven by challenge problems. There are wide variety of techniques in machine learning, in this module we've seen just a few. And in some other modules in the course, we've covered a few more. But, that's by no means comprehensive. When it comes to machine learning, everyone has their favorite tools that they bring to bear. But how good are these approaches, compared to each other? Within the computer vision community, there's a famous dataset called ImageNet, at a yearly competition. Let me tell you a bit more about it. ImageNet is comprised of 1 million labeled images.\n\nEach image is labeled with 1 of 1,000 different possible categories. Some of these categories are quite specific, such as a particular breed of dog. Now, this is a really massive collection of images. The goal is to train a classifier that can correctly label images with the smallest possible error rate. Now there's one technicality, that the way we measure error rates, is to let the classifier choose five different labels, and we count how often the true label of the image in the top five. Every time it is, we call it a success, and every time it isn't, we call it an error. So what makes image classification so challenging, is that it's something that humans are quite good at, and machines are relatively bad at.\n\nThe opposite type of problem would be something well defined, like computing products of, say, eight-digit numbers. Surely, your computer or smartphone is much better than you at this, and many other similar tasks. But there are still some things that humans are very good at, and machines simply haven't caught up to. For those of you who have kids, remember back to the time when your kid was first learning to recognize animals. At first, he may not know the difference between an elephant and a dog. Maybe he's seen many dogs in person, but never an elephant. So you might point to a picture of an elephant in a story book, and shout, dog! But, it takes a very few examples to teach your child the difference between elephants and dogs. You could show him a few examples of each, and point out the long trunk, and he would be able to extrapolate to all sorts of new pictures that hes never seen before.\n\nThe human mind is amazing, this phenomena is often called one-shot learning, because it takes just a few examples to learn entirely new category. We're quite far away from having machines that can learn nearly so fast. There's been decades of work on computer vision, developing a cornucopia of techniques. But until recently, the state of the art algorithms can only achieve error rates between 25 and 30% on ImageNet.\n\nThis is still quite good. If you were to guess five categories randomly for each image, your error rate would be 99.5%. But it's still quite far away from what humans can achieve, which is somewhere in the range between 5 and 6%. Some of the categories are so specific, that humans get them wrong too. This all changed in 2012 when a team of researchers used deep learning to achieve error rates about 15%. In fact, even these bounds were quickly improved upon. Every year since 2012, the competition has been won by deep learning. And the entire field of computer vision has shifted its center of mass towards these new techniques. The current best algorithms achieve error rates less than 5%, and amazingly, perform about as well, if not a little bit better, than humans do. This is something that many researchers thought would take decades to accomplish. But in this problem, and many others, deep learning is rapidly reshaping the boundary of what we think of as possible. It's enabling machine learning in a way that's both awesome and exciting. But it's also a technology, unlike most of what we've covered earlier, that we don't understand the algorithmics behind.",createdAt:mi,updatedAt:mi,publishedAt:"2023-03-16T01:24:51.073Z"}},{id:1204,attributes:{title:"Classification Using a Single Linear Threshold",slug:"classification-using-a-single-linear-threshold",duration:"7:51",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience52ClassificationUsingaSingleLinearThreshold",objective:"Now that we've gotten a sense of the excitement surrounding deep learning, let's get into the mathematics of what it is and what it's all about.",englishTranscript:"Now that we've gotten a sense of the excitement surrounding deep learning, let's get into the mathematics of what it is and what it's all about. Later, I'll tell you about some of the philosophy and motivations from neuroscience behind deep learning. But I think of this as superfluous to understanding on a pragmatic level what deep learning means. Now is a good time to mention that the word deep in deep learning isn't the judgement call about the intellectual merits of the field. But rather, refers to layering.\n\nSo, for starters, whenever someone mentions the term deep learning, you should immediately think layered learning. That's what deep learning is about, building complex, hierarchical representations from simple building blocks. Let's start with the building blocks. In fact, when we covered support vector machines, we already covered what's a common building blocks of deep networks. Remember the classifier itself? That's sometimes called a perceptron. Let me remind you what a perceptron is.\n\nA perceptron is a function that has several inputs and one output. Let's say that it has n inputs just to fix some parameters. Then the output of a perceptron is computed in two steps. In the first step, we compute a linear function of the inputs. The coefficients of the linear function are called weights. In the second step, we take the output of the first step and compute a threshold. This threshold takes any value above some cut off tao and maps it to the value +1. And maps everything below tao to -1. The second step is the only non-linearity. Now just to make sure that the previous definition was clear, how many parameters define an n input one output perceptron? While we need n weights and one threshold tao, so in total that's n+1 parameters. Now what's the connection to support vector machines? The class fire that we learned was itself a perceptron. As a class fire it only recognizes linear patterns.\n\nAnd to combat this, we introduced the idea of mapping the input space to a new space using a kernel. In deep learning, we will take a very different approach where we layer perceptrons on top of each other to get our non-linearities. So let's work up towards that and start with a concrete example. Let's return to the problem of image classification. Now what if we're interested in identifying which images contain a dog and which images don't? This is already an easier problem than the types of problems that we face in the image net challenge problem, where we even need to distinguish between different breeds of dogs from each other. In any case, as usual, we can represent our input, say a 256 by 256 size image as a vector.\n\nIf each pixel is associated with three values, that's, red, green, and blue composition, in what dimensional space should we think of our picture? Well, we can can think of it as 256 times 256 times three dimensional space. Now if we want to use a perceptron to solve our image classification problem. What we're really asking for is a linear function of the pixels that's positive if the image contains a dog, and negative if it doesn't. As you might imagine a perceptron is just too simple a function to solve such a complex task in vision. There probably isn't the nice linear function of the pixels that decides whether or not there is a dog in the picture. So how can we create more complex functions out of perceptrons as building blocks? Well we can layer them?\n\nIn the simplest set up, imagine that there are just two layers of perceptrons. Our input is an end-dimensional vector representing a picture. Each perceptron in the first layer has n inputs, each with its own weight. So each perceptron in the first layer computes its own thresholded linear function. Now comes the interesting part. If we have n perceptrons in the first layer we can have a single perceptron in the second layer, which takes as its inputs all of the outputs of the other n perceptrons. So how many parameters define this model?\n\nWell, we have (n+1)m parameters for the first perceptrons in the first layer and (m+1) parameters for the perceptron in the second layer. What's important is that the overall function we've computed that takes in an n dimensional input factor and outputs a plus one or minus one is much more interesting and complex than the type of function we got from a single perceptron. We can take this idea much further and have more than two layers. The functions that we get are called deep neural networks, and in practice one usually sets them to have between six and eight layers and millions of perceptrons in between.\n\nThere are several fragments of intuition behind these types of functions. The hope is that the lower level layers of the network identify some base features, like edges and patterns, and that each layer on top of them builds on the previous layer to create much more complex, composite features. The intuition is how might you build up a feature that represents whether or not a dog is present in an image? First you would identify its edges. And then you'd identify which collections of arrangements of edges represent legs and which represent the body and the head and then which arrangements of those parts represent the dog? Recall that in ImageNet we want to correctly classify according to 1,000 different labels at once. Even though there a million total images, that's not actually that many examples per label. So what's important is that the features that are useful for identifying one breed of dog can be useful in identifying other breeds of dog as well.\n\nIn this sense, a deep network can have a thousand outputs, one for each label, built on top of a common deep network underneath it, one which is hopefully identifying useful, high level representations that are needed to understand images. Another rationalization for deep neural networks is that they parallel what happens in the visual cortex. There's still a lot about the brain, okay, most of the brain, that we don't understand. But it does seem that the visual cortex has a similar type of hierarchical structure with neurons on the lower layers recognizing lower level features like edges. Moreover, we can measure how quickly a human can recognize an object and how quickly a neuron fires.\n\nThese tell us that even though the visual cortex is performing some hierarchical computation, it requires at most six to eight layers in order to solve high level recognition problems. So let's conclude this discussion by first generalizing what we've talked about so far, and then pointing towards the main topic that we will be interested in for the rest of the unit. First, the perceptron has a linear and non-linear part, the threshold function. If it wasn't for the threshold, creating deeper networks would not buy us anything. We would still be composing linear functions. And no matter how deep we make the computation, the function we get would still be linear. It's the non-linearity that we added that makes deep network so functionally expressive. In fact, there are many other non-linear functions that we could have chosen instead of a threshold. We could have chosen a logisti, sigmoid, or hyperbolic tangent, or any other smooth approximation to a step function. This and many other aspects of the architecture of deep neural networks are all valid design choices that have their own merits.\n\nThere are many research papers that grapple with issues like which non-linear functions work the best, and how should we structure the internal layers for example, using convolution. We won't talk more about these issues, but it's good to know that they're very important. Second, we've said nothing about actually finding the parameters of a deep network. Modern deep networks have millions of parameters, which is a very large space to search over. When we talked about the support vector machine, we had the perceptron algorithm, that told us if there is a linear class file, we can find it algorithmically. But even if there is a setting of the parameters of the deep network, that really can classify images accurately, into, say different breeds of dog, how can we find it? There is no simple answer to this question. There are approaches that seem to work in practice, but why they do it is still very much a mystery and perhaps a phenomenon that has to so with some of the strategies of searching in such a high dimensional space.",createdAt:mj,updatedAt:mj,publishedAt:"2023-03-16T01:24:49.872Z"}},{id:1203,attributes:{title:"Hierarchical Representations",slug:"hierarchical-representations",duration:mk,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience53HierarchicalRepresentations",objective:"Learn how to set the parameters of a deep neural network",englishTranscript:"The nagging question that should be on your mind is how do we set the parameters of a deep neural network. So far, we've been thinking about them as functions that map a high dimensional vector to multiple outputs, and we constructed these functions hierarchically. But that only describes the class of functions that we're working with, not how to find the best function for some image classification task that we actually want to solve.\n\nOur first goal is to cast the problem of finding good parameters as an optimization problem. The catch is that not all optimization problems are created equal. There is an important class of optimization problems called convex optimization problems for which we have many good algorithms, and we understand quite well when and why they work. But life is not so easy in the case of deep neural networks. The optimization problems that come out will be non-convex and unwieldy. We'll talk more about this issue later, but first, let's work towards defining the optimization problem we'll be interested in. Returning to the image net example, our deep neural networks have n inputs and 1,000 outputs, one for each category that we'd like to assign images to. What we're hoping for is that when we feed in a picture as an n dimensional vector, that the output in the last layer contains a 1 in the correct category and 0's in all the other categories. This is sometimes called the one hot encoding. Now, what if you give me all the parameters of a deep network? How could I evaluate how good these parameters are?\n\nI could take each one of the 1 million examples in ImageNet, feed each picture into the network, and check whether you got the correct label. To be concrete, let me introduce what's called the quadratic cost function. Let's say the input is some vector x and it's true label is category j, then the output that I would like to get is all 0's except for a 1 in the j of output [? bit. ?] Now, I could penalize you by how far off your output is from this idealized output. If on input x, you output a1, a2 up to am. As your m output bits, I could take as a penalty aj minus 1 squared, plus the sum of all the other ai's squared. This is a function that evaluates the 0 if you get exactly the correct output, and evaluates to something non-zero if you get the wrong category. What we've done is we defined a cost function. If you give me the parameters of the deep neural network, I can evaluate how good it is by computing the average cost on a typical example from ImageNet.\n\nNow if we want to find a good setting of the parameters, why not look for the setting of the parameters that minimizes this average cost. This is an optimization problem. We have an explicit function that depends on the parameters, as well as the training data, that we'd like to minimize. It turns out, that if you find a setting of the parameters that works well on the training examples, i.e. Has low average cost, it achieves low error on new sets of examples that you give it too. This can be made mathematically precise, but it's too much of a digression to get into here. In any case, we have what we're after.\n\nWe have an optimization problem that, if we could solve, would find good parameters. Is there just some off the shelf way that we can plug-in any optimization problem we'd like to solve and get the best answer? Absolutely not. The difference between what optimization problems are easy to solve and which are hard is one of the foundational issues in theoretical computer science. We don't need to delve too much into that, so let me tell you just what you need to know. First, what types of optimization problems are easy? Let's start with the one dimensional case to keep things as simple as possible.\n\nWhat if I give you some function, f of x, and I want you to find the x that minimizes it. Here, x is a real variable, so it can take on any real value. There's an important class of functions called convex functions. There are many ways to define convex functions, but let's start with an example. Take f of x equals x squared-- that's convex. Now, why is it convex? It's convex, because whenever you take any two points on the curve and draw a line between them, the line lies entirely above the curve, except at the endpoints. That's one definition of convexity. It's not the best definition for our purposes, but it's the easiest one to start with. A definition that's more natural for us has to do with its second derivative.\n\nA function is convex if, and only if, its second derivative is always non-negative. [? Modular ?] issues, like what if it's second derivative is not defined. So what's an example of a non-convex function? Let's take, f of x equals x minus 1 squared, times x minus 2, times x minus 3, as our example. There are points on the curve where the derivative is 0, and it's positive before and negative after. This definitely means that its second derivative is negative around here. The real question is, why is this bad? The important point is that convex functions have no local minima that are not also global minima.\n\nThere is nowhere that you can get stuck if you're greedily following the path of steepest descent that isn't actually the globally optimal solution. But in the non-convex case, you absolutely can get stuck. In our second example, you could get stuck at x equals 1, which achieves f of x equals 0, even though there are x's which make the function negative. What's abstract? What's going on here? If you have a convex function, and you're at some value x, and you're searching for the value that minimizes f, what you would do is take the derivative at x. If it's negative, you would take a step to the right, increase x. And if it's positive, you would take a step to the left. If you choose the step size correctly, this is guaranteed to converge to the global minimizer of the function. As you may notice, you need to decrease the magnitude of the step based on how large the derivative is.\n\nNow, what happens if you try the same strategy on a non-convex function? All you can say is that you reach a local minimum, but, in general, it will not be the global minimum. All of these ideas can be extended in a straightforward manner to higher dimensional spaces. Instead of taking the derivative, you would take the gradient. The gradient points in the direction of largest increase for the local linear approximation. And wherever you currently are, you would take a step in the direction opposite to the gradient. When you have a convex function in high dimensions, this is, again, guaranteed to converge with a globally optimal solution. When you have a non-convex function, it might only converge to a locally optimal solution.\n\nOr even worse, it could get stuck in a saddle point, which is not a local optima, but for which the gradient is 0. So what we're going to do is use gradient descent to find the best parameters for our deep neural network, even though the function we're trying to minimize is non-convex. We're taking an algorithm that's guaranteed to work in the convex case, that we know does not always work in the non-convex case, and using it anyways. One of the greatest mysteries is that it still seems to work. What it finds is not necessarily the globally optimal solution, but even the locally optimal solutions it finds are, seemingly, still good enough.",createdAt:ml,updatedAt:ml,publishedAt:"2023-03-16T01:24:48.526Z"}},{id:1202,attributes:{title:"Fitting Parameters Using Back Propagation",slug:"fitting-parameters-using-back-propagation",duration:"5:00",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience54FittingParametersUsingBackPropagation",objective:"Find out how to apply gradient descent to a deep neural network",englishTranscript:"Now let's talk about how to apply gradient descent to a deep neural network. We have a cost function that we'd like to minimize. Let me mention a technicality that we haven't addressed so far. When we talked about perceptrons, we described them as a linear plus nonlinear part. We took the threshold function to be our nonlinearity. But there are many other reasonable choices like a sigmoid function. For what we talk about next, you should think sigmoid. Our perceptrons compute a linear function of their inputs, using their weights, adding a constant term called the bias and feed this value into a sigmoid function to get their output.\n\nThe reason it will be important to work with sigmoids is that these functions are smooth and de-frenchable. Or as the derivative of a threshold is everywhere either 0 or undefined. In any case what we've talked about so far is using grading decent to minimize or approximately minimize a non-convex function. Our functions come from computing the quadratic cost function on the output of the last leg. And it depends on all of the weights and biases inside the network. The gradient is a measure of how changes to the weights and biases change the value of the cost function. And we know we want to move in the direction opposite to the gradient because we want to minimize the cost function. The main question now is, how do we compute the gradient?\n\nIn the context of neural networks, we can compute the gradient using what's called back propagation. The equations that define back propagation are quite a mouthful, and involve a lot of matrix vector notation. The intuition is much simpler, though. What back propagation is really doing is using the chain rule to compute the gradient layer by layer. Let's get into the intuition? Imagine in the M outputs of our our neural network are a1, a2 up to am. And these are parameters that we feed into the quadratic cost function. Then it's straightforward to compute the partial derivative of the cost function with respect to any of these parameters. Now the idea is that these m outputs are themselves based on the weights in the previous life. If we fix the IF output, it's a function of the weights coming into the ith perceptron in the last layer, and also of its bias. We can compute again how changes in these weights and biases affect the ith output.\n\nThis is exactly where we need the nonlinear function, in our case, the sigmoid, to be differentiable. Back propagation continues in this manner, computing the partial derivatives of how the cost function changes. As we vary the weights in the layer based the partial derivatives that we've computed for the weights and biases in the layer above it. That's it, that's back propagation. Now we have all the tools we need to apply deplar. We talked about how to build up complex nonlinear functions from perceptrons as building blocks. We talked about how to cast the problem of fitting its parameters as an optimization problem, albeit a non-convex one.\n\nAnd we talked about how to compute the gradient in a layer wise manner using back-propagation. Now let me say a few more words about what we've learned so far before we start to apply. Why does gradient descent on a nonconvex function work at all? In lower dimensions, it seems obvious that it really does get stuck. The truth is that no one knows why it seems to work in high dimensions. There's several possible explanations.\n\nMaybe these functions are closer to convex than we think. And are at least convex on a large region of space. Another factor is that what it means for a point to be a local minimum is much more stringent than higher dimensions. A point is a local minimum if every direction you try to move him, the function starts to increase. Intuitively it seems much harder to be a local minimum in higher dimensions because there are so many directions for you to escape from. Yet, another take away from this discussion is that when you apply back propagation to fit the parameters, you might get very different answers depending on where you start from.\n\nThis just doesn't happen with convex functions. Because wherever you start from, you'll always end up in the same place. The globally optimal solution. Also, neural networks first became popular in the 1980s. But computers just weren't powerful enough back then, so it was only possible to work with fairly small neural networks. The truth is, that if you're stuck with a small neural network and you wanna solve some classification task, there are much better approaches such as support vector machines. But the vast advances in computing power has made it feasible to work with truly huge neural networks. And this is a major driving force behind their resurgence. Lastly, in our discussion about hierarchical representations.\n\nWe talked about some of the philosophy in connections to neuroscience. There is at best a very loose parallel between these two subjects. And you're advised to not take this too literally. In the early days, there was so much focus on doing exactly what neuroscience tells us happens in the visual cortex that researchers actually stayed away from gradient descent because it wasn't, and still isn't, clear that the visual cortex can implement these types of algorithms. As one of my colleagues says, they got caught up in their own metaphor and were missing out on better algorithms just because they weren't neurally plausible.",createdAt:mm,updatedAt:mm,publishedAt:"2023-03-16T01:24:47.089Z"}},{id:1201,attributes:{title:"Convolutional Neural Networks",slug:"convolutional-neural-networks",duration:"2:31",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience55ConvolutionalNeuralNetworks",objective:"Learn about convolutional neural networks",englishTranscript:"Let's talk about some of the architectural tricks of the trade. The first and arguably most important is the notion of the convolutional neural network. Let me explain how the first layer works. Let's say that our input is a grayscaled image that's 256 by 256. We can break it up into 8 by 8 image patches. In total we have a 64 by 64 grid of these patches that comprise the image. Now let's consider a linear function on just the 8 by 8 image patch.\n\nSometimes this is called a filter, in our case a filter is an 8 by 8 grid of weights. And when we apply a filter to an image batch what we do is we take an inner product between them as vectors. So if we apply a filter to each one of the patches we get a new 64 by 64 grid of numbers. What good is this? Remember the intuition is that the first few layers detect simple objects like edges. We can think of a filter as a naive object detector. And instead of having it have different parameters when we apply it to each of the different image patches. Why not have the same parameters throughout?\n\nThis drastically reduces the number of parameters and consequently there are many fewer things to learn. If we take this idea to its natural conclusion we get convolutional neural networks. In the first layer, we have a collection of filters, each one is applied to each of the image patches. And together, they give the output of the first layer after applying a non-linearity at the end. This is already a major innovation because it means we can work with much larger neural networks. In practice, just the first few layers are convolutional, and the others are general and fully connected. Another important idea is the notion of dropout. Here when we compute how well a neural network classifies some image, say through the quadratic cost function.\n\nWe instead randomly delete some fraction of the network and then compute the new function from the input to the outputs. The idea is, if a neural network continues to work even if we drop perceptrons from the intermediate layers. Then it must be spreading information out in a way that no node is a single point of failure. Training a neural network with dropout makes the function we learn more robust. This is not a precise statement, but it's the prevailing intuition. Now, you know the secret sauce of deep learning. Sometimes it's hard to wade through what's fact, what's fiction, and what's hype. We've done all the hard work and laid out the foundations.\n\nAnd now it's time to have some fun. In the remainder of the unit, we'll cover some of Deep Learning's truly amazing applications.",createdAt:mn,updatedAt:mn,publishedAt:"2023-03-16T01:24:45.613Z"}},{id:ht,attributes:{title:"Limitations of Deep Neural Network",slug:"limitations-of-deep-neural-network",duration:"3:55",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience56LimitationsofDeepNeuralNetwork",objective:"Now that we know what deep learning can do, learn about the limitations of this method",englishTranscript:"I hope that I've given you a sense of the excitement surrounding the networks. But now it's time to take a sobering look at a more frightening aspect. In the examples we've seen, deep learning has been able to conquer some very hard challenge problems. And that's a great reason for being optimistic that they'll lead to practical self-driving cars very soon and all sorts of other technological advances. But there's an important worry. When we talked about fitting a deep network's parameters, we discussed the difference between convex and non-convex functions. The truth is that we don't really understand why things like gradient descent work so well and even then, the actual features that it finds are difficult to interpret.\n\nWe can't readily take a deep neural network that we've learned and see why it's working so well. This leaves a lot of room for manipulation. Following the work of what if we start with an image of a deep network successfully classifies? How much do we have to change the image to change the label that the network assigns? Let's say I give you an image and a target label, say ostrich. I want you to fool the deep network into thinking that the image is an ostrich, even though it's very clearly not. As usual, we represent the image as a high dimensional vector x.\n\nNow we can look for the smallest vector R, which we think of as a perturbation to the image. So that when the deep network is applied to X plus R, the output has a 1 in the ostrich category and 0s in all the others. Finding such an R is a non-convex optimization problem. But, as usual, there are approaches for approximately solving it, nonetheless. Obviously, if I'm allowed to change the image entirely, then I can make the deep network think it's an ostrich by making the picture be of an ostrich. The question is, how small can the perturbation be? It turns out that you can get away with the perturbation that's imperceptible to the human eye.\n\nNow let's put that into perspective. Much of the initial excitement of deep learning came from the fact that now we are able to classify images automatically with error rates that are even slightly better than what humans can achieve. But the classification function that they've learned is so discontinuous and unwieldy, that changes that ought not to change the category. In fact, you wouldn't even see them as a change to the image at all, can be used to fool the deep network into thinking the image is whatever category you like. In fact what's even more frightening is that the same perturbation works across many neural networks. So you could think that if I don't know what neural network you've learned, and don't have access to the millions of weights and biases that define your model, I won't be able to fool it.\n\nBut it turns out that I could learn my own neural network even using a subset of the training data that you've used, find the perturbation for a given image that makes my network think that it's an ostrich, and the same perturbation would work for yours too. So all this time we've been talking about deep neural networks being able to compete with, or even out perform humans. If they can find new moves in the ancient game of Go, then why not solve other sorts of decision making problems for us? We could imagine relying on them to keep us safe on our daily commute in self driving cars. Or diagnosing us based on our symptoms and medical history.\n\nOr even estimating the credit risk of an individual by incorporating more information than currently goes into your FICO score. But all of these wonderful possibilities come with a caveat that if we don't understand why deep learning works, the classifiers we learn might be subject to all sorts of nefarious manipulations. Or might learn unfair and uncontrollable biases in how they make decisions.",createdAt:mo,updatedAt:mo,publishedAt:"2023-03-16T01:24:44.141Z"}}]},instructors:[{id:637,name:mp,position:a,nationality:a,achievements:a,focus:a,bio:mq,learnMoreURL:a}]}},{id:aV,attributes:{title:"Recommendations and Ranking",slug:"recommendations-and-ranking",description:"Learn what a recommendation is and what data it involves.",prerequisite:a,duration:kZ,objective:a,createdAt:"2023-10-18T21:49:04.584Z",updatedAt:"2023-10-18T21:49:45.490Z",publishedAt:"2023-03-16T13:13:05.376Z",chapters:{data:[{id:1199,attributes:{title:"What Does a Recommendation System Do?",slug:"what-does-a-recommendation-system-do",duration:"6:18",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience61WhatDoesaRecommendationSystemDo1",objective:"Learn what a recommendation system does and how it works",englishTranscript:"Welcome to this module on recommendation systems. I am Philippe Rigollet from the Mathematics Department and the Center for Statistics at IDSS. A large part of my research is devoted to high-dimensional statistics, and I also teach courses on this topic at MIT. In a nutshell, the goal is to find a needle of information in a haystack of data. As we will see, the goal of recommendation systems are similar in many ways. \u003E\u003E Hello, I'm Devavrat Shah. I'm with the Department of Electrical Engineering and Computer Science at MIT. I'm member of the Center for Statistics at ITSS Institute for Data Systems and Society. Philip and I will be your co-instructors for Modular Recommendation System. My background is in machine learning, and large scale data processing. I teach courses on data topics in MIT regularly. I have spent the past decade thinking about extracting meaningful information from social data, the data that we generate. As we shall see in this module recommendations system represent an important large class of societal questions that precisely deal with extracting information about people's preferences. What people like, what people don't like, from the data that they provide. The goal of this module is to discuss radius challenges associated with designing recommendation system from both statistical viewpoint, and computational viewpoint. \u003E\u003E Before we dive into the subject, we should answer an important question if ever. What is a recommendation system? \u003E\u003E Great question, Philip. So in a nut shell, a recommendation system is something that helps you find what you're looking for. \u003E\u003E I see, like a search engine, for example, Google. \u003E\u003E Great point. It's a little different than that.\n\nYou see, in Google, when you research something, you need to know exactly what you want to look for. For example, if there's a restaurant that I know precisely that I wanna go for dinner tonight. All I will do is in Google search bar I will type the name of the restaurant and then boom. Google will give me the address of the restaurant, the menu of the restaurant, the hours of the restaurant, and what not about the restaurant. \u003E\u003E I see but what if I don't know what kind of restaurant I'm looking for? Google is not making any recommendations to me. \u003E\u003E Great point again, Phillip. See, Google search is helping me find answer to the question. When I know precisely what question I'm trying to and get answer. Recommendation system is less concrete. It's a bit mysterious you see. Let's take an example of Yelp for that matter. Yelp is perfect for recommendation because it's suppose recommend restaurants to us. So let's say hypothetically we are new to this great town of Cambridge where MIT is, and we need to go for dinner tonight. \u003E\u003E Great, let's find a good barbecue. \u003E\u003E Well, if you forgot, I'm a vegetarian. \u003E\u003E How about we try American, or whatever that is? \u003E\u003E Okay, American cuisine it is. \u003E\u003E All right, so let's find an American restaurant nearby using Yelp and that fits our budget. \u003E\u003E In a sense, a recommendation system is also a search engine. When I don't know precisely what to search for. \u003E\u003E Absolutely, Philip, and here search is more semantic. See, the semantic associated with the search intent is up to the interpretation. It's a context dependent. To a large extent, it's very personalized. It's personalized depending on the person who is searching for the answer. For example, what a good restaurant might mean to you, it may mean not so good to somebody else. Or what a good restaurant, to you today, might mean something different yesterday, and might mean something different tomorrow. \u003E\u003E In a sense a recommendation system is also a search engine when I don't know precisely what to search for. \u003E\u003E That's absolutely correct.\n\nThe search here is more semantic. The semantics associated with this search intent is up to interpretation. It's a context dependent and to a large extent it's very personalized to the person who is searching. For example, what a good restaurant might mean to you, may not mean good to somebody else, like me. Or what a good restaurant might mean to you today, might mean something very different yesterday, and might mean something very different tomorrow. It is this semantic aspect of the search that makes designing a recommendation system much, much harder than building just a search engine. \u003E\u003E So see, here is the top American cuisine restaurant in the area according to Yelp users. So see, for example, I don't like this restaurant. I think it's too loud and it's too expensive. Sorry, but I guess I'm not your typical Yelp user. \u003E\u003E Indeed, that's exactly what I meant about personalization a few seconds back. We'll see that it's a key feature of powerful recommendation systems.\n\nThe recommendation is different for each and every user. Customized to ones taste and is different depending on the context be it time, place, or even ones mood. \u003E\u003E That's correct. You see this when you shop on Amazon. The site makes recommendations that are based on your past purchases. In my case, I'm a huge grilling fan. It recommends barbecue items all summer long. \u003E\u003E The same applies when you watch TV or other media. The Netflix recommendation system has ordered the catalog for you so that you don't have to search forever. A recommendation system is the primary reason for the success of Netflix as a business. They have managed to make it's users feel as if there's an infinite pool of catalog, even though, actually they're about a really small catalog. \u003E\u003E And music too. It's pretty hard to keep up with music trends these days and recommendation systems help you do just that. It tells you what music you might enjoy listening to. For example, systems like Pandora or Spotify provide excellent recommendations. I've already discovered this band I like called The Arcs using Pandora's recommendation system. \u003E\u003E But sometimes recommendation is not just for convenience. But it is necessity. Take for example of YouTube. You know how much content that's from YouTube generates? Every minute, here's the statistics I came up with. Every minute, 300 hours of video is uploaded on YouTube. So let's hypothetically assume that you and I are going to 100% of our time continue watching YouTube. That it will take a million lifetime for us to watch content generated in one lifetime. In a sense, recommendation system is essential to discover the content that we care about on YouTube otherwise we would not be able to deal with it.",createdAt:mr,updatedAt:mr,publishedAt:"2023-03-16T01:24:42.645Z"}},{id:1198,attributes:{title:"Types of Recommendation Systems",slug:"types of recommendation systems",duration:"6:02",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience62WhatDoesaRecommendationSystemDo2",objective:"Explore more types of recommendation systems",englishTranscript:"So is that all? Do recommendation systems help me find things that I might want to consume? \u003E\u003E No, no, we're just scratching the surface, it's tip of an iceberg. You see, recommendation system is a very large class of prediction problem. At the core of it, it's trying to understand an individual's choice or preference or personalized preferences for that matter. So wherever it is crucial to understand once choice, things you like, things you dislike. Recommendation systems are very useful. For example, consider online advertisement. Like today morning, I visited my New York Times website. So, when I was in New York Times website, I was reading my content, but hey you know what, on the side of it or sometimes in the middle of it, they showed me an advertisement. So, now the question is that, what advertisement should they show it to me that is already personalized. And, this is a recommendation problem. \u003E\u003E Absolutely. And in this scenario, the question is about determining what you as a customer is going to be interested in the most in order to target advertising. This is actually very similar to the problem of determining what you might like to eat, read or watch, a problem of recommendations. One way to look at a recommendation system is that it's trying to match people to products, restaurants, movies or ads. But more generally, it could match people to people just like in a dating system.\n\nThink of the website eHarmony for example. Here people are seeking partners to date and eventually marry or never see again in some instances. The goal of eHarmony is to match two people who are likely to be attracted to each other. We do not know exactly what eHarmony does but in principle a system like this is trying to do the following. Consider an eHarmony customer. Let's call it Devavrat. Sorry, that was unfortunate. So Devavrat provides information to the site, mostly about himself, but also about his preferences. Using this information, the system tries to figure out his preferences and find potential matches in the database. Now, Devavrat may have very high standards. And among his matches, the system also has to find those for whom Devavrat is, in turn, a match. And if we can achieve this perfectly, it's truly remarkable. Most of the time, people are like Devavrat, they don't even know what they are looking for. \u003E\u003E Totally, I did not know what I was looking for til I saw my wife, Salvia. Look, I got married. Now Tinder, it's another system like this, another dating system of the modern time. The way it works, at least in principle, is as follows. Tinder is an app. You enroll into it and then you are shown pictures of other Tinder inhabitants who are in your geographic proximity. You swipe to left or right depending upon whether you like the person or you don't like the person. And the person on the other end, if you like the person, gets notification. In principle, Tinder would like to show you exactly those people that you like. So you're always swiping on one side. But of course, that doesn't happen. But it's trying to learn this by asking you question. And the brilliance of this system is, the way they're asking you the question, they're incentivizing you to answer them correctly. You see, you want to answer the likes by sort of the ones you like and the ones you don't like. Now Philip as a Tinder expert, let me give you advice. Get a better headshot so that your wife will like you more. \u003E\u003E How about something like this? \u003E\u003E Works. \u003E\u003E Or maybe the intellectual. All right, so we know that recommendation systems are useful for finding people's preferences and use them to match them to the choices that they may prefer. And that's it right? \u003E\u003E Actually no. Recommendation systems are like a bottomless pit.\n\nThey have far reaching implications in the operation of work. Take any social systems that you can think of. Be it infrastructure like transportation, e-commerce, brick and mortar retail, entertainment, media, and even financial institution. They are all affected by how people make choices. What people like, what people dislike. After all, it's about people's choice that determines demand. And as we all know from basic macroeconomy, there are two sides of economy. One is supply, and another is demand. So if we can understand demand really well, then we can actually plan the supply well, and that will make, sort of, operations better. So as a matter of fact, consider one of the largest operational problem of our time. The logistics, or managing supply chain. A supply chain that takes goods from the hand of manufacturer, from say somewhere far away in China, and brings it to my, or your doorsteps.\n\nThis is the key operational problem that businesses like Amazon, in United States, Alibaba in China, Flipkart in India are grappling wit. On one hand, as we have discussed in depth, the ability to solve the right product recommendations on e-commerce website helps consumers make the right purchases. On the other end, those exact interactions can help them inform what are the things that people like and dislike and hence manage supply chain slash manufacturing really efficiently. \u003E\u003E Wow, so it looks like recommendation systems really are everywhere. Any other aspects that we should discuss? \u003E\u003E Actually, it might be fun to discuss that Tinder example again. Remember, we are Tinder veterans, without real Tinder accounts. Recall the movie Social Network a la Facebook, Zuckerberg. Remember the game of Hot or Not? So, in an interactive world, where individuals are available to engage fully, one way to think of a recommendation system is like playing a game of 20 questions. I want to know what you are thinking, and I want to know that by asking as few questions as possible. \u003E\u003E Absolutely, it's like playing the game Guess Who against the Gary Kasperov of Guess Who? He guesses right in a record number of questions. \u003E\u003E Bingo, this is exactly how a recommendation can work in an interactive situation. Later in this module we will discuss a case study related to selecting beers and gathering opinion of people using comparison like questions and answers.",createdAt:ms,updatedAt:ms,publishedAt:"2023-03-16T01:24:40.431Z"}},{id:1197,attributes:{title:"Implications Behind Recommendation Systems",slug:" implications-behind-recommendation-systems",duration:"6:07",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience63WhatDoesaRecommendationSystemDo3",objective:"Learn about the heavier implications behind recommendation systems",englishTranscript:"Tinder, beer, and Guess Who are all fun and games. But recommendation systems are also very serious business. They have close conceptual ties to the core of a democratic society, such as how to run elections or elect governments based on collective opinions. This is called Social Choice Theory. How does it work? We have discussed enough example of what the recommendation system is and its utility. Now the question is how do we do it? How do we go about building it? Are Tinder, Pandora, and the Amazon recommendation system built on the same principles? \u003E\u003E Well. recommendation systems are age-old. They way they work historically is by we calling up our friends, people we know, and whose opinions and preferences we trust and ask them for suggestions. At the core, modern recommendation systems are not much different from this. They are automated, they operate at scale, and hence require some sophistication. Very crudely speaking, to find what you might like we can find other people with similar taste, similar interest, similar opinions, and then use that preference or dislike, what I might like or I might dislike. Such algorithms are known as collaborative filtering. We filter or predict our preferences through collaboration.\n\nWe shall spend quite a bit of time in this module discussing various algorithms for recommendations based on this very basic insight. This insight, when viewed mathematically, leads to unusual algebraic approaches for recommendation. It might be worth cautioning while the insight described seems very simple, it takes a lot more to actually building a real system, and we shall discuss all of this as part of this module. So before we go into details, lets discuss a few of the nuances involved in building a recommendation system. \u003E\u003E Transparency of recommendation systems are important. When systems like Amazon product recommendation chooses to display an item, it is important for the user to understand what that means. For example, it is important to tell them that this is the item that they may want to buy together with another item. If you're buying an electric snow blower, make sure that you get a long extension cord as you will need that. When a recommendation for such an extension cord is made, it is important that the user understand why this item is recommended.\n\nIn this case, that the cord was frequently bought together with the snow blower. Similarly, When you are recommended a movie, like Goodfellas, you may want to know that you are suggested it because you watched and liked The God Father, not because you watched Sponge Bob with your three year old. \u003E\u003E Simple machine learning doesnt work well for recommendation. Remember Pandora, the music streaming company we spoke about earlier? It works like this. You create a virtual station on Pandora by suggesting the name of a band, or a song, or whatever. And then Pandora keeps streaming music that corresponds to that station's flavor. Initially when Pandora started, they were trying to build such a system without really using recommendation system data. They didn't have any to begin with. Instead they had specialists, specialists that annotated songs based on musical characteristics such as electric guitar riffs or minor key tonality and whatnot. And then in a blink a very good system that thought that BeeGees was like Beatles. Not sure if BeeGees fan, or for that matter Beatles fan, would agree with that. Looking under the hood, they realized that the reason behind such a mess was very simple. In early part of their career, the BeeGees were aspiring to become like Beatles, in fact they were a Beatles knockoff. \u003E\u003E Now, of course, if we looked at the people who listened to those popular BeeGees songs, it would be very different from those who listened to The Beatles.\n\nAnd this suggests that the importance of behavioral data in building recommendation systems, it's truly social. To know your preferences, you don't really need to understand why you listen to The Beatles, but find out who also likes The Beatles, and see what else they like. \u003E\u003E But behavior data is not easy to get. Think of Netflix on day one. \u003E\u003E Or Pandora on day one, for that matter. \u003E\u003E When you do not have any behavioral data, it makes it really hard to get going, or you do really poorly in the beginning. This is known as the cold start problem. There are many ways to get around this, and we shall discuss this aspect later in the module. Now, as we know Pandora has gone on to become a very successful recommendation system. So, they really did solve the cold start problem. \u003E\u003E Self Fulfilling Prophecy. Major flip side of recommendations is that it may not allow for discovering new things by chance. Effectively, a good recommendation system will keep recommending things that are liked or preferred by most, and create a herding phenomenon. Newcomers, who are really good, might not get any attention at all, this is a typical self fulfilling prophecy phenomenon. We'll discuss how to avoid this in a principal matter in this module. \u003E\u003E Recommendations are means to an end.\n\nLooking for a suitable person to date or marry, when you don't know exactly what you are looking for, or a new restaurant to eat, movies to watch, songs to listen to, or- \u003E\u003E There are businesses opportunities. Targeted advertising, product purchase suggestions and what not. \u003E\u003E Recommendations should be objective driven. Advertising to increase click through rate. Product recommendations to increase conversions. Recommendations for profit increase might be very different from the recommendations that you provide for customer satisfaction. \u003E\u003E Building a real system is a lot more complicated. Building a real recommendation system requires a lot of context specific system design in addition to all the basic principles that we will learn in this module. So if you have to build a recommendation system that works for you, this module will provide you with all the essential knowledge that you need In addition to all the knowledge that you have from your domain context or application. When put these two together, you will have a functional, high performing, recommendation system.",createdAt:mt,updatedAt:mt,publishedAt:"2023-03-16T01:24:37.953Z"}},{id:1196,attributes:{title:"Data Science Questions",slug:"data-science-questions",duration:"4:39",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience64SoWhatIsARecommendationPredictionProblem1",objective:"Now, that we have a good idea of what recommendation systems are and where they are used, let us look a little closer at the data science questions behind them.",englishTranscript:"Now, that we have a good idea of what recommendation systems are and where they are used, let us look a little closer at the data science questions behind them. Specifically, what does the data look like and what do we want to do with it? It seems that a simple approach would be to consider this problem as a binary question. Given a pair, a user and a product, should I that recommend this product to this user? Yes or no? Classification is perhaps the best known machine learning problem. And specifically answers yes\u002Fno questions. So we could just use our favorite classification method, such as support vector machines or boosting for example, in the context of recommendations. Actually, we're going to be able to do much more than just giving a yes\u002Fno answer. We're going to be able to rank products. And even guess how the user would rate them. Let us do the first thing that we should always do in data a science, take a closer look at our data. Netflix, the video streaming service, is a name that is strongly associated to recommendation systems. Clearly, their goal is to recommend movies to their users. But the main reason for this is that in 2006, Netflix did a superb PR coup.\n\nThey organized the competition with a grand prize of one million dollars to the first person that would improve the performance of their current system by at least 10%. While this may seem like a lot of money, it was actually a rather economical way of getting some of the sharpest minds on the planet to work hard on their problems. Academics, researchers, engineers, students, or even hobbyists, thousands of teams enrolled. It was a huge success and started the whole data science competition business. The problem was simple Netflix released a data of how users had rated movies on a scale from one to five. About 100 million triplets of the form-- user, movie, rating-- were released to the competitors. It was effectively a giant list of the form-- Bob rated Goodfellas, four; Emily rated Titanic, three; Frank rated Dirty Dancing, five. This is Frank. He really likes Dirty Dancing. This is called the training set and competitors use it to calibrate, or train, their algorithm. Along with the set, Netflix also released another million incomplete triplets of the form-- Ted rated Rio Grande, Soledad rated Deliverance. The goal is to predict hidden scores. These were real number scores, integer number-- one, two, three, four, or five, but the predictions could be any number like 4.52. This was useful if your algorithm couldn't decide between a four or a five, it could mitigate the losses. The performance was measured in terms of what is called root-mean-squared error. Let's parse this term to define it. Let's say that a competitor predicts 3.4, 4.1, and 2.0 but the true scores are respectively four, two, and three. The errors are 3.4 minus four, which is negative 0.6. 4.1 minus two, which is 2.1. And 2.0 minus three, which is negative 1.0. These can be negative and cancel each other, so we square them to get the squared errors. 0.6 squared is 0.362. 2.1 squared is 4.41. And 1.0 squared is 1.0, then we average them to get the mean squared error. 0.36 plus 4.41 plus one divided by three, equals approximately 1.9. Finally, the root mean squared error is just the square root of the mean squared error. Square root of 1.9 is approximately 1.38.\n\nThe last square root operation is not very important but it allows for the following interpretation-- on average, the algorithm made an error of about 1.4 per user movie pair, which is not very good in this case actually. In 2006, the state of the art Netflix system had a score of 0.95 and the winning team dropped this number down to 0.85 after three years of hard work. Once the missing numbers are predicted, we can recommend movies using simple rules. For example, we could recommend a movie to a user if the predicted score is at least 4.2. Towards the end of this video, we'll see what else can be done with this predicting numbers and if there's something else we should be trying to predict.",createdAt:mu,updatedAt:mu,publishedAt:"2023-03-16T01:24:36.473Z"}},{id:1195,attributes:{title:"So What Is a Recommendation Prediction Problem?",slug:"so-what-is-a-recommendation-prediction-problem",duration:lf,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience65SoWhatIsARecommendationPredictionProblem2",objective:"Now that we know our data set, let's try to organize it.",englishTranscript:"Let's get back to our data. 100 million triplets is huge. And remember that this is only what Netflix released for this competition, but the data set of all their users and all their movies is actually much, much larger. Let's try to organize this data a bit. The most natural way to do this is to put these numbers into a table, where the rows are users and the columns are movies. The cells of this table contain the ratings. The Netflix Prize data set had about half a million users and 17,000 movies, so we should build a table with half a million rows and 17,000 columns. Here's the row corresponding to Frank, and here is the column corresponding to the movie Dirty Dancing. This entry of the table is Frank's rating of Dirty Dancing, a five. Let's see where 100,000,000 scores looks like in this table. It's a drop in the ocean. We know about 1.2% of the entries of the entire table, and the remaining entries are completely unknown. To be fair, I should say that in the case of the Netflix Prize, the goal was to predict only a few missing numbers-- about one million, still. But Netflix needs to predict all numbers every day.\n\nThis looks like an impossible task, right? How does knowing that Frank is a Dirty Dancing fan can help me predict how Ted is going to like the Western Rio Grande? Well, we'll see our statistical modeling coupled with efficient algorithms can help us extrapolate known ratings to the entire table by leveraging realistic structural assumptions. In mathematics, a table filled with numbers is called a matrix. This is not just to sound fancy. Matrices are a well-studied object in a field called linear algebra. Using this theory, we can not only describe mathematically our structural assumptions, but also leverage a powerful algorithmic toolbox that can be scaled to the size of the whole Netflix data set and even larger. It turns out that the Tinder data set is essentially the same. Men are rows-- here's Frank-- women are columns, and the entries are binary here. Zero is swipe left, one is swipe right, and then a lot of question marks. The eHarmony data set is essentially the same, but the meaning of the entries is not as clear. It's some sort of a compatibility index it's computing using side information. This is the first thing that a dating site asks you when you subscribe, answer a long list of questions to collect information about you. We can even extract some of your physical features or type of activities from the photos you post. Exploiting this side of information requires a lot of domain-specific knowledge, and this is a whole field in itself. In a way, this is the secret sauce of E-Harmony. Let's go back to our Netflix example. Netflix knows a bit about their users-- location, type of credit card they used at time of subscription, their browsing history, what type of smartphone they use, and a lot about movies-- cast, director, year, rating, budget, et cetera. Leveraging this information has been instrumental to win the Netflix prize, as well.\n\nTo conclude this video, let me come back to a remark that I made earlier. Netflix may be interested in other objectives than simply recommending you a movie that you will like. For example, Netflix may be interested in showing you as many summary pages as possible to collect even more information about your browsing pattern. To do this, all the numbers fill the table, not just the ratings. Here's another example. Amazon may be interested in recommending a product on which they expect to make more profit rather than the one you are the most likely to purchase. For example, assume that the chances that you purchase Coffee Machine A are 20% if recommended to you and 10% for Coffee Machine B. However, Amazon receives $100 each time they sell Coffee Machine B versus only $10 for Coffee Machine A. In expectation, Amazon will receive $10 times 20%, equals $2 for Coffee Machine A, versus $100 times 10% equals $10 for Coffee Machine B. So Amazon may be tempted to recommend Coffee Machine B rather than A. Unlike Amazon or Netflix, or matching users to products, Tinder or eHarmony are matching users to users. The key difference is that products don't have any preferences, but users do. In other words, Tinder and eHarmony have more complicated problems on their hands. And we'll touch upon the specifics later in this module. To account for these different objectives, a good data scientist must know what values to enter in the table-- chances of buying, or expected revenue, for example. This is, again, domain-specific, and often, several recommendations systems are run in parallel to account for these objectives. To summarize, we've seen that the data can be thought of as a matrix with lots of missing entries. We need to fill out these entries. The meaning of these entries is very much context-specific. In the next few videos, we'll see how we can make simplifying assumptions that can help us solve the problem.",createdAt:mv,updatedAt:mv,publishedAt:"2023-03-16T01:24:35.343Z"}},{id:1194,attributes:{title:"Your First Recommendation Algorithm",slug:"your-first-recommendation-algorithm",duration:"6:43",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience66RecommendationAlgorithm101UsingPopAverages1",objective:"So far you have learned what a recommendation system is and where they can be useful. Now learn about an actual algorithm",englishTranscript:"Welcome back. Today's topic is going to be the first recommendation algorithm. Thus far, in this module, we will spend time discussing 1, what a recommendation system is. 2, where a recommendation system can be useful. And 3, what is the basic problem of recommendation. As we have seen, in its simplest form, the problem of recommendation system is filling missing entries. For example, we can think of the available data as a sheet of an Excel file. So you've got the rows, and you've got columns in your Excel file. In the context of Amazon product recommendation, rows would correspond to customers, and columns would correspond to products. In the context of Netflix, rows would correspond to, again, customers, columns would correspond to movies, and the cells or the entries in the cells would correspond to the ratings. In the context of Yelp, it is about whether a customer is going to be interested in eating at a restaurant or not. In all set scenarios, a large fraction of entries are actually missing. After all, each customer ends up eating at a very small fraction of restaurants in a town. So if we threw a dart at random on this big Excel sheet, It will most likely end up on an empty cell. So really, that says data is really sparse. In the next few chunks, we shall discuss algorithms for filling these missing entries. Now to fill these entries, we will leverage structure in the data. In today's module, we'll start with the simplest possible structure, and then evolve to more complicated structures as we go along. The simplest possible structure, the entire population thinks alike, eats alike, sleeps alike. A bad assumption in general, but excellent place to get started on this long journey of designing recommendation algorithms. The resulting algorithm is effectively what we shall call population averages.\n\nLet's start with our classical Yelp scenario. Remember, we were looking for a restaurant to have dinner, and Philippe and I settled on having an American cuisine. So let's assume that you are going to go to Cambridge or anywhere in Boston, for that matter, to have American cuisine. Let me add a twist to it, you want to celebrate an occasion. You have successfully gone through first few chunks of recommendation system, and so you're happy to go to an expensive restaurant. So now, you search, various restaurants come up as a result. Just like a search result, but filtered by your preferences. There are too many restaurants qualifying your constraints. So now, what? Well, you need to order them somehow. And this is where recommendation system becomes really useful. And this is precisely what Yelp does. It provides options for you to rank your results or alter them. One of the options is by the proximity, which is not really relevant, because remember, we are happy to go anywhere. Another option corresponds to how pricey the restaurant is. Again, we're keeping only the most expensive restaurant, not sure it is relevant either. At this stage, we are looking for what restaurant is really good. Yelp is ordering for this by suggesting order by rating. Great, various people who have been to these restaurants have provided ratings to these restaurants between one to five stars, and one can use these ratings to produce ranking for restaurants. And this is what Yelp is doing in this option. So one way to compute these rankings is as follows. 1, for each restaurant, assign it a score, a score which is equal to the average value of the ratings that it has received, okay? So for example, if a restaurant has received two ratings, five and one, then its average would be five plus one equals to six, divided by two, which is three. 2, Order restaurants as per their score. So restaurants with higher score gets higher rank, restaurants with lower score get lower rank. That is it. In a sense, this is what Yelp is doing when it's showing you that option of order by ratings. Super easy. If you are building a system, this can be obtained using a native SQL like database queries. In other words, very easy to implement and scale. In our Excel sheet visualization, effectively, we are filling out missing entries for a given cell by taking the column average of the entries. In practice, one has to be a little bit more careful than just taking averages. To understand this, we will have a small lesson. So suppose you have a coin, a coin like this, which has two sides. It has a head, and it has a tail. Every time you flip it and toss it like this, it comes up, it lands in my hand, and either it's head or it's tail.\n\nNow you want to understand, what is the chance that this coin, when I tossed, comes up head or tail? Now, a general belief for all of us is that, well, a coin is built symmetrically, so it will be 50% chance it will be head, 50% chance that it will be tail. But that may not be necessarily true. Somebody might have rigged it, maybe the physics of the coin is different, who knows? So maybe you and I want to figure out exactly what the bias is. Now, in this case, we can verify. How can we verify? Well, we can measure it. We can toss this coin many times and observe its outcome. So when I toss a coin once, it comes up head, coin toss twice, tail, head, tail, etc. Now, while I'm running this experiment, at any point, we can state the chance of a head showing up, right? Because I can say, in so many tosses, let's say 100 times, I have observed 46 times heads, which means that 46% chance for it to be sure it's coming up head. \u003E\u003E Every time when I toss a coin, and it comes up head, we are assigning score 1. Every time tail shows up, we assign a score of 0. In that case, all I'm doing is I'm computing average score. Now mapping it back to our Yelp setting, if we think of head as like or 1, and tail as dislike or 0, then effectively, average score of a coin, which is the same as computing average rating of the restaurant the way we discussed sometime back.",createdAt:mw,updatedAt:mw,publishedAt:"2023-03-16T01:24:34.224Z"}},{id:1193,attributes:{title:"Algorithm Design",slug:"algorithm-design",duration:"5:28",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience67RecommendationAlgorithm101UsingPopAverages2",objective:"Continue to learn about the algorithm presented in the last video",englishTranscript:"Now, let's consider this setting. In this setting our intuition suggests that this way of measuring the chances of head is the right thing. And indeed in probability theory this is known as the Law of Large Numbers. If we toss a coin enough times then the measured score of the coin becomes arbitrarily close to the actual chance of seeing the head. So this way of measuring the bias or the score of a coin is a perfect thing to do. Maybe there was a reason why we considered the population averages as an algorithm to begin with. But this is good when you have a lot of data. It is assuming that there's a lot of ratings in the case of restaurants that are available. In reality we have very limited data for some restaurants. Consider a newly opened restaurant. It might have only a few ratings. Therefore, we may need to worry about what happens to such an estimator when we have very few observations. Back to coin setting, suppose we have only one observation. So let's say I toss this coin first time and first time I saw head. Now what? Well, my estimator would say the bias of this coin is one in favor of head. That does not make sense because I have just got one observation. Since no matter what the bias is my estimated bias would be one or zero, independent of the coin's actual chances. And this is clearly wrong. And it's clearly wrong in the sense that I have got so few observations so I need to correct for them. Okay? Again, in the case of restaurants, each restaurant has a different number of ratings. So simple average is definitely not the best estimator. It needs correction, correction due to a limited number of observations.\n\nJust the way estimating the bias of a coin needs a correction. What should we do? This is where another principle of probability comes to our rescue. It is known as the central limit theorem. It states that the error in the average estimator of the bias compared to the true bias behaves in a very specific manner. To be precise, let's consider the normalized error. That is, estimated score minus the true score divided by the square root of the number of observations. This normalized estimator or error has a distribution called Gaussian or normal distribution with mean 0. This is named after a famous German mathematician named Carl Friedrich Gauss. Gauss lived from 1777 to 1885. He's also referred to as Princeps mathematicorum. In Latin it means one of the greatest mathematicians since antiquity. Now here's how Gaussian distribution looks like. The variation in the estimation or the amount of error varies as true score x (1- true score). Therefore, if true score is 0 or 1.\n\nThat is, truly always tails or heads, that is, always disliked or liked. Then error is 0. Totally makes sense. Effectively in that case, one coin toss is enough. On the other hand, if the true score is close to half, then error is going to be large. The problem is, we do not know a priori which is the situation. So we have to assume the worst possibility. That is, error is the largest. In that case, the key takeaway from this mental experiment is that the error behaves like 0.5 \u002F square root of # of Observations and number of samples. Therefore the true score is most likely 1 larger than the estimated score- 0.5 \u002F square root of # of samples. And smaller than the estimated score + 0.5 \u002F square root of # of samples. This suggests a robust approach to order or rank or rate restaurants. One, assign scores to restaurants as average rating- constant \u002F square root of # of ratings that the restaurant has received. Now here what is the constant c? In the case of the coin it was 0.5. We shall discuss briefly how we choose c. Two, use this score to rank order the restaurants as before. This way, if there is a restaurant which is new and has been rated very, very highly by very, very few people. Think of the owner and his friends. It will not be able to fool the system. On the other hand, if the restaurant is truly remarkable because its average is high based on a very large number of reviews.\n\nThen of course the correction would be very small, in which case average score would dominate. Now coming to the choice of constant c. Well, in the case of Yelp example, choice of c = 2.5 is great because the ratings are between 1 to 5. All right, this is great. Now we have found a solution to combat the challenge of sparse data. And the solution is, again, very simple.",createdAt:mx,updatedAt:mx,publishedAt:"2023-03-16T01:24:29.968Z"}},{id:1192,attributes:{title:"Population Averages",slug:"pop-averages",duration:iq,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience68RecommendationAlgorithm101UsingPopAverages3",objective:"Finish off learning about recommendation system algorithms using population averages",englishTranscript:"You are thinking through this carefully, and playing a bit of devil's advocate at this stage. Is there any situation where the solution that we just found is not good? Can you be too conservative? To answer this, let us consider an example. Suppose a brilliant but lesser known chef opens up a new restaurant. Everyone who goes to the restaurant raves about it. In a few weeks it gets four reviews, few genuine, four reviews on Yelp which are all five stars. In this case, double correction would suggest that the score of this restaurant should be, well, 5, which is the average. Minus correction, which is 2.5 \u002F square root of number of samples, here it being 4.\n\nSo it turns out to be, if you do the calculation, 3.75. Well, this would mean that when we order restaurants by this adjusted score. This new restaurant is potentially unlikely to show up in even top-ten restaurants. Because there would be many veteran restaurants with average score of 4. Will this prevent this restaurant from ever becoming a top restaurant on Yelp? No, not really. Since hopefully over time enough people will visit the restaurant. And then this would naturally bring it to the top because correction would become smaller and smaller. On the other hand, our conservative correction will prevent it to be discovered early on by various Yelp users. And this would make it longer for this restaurant to become popular. So this suggests that our conscious conservative correction is preventing some kind of exploration among users early on. If we wanted to encourage some form of exploration then we should do optimistic correction rather than the conservative correction. That is the adjusted score should be average rating plus, not minus, plus, constant \u002F square root of # of reviews. Is that it? Are there other corrections that can benefit scores of restaurants at the population level? Well, for one, the freshness of ratings or reviews is important. This is especially important in the context of restaurants.\n\nAs time progresses, the restaurant can become better or worse, depending on many factors. Including changes in chef, management, or whatnot. Therefore, it may make sense to put more weight on recent restaurant ratings, compared to those further in the past. So how do we account for this? In computing average rating, we shall assign weights to the ratings. You see, before, we assigned uniform weight to each review or rating. But now, we can assign weight to each review or rating depending upon its age. For example, if we think of half-life, remember half life the way we think of in nuclear physics? Let's think of a half-life of a restaurant as 3 months. That is, this is time scale at which changes in restaurants become relevant. Then we can define normalized age of review as actual age of review \u002F 3 months.\n\nSo for example, if the restaurant had a rating 12 months back, then the normalized age would be 4. And then the weight of the review would be exponential of (-4). Says that as age increases, the rate decreases. Now, the new weighted average score would be computed as follows. (weight review 1 x score review 1) + (weight review 2 x score review 2)... \u002F weight of review 1 + weigh review 2... Okay? Again, remember previously weights were uniform, that is, equal to one. Now we are gonna change that. And correction in this average score would be given by, as before, a constant divided by square root. But not square root of just number of samples. But a weighted sum of the reviews. That is, weight review 1 + weight review 2..., there's a small twist. We will also divide this number by the maximum of the weights among all end reviews. Again, if you remember when weights of all reviews were equal in the earlier setting. This will be nothing but the square root of number of samples. In summary, in this section, we learned the following. The average rating or score is an excellent proxy of the restaurant's true score.\n\nThat is, we learned the algorithm of population averages. Now, due to a limited number of reviews or ratings in the context of restaurants, it needed an adjustment. Conservative adjustments suggest reducing rating depending upon the number of reviews. Fewer reviews leading to more reduction. More reviews leading to smaller reduction. Optimistic adjustments suggest increasing rating depending upon the number of reviews. With fewer reviews leading to more increase. The freshness of ratings is very important to take into account. Weigh recent ratings more compared to older ratings. So this is the end of the section on how to use population averages. In the next section, we are going to understand how to use population averages when data is in the form of comparisons. Thank you.",createdAt:my,updatedAt:my,publishedAt:"2023-03-16T01:24:28.017Z"}},{id:1191,attributes:{title:"Ranking Using Population Comparisons",slug:"ranking-using-population-comparisons",duration:mz,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience69RecommendationAlgorithm101UsingPopComparisons1",objective:"You just finished learning about the first type of recommendation algorithm. Learn about second involving ranking using population comparisons",englishTranscript:"Today we are going to discuss our second recommendation algorithm of how to obtain ranking using population comparisons. In the last section, we discussed our first recommendation algorithm based on population averages. It was super easy. In the context of restaurants, we assigned each restaurant a score that is equal to the average value of all the ratings it has received and then used that to rank them. We discussed various adjustments to it, but in a nut shell that was it. In many scenarios, we do not have access to ratings and even if we might have ratings, we might not want to use them as is. This section discusses such widely present scenario. When preference data is available in form of comparisons. Consider your favorite sport, say American football. Well you know where the ball is kept in your hand during entire period rather than by foot. Or actual football, or soccer. Tennis, cricket, chess, or even baseball. In any of these sports, the ultimate question that we all as fans are trying to answer is, how can we rank teams or individual players based on the outcomes of games? The games are always played between two teams, like the New England Patriots and New York Giants. Or two players, like Roger Federer and Andy Murray.\n\nThe outcome of the games is usually one of the teams, or players, wins, and the other loses. This is precisely a comparison between two teams, or players. For example, if Federer defeats Murray, that it is a comparison between Federer and Murray, in favor of Federer. Or if Patriots defeat the Giants, then it is a comparison between the Patriots and Giants, in favor of Patriots. Given all these comparisons or outcomes of games, we want to stitch them together across teams or players to eventually obtain a ranking or ordering between all of them. That is what happens in any sports championship or tournament, such as US Open tennis tournament. Clearly this is like the situation where we wanted to rank all restaurants. But instead here, we want to rank them based on comparisons rather than ratings as before. Comparisons are not like ratings. There is no easy way to take population averages of them. This requires some careful thinking. That is exactly what we will do in this section. Before we go down the part of figuring out how to rank teams or players based on outcome of games, it is worth pointing out that this view is very useful even in ranking restaurants. When we have ratings for restaurants. Recall the restaurant scenario. Each restaurant was given ratings by different patrons who visited them and were kind enough to provide their views. It is just like re-watching movies and then rating the movies. Now the problem with rating is that it is phenomenally subjective.\n\nYou and I might like food in restaurant equally but I might be stingy in giving ratings and you might be very liberal. What that means is that rating of four that you give, is different from the rating of four I give. Or depending on the mood of the day, I might be more or less liberal in giving my rating. In a sense, there is nothing absolute about ratings. To explain this important nuance, I would like to play a small game with you. Suppose I give you a blue color square and ask you, tell me the hexadecimal code of this. You'll wonder what is that? This is like me asking you to rate the restaurant or my orthopedic surgeon asking me during my visit, on a scale of 1 to 10, please rate your pain. How do I know what nine means? Or for that matter, I'm not even sure what is the difference between eight and nine. And I wonder every time I'm asked this question, should I tell him a high number so I can get his attention? Well, a better situation is going to an optometrist who asked you, compare this scenario versus that scenario.\n\nWhich one provides you better vision? And through a sequence of such comparisons, we find the perfect eye power leading to somebody like me, able to see clearly and without any headache talking to you. This is like asking question. Which one of two different blue squares is more blue? It's very easy to answer for you and me. And it is definitely not more absolute than asking for ratings. Back to setting up restaurants. If we are asked to compare two restaurants, when asked preference for West Bridge or Catalyst, it means the same to me and the same to you. And therefore, we can use comparison data between restaurants to rank them as well. But this is the place where you say wait, Yelp has collected ratings on restaurants rather than comparisons. Now what to do? No worries. Ratings provide comparisons. Suppose I have rated West Bridge four-star and Catalyst three-star. Then I have implicitly compared West Bridge and Catalyst, in favor of West Bridge. Okay so we are all comfortable with the fact that even in the restaurant setting, we have comparisons even though it was not obvious to start with.",createdAt:mA,updatedAt:mA,publishedAt:"2023-03-16T01:24:26.814Z"}},{id:1190,attributes:{title:"Ranking Algorithm",slug:"ranking-algorithm",duration:"4:04",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience610RecommendationAlgorithm101UsingPopComparisons2",objective:"Continue to learn about the ranking recommendation algorithm",englishTranscript:"Now that we have a bag of comparisons, between players, teams, restaurants, movies, or what not, we want to come up with rankings between all of them. This is a tricky question. This question has been a difficult puzzle for centuries. The essence of the difficulty was captured in the so-called Condorcet paradox, which is named after a French philosopher and political scientist named Marquis de Condorcet. In late 1700s, around the time of French Revolution, he was one of the leading thinkers in understanding what is the best way to elect leaders in a democratic society. Naturally, in elections, we are trying to elect leaders by ranking, or ordering them collectively through our votes. In presidential elections in US, we cast vote for our favorite candidate only. But suppose instead, we were allowed to cast votes in form of comparisons.\n\nThat is, votes would consist of comparisons between pair of candidates, precisely the setting we have been discussing. And this is the setting that Condorcet was talking about. He considered the following hypothetical scenario. Let's say there are three options, A, B, and C. We can think of these options as candidates, or restaurants, or sports teams. Now suppose we have three votes, vote one, A's preferred over B, vote two, B's preferred over C, vote three, C's preferred over A. If we look at only first two votes, it naturally suggests that A's preferred over B and B's preferred over C. So that means the ranking should be A be first, B be second, C be third, but the last vote contradicts this. Similarly, if we take any two votes and try to order options, we will now perfect ordering, but the remaining third vote will contradict it. This is a paradox, because we can not order them in any meaningful way, and this is what makes ranking really hard. At this stage, if you are wondering whether modern political science or the so-called social choice theory has anything to do with our world of ranking and recommendations, you are right on the money.\n\nThere has been a very long intellectual history in social and political sciences, starting early-1900s. And lately, statistics computer science, and mathematics have been active in this topic due to various modern questions, such as the topic of our discussion today. I would suggest an interested listener to read up a few notable works. One, A Law of Comparative Judgement by Louis Thurstone, published in 1927 in Psychology Review. Two, Social Choice and Individual Values by Kenneth Arrow published in 1951, and republished in 1963. Back to our Condorcet paradox, clearly it applies to our setting, and hence we need to resolve it. So how to resolve it? Well, one way to resolve this, is to try to assign scores to each option. The scores in the setting, like above, will be equal for all options. That will be successful resolution of the paradox for the particular three vote scenario. And when it will make sense, the scores might be different for different options, leading to a global ranking. Let's start with one such simple approach. To recall, we are given a collection of votes or opinions in form of comparisons. Given this, we want to assign scores to all options, and here's how we're gonna do it. The score of an option equal to the number of votes are comparison in which it is a winner, divided by the number of votes of comparison in which it is present, as either winner or loser. Use this score to rank the options. The option with highest score is top ranked. For option A, it participates in two votes of which it wins in one, therefore it's score is 1\u002F2. Similarly, a score of B and C will be half, and this resolves that paradox. Nobody gets a higher score than other.",createdAt:mB,updatedAt:mB,publishedAt:"2023-03-16T01:24:25.672Z"}},{id:1189,attributes:{title:"Scoring Rule",slug:"scoring-rule",duration:"5:34",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience611RecommendationAlgorithm101UsingPopComparisons3",objective:"Learn about how the scoring rule affects the algorithm",englishTranscript:"Let us understand the scoring rule. For this, let us consider a scenario where we have only two options, A and B. That could be candidates in an election, restaurants, or sports teams. Imagine what you are excited about right now. I'm gonna think of A and B as sports teams. Suppose A and B play ten games between them. Out of these ten games, A wins nine and B wins one. So what should the scores for A and B be? If you follow about simple rule that I described, it would suggest that A won 90% of the games and hence its score should be 0.9. B won 10% of its games,so its score should be 0.1, simple. But what if they A had won all ten games. Should we be giving score of one to A and zero to B? Think about it. This doesn't make sense because implicitly it seemed to suggest that B has no chance of winning against A, ever. It is true that A is pretty good compared to B. Self explanatory in A winning all the ten games. But that does not rule out possibility of B not winning once in future. It is worth reminding us of our favorite coin example. One where we think of outcomes of games between A and B is the outcomes of tossing a coin. Every time A and B play a game. We toss a coin. If toss comes up head, A wins. If toss comes up tail, B wins. Then we're asking the question that if we have tossed coin ten times and it has turned up head all the time, does that mean that the bias of the coin is one? No.\n\nRemember We can have a small chance of this happening even when bias of coin might be much smaller than one. And more so if bias of coin is close to one. For example, if bias of our coin was 0.99 then it will take, on average, 100 coin tosses to see even one tail. Putting it other way, whether your coin has bias of 0.990 or 0.995, You would most likely see the first ten outcomes as heads in either case. And so no way you can differentiate between these two scenarios with limited data. And the reason why we should not assign one to A and zero to B when we see that A has won all ten games. All right, so we know that we cannot do that. But then what shall we do? Well, like most simple questions in life, this question has a fascinating history too. Let me ask this question another way. What is the chance of Sun rising tomorrow? Let me try to explain this with the following mind experiment. Suppose every day in the morning before Sun rises, I'm sitting and tossing a coin.\n\nThe coin turns up head, we see sun rising. If coin turns up tail, we see sun not rising. For age of Earth which is roughly 1.6 trillion days, give or take, we have seen sun rising every day. So we have seen outcome of coin being head 1.6 trillion times. Now what is the chance that Sun will not rise tomorrow? That is, can we estimate bias of the coin based on our observations thus far? Like setting off A and B, where A has won all games, we can not rule out a small chance of Sun not rising. So the question is, how small is it? Pierre-Simon Laplace, a French mathematician and physicist asked this very same question in the 18th century. He suggests a very simple answer. Well, in one extreme case, we will see that sun will not rise tomorrow then why not we go with that as our observation and open the biased estimate? That is we add a correction of plus 1 to both of our observations, that is, number of times sun has risen and number of times sun has not risen. Or number of times A has won and the number of times B has won. And use them to obtain the bias estimation. In the case of our example, we will take the true outcomes of A's loss and wins and add plus one to each. So in this example, where we had A winning ten games and A losing zero games, you will define the score of A as 10 plus 1 which is corrected victories divided by 10 plus 2, which is corrected total number of games it has played. That is, 11 by 12, rather than 1.0. And this will give the score of B similarly 1 by 12, that's it. Effectively each option sports team or restaurant is given score equal to the fraction of games it wins, with the added plus one correction to allow for error introduced due to small sample size as we just discussed.\n\nThis is a reasonable scoring rule. What is so tricky about it? It's like saying that a team score is equal to the proportion of the games it wins. Hm, sounds too simple isn't it? Because if so you would like your team to keep playing with the weakest team in the league and hence it keeps winning all the time. Not good or a robust code. Defeating a player ranked number one should definitely count for more than defeating a player with thousands ranked multiple times. In some sense, we want to have a weighted scoring. You want to weigh each victory, depending upon whom you have defeated. And then average. Great, a lot of fun, we have covered so much history while dealing with games, coins, and comparisons. Is there any hope of having more excitement in the same section today? Well, if you doubted that, you were wrong. We have a lot more to discuss before we end this section.",createdAt:mC,updatedAt:mC,publishedAt:"2023-03-16T01:24:24.237Z"}},{id:1188,attributes:{title:"Recommendations Overview",slug:"recommendation-overview",duration:"8:49",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience612RecommendationAlgorithm101UsingPopComparisons4",objective:"Finish off learning about recommendation system algorithms using population comparisons and ranking",englishTranscript:"The big question is how do decide these weights. To answer this question, let us take another look at the same example of two options. We have A and B as our two options. They help play 10 games out of which A has won 9, B has won 1. The character numbers are 9 plus 1 equals 10 in favor of A, 1 plus 1 equals to 2 in favor of B. The scores were, therefore, 10 by 12 in favor of A, 2 by 12 in favor of B. There's another way to get to the same answer. Let us design a dynamical system to compute our scores. Think of a scoring machine if you like. We have two state A and B at each time we will be either be standing in state A or standing in state B. At each time, we decide to stay where we are or we decide to change our state at random. The rule of this random movements are as follows. If you are at state A, then we stay at state A, with probability 10 by 12, or we go to the B, with probability 2 by 12. If we are at state B, on the other hand, then we stay at B with probability 2 by 12, and go to A with probability 10 by 12. We run this experiment for a long time and observe the fraction of times we are in state A or fraction of times we are in state B.\n\nThese fractions are scores of A and B of respectively. Effectively we have designed what is called a Markov Chain, over two states, A and B. A transition rules between these states were designed so that the fraction of time spent in A and the fraction of time spent in state B, in the long term, but precisely equal to 10 by 12 and 2 by 12 respectively. If we carefully examine the transition rule it suggests that fraction of time spent in state A equals 10 over 12 times fractional time spent in A plus 10 over 12 times fractional time spent in B. Since fractional time spent in A plus fractional time spent in B is equal to 1 by definition, we have that fractional time spent in A is equal to 10 by 12, voila. Now, this gives us a general approach to design weights we were looking for. In general, there are more than two options. To explain this, and it starts with simple case of three options first. A, B, and C, how creative? We can create a graph of these three nodes, a node each corresponding to each options. As before, A and B have played 10 games, of which A has won 9, B has won 1. For B and C, they have played 4 games, out of which each one of them have won 2 each. For A and C, they have played 6 games, of which C has won 4, A has won 2. As before, we can set pair wise corrected proportions for each of them. A versus B would be 10 over 12 in favor of A. B versus C, 3 over 6 in favor of B. C versus A, 5 over 8 in favor of C. All right, so we've got our data.\n\nAs before, we want to design a dynamical system, a scoring machine. Where we can work between states A, B, and C. At each state, the rules of the transition to the next state are determined by which state we are standing in. And the other state to which we want to jump. This utilizes the outcomes of pairwise games. For example, if we are currently at state B, next time we will be at state A, proportional to 10 by 12. And at C, proportional to 3 by 6. That is how often A defeated B and therefore, we will go from B to A proportion to that number. And that is how often C defeated B and that is why we go from B to C that often. To make sure that these numbers always add up to less than 1, we will multiply all of these numbers by a third. This normalization number that is one-third here is obtained by looking at maximum number of neighbors, any node or any state has in this graph. In our case, all 3 nodes are connected, and hence, it's 3. Once we have this, we get the following effective transitions. When we are at B, in the next time, we go to A. The chance 10 by 12 times one-third, which is 5 over 18. We go to C, one-half times one-third which is one-sixth. And then we remain at B rest of the time which turns out to be 10 over 18. The similar manner, we can design rules for A and C. In particular, if we look at transitions from A to B and C to B the transition rate should turn out to be 1 over 18 and 1 over 6 respectively. This gives us a weighted formula. In the long run, the fraction of time spent in B is equal to 10 over 18 times fraction of time spend in B plus 1 over 18 times fraction of time spent in A plus 1 over 6 times fraction of time spent in C. That is, rearranging this term, we obtain fraction of time spent in B equals fraction of time sent in A plus 3 times fraction of time spent in C all divided by 8. Similarly, looking at A and C, we get two more such equations that relates to fraction of time spent in each of these states. The weights are coming from the relative wins and losses between the options or teams. These fractions will be the scores of options. Examining the equation for score of B again, score of B equals to, one-eighth times score of A plus three-eighth times score of C. This score naturally incorporates the fact that score of B is 3 by 8 times the score of C and 1 by 8 times the score of A.\n\nThe weights are coming from how often B has been relatively victorious over A and C. And if indeed score of C will be high, then score of B will be high, and vice versa. As we can see, when there are more than three options, the rules can be defined in a very, very similar manner. The resulting ranking algorithm that we just learned is called Rank Centrality. Recapping the algorithm, it is computing scores as follows. We have a network of states. Each corresponds to possible options such as sports team or restaurants. A pair of nodes have connections between each other if they have been compared once or more. At each time step, we are at one of these nodes in our machine. And we transition to any of the neighboring state next time but probability that is proportion to how often that neighbor has defeated us, the current state. So we are likely to go towards the winners more. And less likely to go towards losers. And then we assign the score of each node as a fraction of time we spent in that particular node as part as dynamics in a long term. In the language of Markov chain, this fraction of time is called the steady state distribution of this design, the Markov chain. Given this algorithm, a restaurant that is new, but has been compared very favorably to an old, well known Michelin three star restaurant might have a very high score and thus resulting into really meaningful scores.\n\nIn summary, in this section, we discussed the following topic. In many scenarios, the preference data available is not necessarily with ratings, it is comparisons such as those in sports. Comparisons are more absolute way to capture preferences compared to ratings. Comparisons can be easily derived from ratings. When preferences are comparisons, obtaining global ranking from them is not easy. Condorcet's paradox is a nice example that explains this difficulty. We discussed a simple fraction of wins as a scoring rule to decide ranking. Naturally, it turned out to be too simple and it needs a weight adjustment. We used a dynamic system, a machine, Also known as Markov Chain in probability to design such an adjustment. The algorithm that we describe is known as the Rank Centrality. In the process, we also touched upon the questions of, what is the chance that sun will rise tomorrow? And if it indeed does, we will see you in the next section and we'll discuss how to move beyond the simple algorithm that is just based on population view, and start making algorithms really personalized. Thank you.",createdAt:mD,updatedAt:mD,publishedAt:"2023-03-16T01:24:22.794Z"}}]},instructors:[{id:635,name:ir,position:a,nationality:a,achievements:a,focus:a,bio:is,learnMoreURL:a},{id:636,name:it,position:a,nationality:a,achievements:a,focus:a,bio:iu,learnMoreURL:a}]}},{id:aR,attributes:{title:"Collaborative Filtering and Personalized Recommendations",slug:"collaborative-filtering-and-personalized-recommendations",description:"Learn the limitations of traditional preditction and the fundamentals of personalized recommendations. Also learn the many variations such as the use of side information, dynamic models or active models to develop even more accurate recommendation systems.",prerequisite:a,duration:"1 hr 20 min",objective:a,createdAt:"2023-10-18T21:49:04.553Z",updatedAt:"2023-10-18T21:49:45.457Z",publishedAt:"2023-03-16T13:12:57.100Z",chapters:{data:[{id:1187,attributes:{title:"What is Collaborative Filtering?",slug:"what-is-collaborative-filtering",duration:"4:10",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience71PersonalizingUsingCollaborativeFilteringUsers1",objective:"Learn what collaborative filtering is with regards to the users themselves.",englishTranscript:"Recall that our problem is to fill out a giant matrix using only a tiny fraction of its entries. In the case of the Netflix Prize data set, each row corresponds to a user and each column corresponds to a movie. Here's Frank. And here are the movies that Frank rated. Here's the movie Dirty Dancing that Frank rated 5. In the previous video, [INAUDIBLE] showed us a method to rank items or movies from noisy observations of their ratings or scores. One of the crucial assumptions that he made was that there was only one single score for each item and, therefore, one single ranking. Assume, for example, that the 500,000 users in the Netflix data set are all of the same type as Frank. So let's call them Frank 1, Frank 2, Frank 3, et cetera.\n\nThey would give the following ratings for the movie Pulp Fiction, say. Frank 1 would rate it a 4. Frank 2 would also rate it a 4. Frank 3 would rate it a 5. Frank 4 would rate it a 3, et cetera. So rather than thinking about all these Franks, we might as well think about Average Frank, whose rating is 4 plus 4 plus 5 plus 3, et cetera, divided by the total number of Franks, which is 500,000. This gives the average score for the Franks, say, 4.1. So we could then fill out all the missing entries for the movie Pulp Fiction by entering 4.1 for all users. And this would be good, if all users had essentially the same opinion of Pulp Fiction, apart from small fluctuations. This approach might be good enough for Yelp. After all, we're all comfortable with the idea that there's a universal skill of good food and culinary guides, such as the Michelin Guide or Zagat are built on this very premise.\n\nMore generally, we live in a society that's obsessed with rankings. And the very notions of top 100 places to live or top 10 party schools implicitly assumed that everyone has the same preferences in these matters. Of course, these notions are very subjective. And this is where personalisation enters the picture. Believe me when I tell you that my preferences are very different from Frank's preferences. I don't like Dirty Dancing. I'm more of a Jerry Maguire fan myself. But if anyone asks, I'm into action movies. I like my Bourne trilogy. Now, let's assume that Netflix has only two types of users. [? Half ?] of them are versions of Frank, who love Dirty Dancing and hate the Bourne Identity. And the other half are versions of me who have exactly the opposite movie tastes. If there are only action movies or chick flicks, then the full matrix would now look like this.\n\nHere, the matrix has been color coded for better visualization. A darker color means a higher rating. With the extra fluctuations that account for small variability among users in movies, it would look more like this. The low ratings would be a bunch of numbers closer to 1. And the high ratings would be a bunch of numbers closer to 5. This picture is somewhat misleading, though. You get such a matrix when the first rows correspond to users similar to Frank and the first columns correspond to action movies. In reality, the rows and columns are in a scrambled order. And even in this simple case, two types of users, Frank and me, two types of movies, chick flick and action, the picture is much more complicated and looks more like this. This picture actually represents the matrix that we're trying to reconstruct. But remember that we don't observe most of the ratings.\n\nThere are only 1.2% of the entries of the Netflix matrix that are known. And what we actually have to work with is this. In this matrix, white means that we do not observe the entry. From this mostly empty matrix, our goal is to recover the original checkerboard pattern. Good luck with that. It turns out that a technique called collaborative filtering allows us to solve this problem using a basic machine learning method called nearest neighbors.",createdAt:mE,updatedAt:mE,publishedAt:"2023-03-16T01:24:21.347Z"}},{id:1186,attributes:{title:"Nearest Neighbors",slug:"nearest-neighbors",duration:mz,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience72PersonalizingUsingCollaborativeFilteringUsers2",objective:"Now that you know what collaborative filtering is, learn how nearest neighbors helps with that.",englishTranscript:"Recall that each row of our data matrix corresponds to one of the two types of users. Even if we observe only very few ratings per row, they should be similar for users of the same type. To see this, let's take a closer look at the similarity between rows. It should reflect the similarity between users' preferences. For each pair of rows, we can measure the similarity by looking at their inner product. This notion comes from linear algebra, and it's a single number that can be computed as follows. First, stack the rows on top of each other. Here we only show rows of size 10 rather than 17,000 for the sake of demonstration. Then we multiply each entry of the top row by the corresponding entry of the bottom row, and add these products. Of course, question mark times four is not well-defined, so we simply replace the question marks by the average rating three.\n\nWhen we multiply each corresponding pair of numbers, the one on the top with the one on the bottom, we get this new sequence of 10 numbers. Finally, we add all these numbers to get 101. [INAUDIBLE] the inner product between the top row and the bottom row is 101. What does this number mean? Well, it's a matter of proximity between the rows. So the larger the inner product, the more similar the rows. Back to our Netflix example, a large inner product between two rows means that the corresponding users have similar preferences. In the [? toy ?] example that we've just constructed where there are two types of users, Frank and me, and two types of movies, chick flicks and action movies. let's see what these inner products look like. We could of course write a long list of inner products, but there are a lot of them.\n\nActually, this number is the number of distinct pair of rows, right? For each pair of distinct rows, we can compute a new inner product between them. For n rows this number of pairs is given by the formula n times n minus 1 divided by 2. So for n equals 500,000 rows, as in the Netflix prize data set, that number would be around $125 billion, which is about the number of stars in our galaxy. So a list of all inner products would be pretty useless, but we can get a good summary of this list using a histogram. In our [? toy ?] example, this is what a histogram looks like. We can clearly see that there are two different modes. This suggests that there are two types of values for these inner products, large and small. When two rows come from the same user type, we get a large value. When they come from two different user types, we get a small value for this inner product.\n\nThis suggest the following picture. The blue dots correspond to users of the first type. One that are into chick flicks like Frank, and the red dots corresponds to users that are into action movies, like me. Of course, this two dimensional picture is only a cartoon. In reality, this picture takes place in a 17,000 dimensional space. 17,000 is the length of each row in the Netflix data set. Moreover, the clusters may overlap like this. Now that we have made this observation, we can devise a personalized recommendation system. The idea is simple and consists of using well-known machine learning technique called nearest neighbors. Since users are clustered into two groups, even if they are not exactly the same, the ones corresponding to similar rows are likely to be users of the same type. Assume that Frank is here.\n\nThen all the users that are in a certain radius are of the same type as Frank, chick flick fans. and their ratings may be averaged without risking to include the ratings of action movie fans. And that's personalization. Here all recommendations will be personalized to each user type, either chick flick fan or action movie fan. In other words, all the rows that have a large inner product with Frank's are likely to have preferences similar to Frank's. Once we have identified the set of users that are similar to Frank, we can apply the techniques that we saw in the previous video, because for these users there is only one single true rating for all movies. This rating can be used to output a single ranking. It is believed that the Netflix data set has at most 15 different types of users. In a way this is not such a large number, but it's certainly more than two. We will see a straightforward extension to more than two groups as part of the next video. This video was about personalizing recommendations. We saw an example where the data contains at most two types of users. In this toy example, we saw how to identify users that are likely to belong to the same group by looking at inner products. Inner products between the rows of the data matrix are a measure of proximity between users. Finally we grouped users according to their proximity. This is the nearest neighbor technique, and we compute in an average rating within each homogeneous group. Effectively, we have personalized our recommendation to each group.",createdAt:mF,updatedAt:mF,publishedAt:"2023-03-16T01:24:20.255Z"}},{id:1185,attributes:{title:"Item-Based Personalization",slug:"item-based-personalization",duration:"4:17",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience73PersonalizingUsingCollaborativeFilteringUsers3",objective:"Now that you have seen collaborative filtering using similar users, learn about the technique with regards to the items themselves",englishTranscript:"When you think of personalization,you think of user personalization. Each user should have his own recommendation, right? We know exactly how to do that. Group users according to similar preferences and fill the gaps based on users that are close to you. In the Netflix example, items were movies. To measure similarity between users we compute inner products between two profiles of users, that is two rows in the matrix. The main limitation of this approach is that these rows are mostly empty. Each user has rated a very small fraction of movies. We end up multiplying a lot of true ratings by our artificial rating, three, that we use as a place holder for empty entries.\n\nRecall this example. Here, about half of the entries were missing, the red entries, and that's where we put a three. In practice we color code black for a present entry and red for missing entry. The picture looks much more like this. It's a drop of black in an ocean of red. This has pretty drastic consequences when we're taking inner products. Let's try to align two such rows on top of each other. As you can see, there's very few overlap between the black present entries. To measure the extent of this problem, we need a better understanding of how missing entries are distributed across the matrix. How are these entries field and spread across the movies? Did some users rate more movies, or did they all rate pretty much the same number of movies? Were some movies rated much more than others? To answer these questions, let us look at some data.\n\nLet us start with the number of ratings per users. As we can see, it has a really long tail. A few users rated up to 40,000 movies, but most of them rated a much smaller number of movies. This image is consistent with having each user rate each movie at random with a probability of 1.2% independently across users. In technical terms, this looks like a poisson distribution. How about the number of ratings per movies? This is essentially the same phenomenon. A few movies have received up to 250,000 ratings, but most of them have received very few ratings. The most rated movies include Miss Congeniality and Independence Day for example. The impact of the second plot is much more drastic though. Indeed, if a few movies have a lot of ratings then they're probably popular, which in turn means that they are likely to be watched and then rated.\n\nTherefore, it makes sense to focus our efforts on these movies. For these movies, we have a lot of ratings. And therefore, we can take the inner products between columns rather than rows. It has a significant advantage over taking in products between rows. There are many more collisions. Let's make a quick calculation. The proportion of entries that are filled for the movies Independence Day and Miss Congeniality are about 250,000\u002F500,000, which is about one half. Therefore, if we place the two columns next tot each other and compute their inner product, we an estimate the number of collisions as follows.\n\nWe have 500,000 times one half for the first column and one half for the second column, which is 125,000 collisions. That's a lot, and the good news is that we'll get a large number for most of the popular movies. In turn, this means that we'll have a much more accurate measure of similarity between popular movies. So rather than doing user personalization, we can do item personalization. And it's exactly the same picture as before. If Dirty Dancing gets ratings that are similar to Ghost, for example, then the missing entries for Ghost should be close to the entries for Dirty Dancing. As we'll see in the case studies, this item based personalization is much more predictive than the user based ones. For exactly the reason that we mentioned before. The movies that really matter have a lot of ratings.",createdAt:mG,updatedAt:mG,publishedAt:"2023-03-16T01:24:19.039Z"}},{id:1184,attributes:{title:"Collaborative Filtering by Similar Users & Items",slug:"collaborative-filtering-by-similar-users-and-items",duration:"6:26",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience74CollaborativeFilteringbySimilarUsersItems1",objective:"Combine both what you know about collaborative filtering of users and items together",englishTranscript:"So we've seen how to personalize recommendations by either grouping users according to their past ratings or by grouping items according to the past ratings they have received. I mentioned briefly that the item-based personalization is better in practice. However, there's something that's potentially even better-- combining both approaches into one recommendation algorithm that's both item- and user-based. There might be something very specific about why Frank likes Dirty Dancing that comes from both the fact that he's Frank and the fact that the movie is Dirty Dancing. It's the user-item combination that really matters here.\n\nTo see how we can leverage this rough idea, let's go back to our geometric picture. Remember, when we talked about user-based recommendations, we thought of users as being clusters in some space. In the case of user-item based recommendation, this is also the kind of picture that we need to see. However, it does not take place in the same space. We need to find a space where we can see the pairs of user items. This is the picture we had for user-based recommendations. This idea is nice in principle, but it looks great only in two dimensions-- the x-axis and the y-axis. In reality, this picture takes place in a very high-dimensional space. It has as many dimensions as there are movies-- about 17,000. And there's one problem-- it becomes difficult to measure similarities accurately in high dimensions. Recall that each user-- for example, Frank-- can be seen as a row of lengths-- 17,000-- which represents all Frank's potential ratings for all the movies in the database.\n\nWe have seen that the similarity between two users is measured by the inner products between their rows. Let us look at the most central user we can think of-- the user that rates all movies a three, the average rating. Next, assume that all other users are variations around the central user. Their ratings for each movie are as follows. With probability 1\u002F2, they rate the movie a 3, like the central user. But with probability 1\u002F4, they rate the movie a 4, one point higher than the central user. And with probability 1\u002F4, they rate the movie a 2, one point lower than the central user. We can represent each new row as the sum of a row of threes for that central user and a row that contains only zeros, ones, and minus ones. Let us call the first row C for central and the second row D for difference. Now let us look at the inner product between two such rows-- C plus D-- and C plus D prime, where D and D prime are two separate, different rows taken independently at random. Let's run some simulation to see the similarities between such users. This plot is a histogram of inner products between all pairs of 10,000 rows of the form D in a product with D prime for D and D prime random.\n\nWe can see from this histogram that most of the inner products are between negative 200 and 200. This range is not very large, so inner products between users from the central cluster should stay close to the inner product between the central user and itself. This is 3 times 3 plus 3 times 3, et cetera, 17,000 times, which gives an inner product of 153,000. Therefore, 200 is negligible compared to that large number. Unfortunately, 200 is still a very large number, and it may be difficult to separate this cluster from other clusters. Indeed, we would like to think of all the users that are essentially the same as the central user up to these random variations, and therefore, as one cluster of user. Visually, this cluster may be difficult to single out because it overlaps with clusters of other users. For example, if we consider a user that rates all the movies three except for movies about dancing that are rated a four, if we assume that all the dance movies correspond to the first columns, the row corresponding to this user would look like this.\n\nNow consider the same type of variations around this user. This is our second cluster. Let's call it the dance cluster. We can create two histograms. The first one that has only the pairwise similarities between rows from the central cluster-- that is, variation around the central user and pairwise similarities between users in the central cluster and users in the dance cluster. In our simulation, we assume that there are 100 dance movies. We can see that there is a huge overlap between the two clusters. In other words, given a user and a central cluster, there are many users in the dance cluster that appear to be more similar to the central user than users in the central cluster. This is a problem.\n\nWe could overcome this problem by only looking at ratings for movies about dance. In this case, the picture would look like this. The two clusters are very well separated. Moving forward, the main question is, how can we automate the selection of dance movie without knowing it? Well, this is exactly the idea of similarity between movies. If they receive a similar rating, then chances are that they are about similar topics-- for example, dancing. User-item based recommendation is simultaneously performing this clustering of movies and users. As it turns out, the problem is completely symmetric. I could have told exactly the same story by switching the role of user and items. We have seen that because the problem is so high dimensional, it might be necessary to group or cluster users and movies simultaneously. This is the subject of user-based recommendations, our next video.",createdAt:mH,updatedAt:mH,publishedAt:"2023-03-16T01:24:17.123Z"}},{id:1183,attributes:{title:"Singular Value Decomposition",slug:"singular-value-decomposition",duration:mk,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience75CollaborativeFilteringbySimilarUsersItems2",objective:"Continue to learn more about collaboritve filtering of users and items together",englishTranscript:"Let us try to visualize what the whole matrix would look like if we were to order rows and columns in such a way that similar rows would be next to each other and similar columns would be next to each other. Visually, this amounts to going from this picture to that one. In this cartoon, there are only two blocks. But in reality, we may have more blocks, and they also may overlap. Each block corresponds to a group of users and a group of movies that are homogeneous. For example, it could be that this group of user would tend to rate this group of movies a bit higher than the rest of the movies or a bit lower.\n\nIt could also capture a bit more complicated pattern that is different from the rest of the matrix. The key mathematical observation is that a map matrix with a block structure can be decomposed as the sum of simple matrices, one for each block. If these matrices are different enough, we can efficiently recover each of them using a numerical algorithm called singular value decomposition, or SVD in short, even when the matrix is only very partially observed. Let's see what this simple structure is. For that, assume for simplicity that our matrix has only one block and that it is located on the top left of the matrix. Here, the red part is the pattern we are looking for. And the blue part is the rest. For example, the blue part correspond to only ratings from the central clusters, threes with probability 1\u002F2, fours with probability 1\u002F4, and twos also with probability 1\u002F4.\n\nHowever, assume that the red block corresponds to bikers and dance movies. As we all know, bikers have a thing for dance movies and tend to rate them a bit higher. Let's say that they rate them four with probability 1\u002F2, five with probability 1\u002F4, and three also with probability 1\u002F4. So on average, 1 point higher than the central cluster. If we call B the blue matrix, then we can actually represent the superposition of the blue and the red matrix as the sum B plus U outer V, where U is a column of the same height as B and V is a row of the same width as B. Specifically, U is a column with only 0s and 1s where the 1s indicate the user is part of the biker's group. And V is a row with only 0s and 1s where the 1s indicate the movie is about dancing. We say that U out of V is the outer product between column U and row V. We say U outer V. Unlike the inner product, the output of an outer product is a matrix rather than in a single number. It's easy to compute. The number of rows in the output matrix is the number of entries in U. And the number of columns in the output matrix is the number of entries in V.\n\nNext, the entry of the output matrix in the i-th row and j-th column is simply the product of the i-th entry of U, Ui, and the j-th entry of V, Vj. In the case of U and V, there are only 0s and 1s as we described, we get exactly a matrix with only 0 entries except for the top left corner that has only 1s in it. It is clear now that B plus U outer V corresponds to the matrix that we have described. Then we can add more blocks. B plus U1 outer V1 plus U2 outer V2 plus U3 outer V3, etc. We can add one outer product for each block. It turns out that the vectors U and V don't have to be made only of 0s and 1s, but may represent more complicated patterns, such as polarized tastes. For example, if they take value one on one group and minus one on another group. To illustrate this example, consider two groups of Netflix users, which we can safely consider to be disjoint; bikers and teenage girls. Then consider two groups of movies; '80s dancing movies and pop concert. Next, take the column vector U with 1s for the bikers and minus 1 for the teenage girls and 0 for everyone else.\n\nTake the row vector with 1 for the '80s dancing movies, minus 4 for pop concerts, and 0s for all other movies. What does the outer product, U outer V, look like? Right, it will have 1s for the pairs biker '80s dance movies and 1s for the pairs teenage girls pop concerts. It will also have minus 1s for the pairs bikers pop concerts and teenage girls '80s dance movies. This indicates that while bikers tend to like '80s dance movies more than the average Netflix customer, they tend to dislike more pop concerts than the average Netflix customer. The opposite is true for teenage girls. Of course, more complicated patterns arise in practice. Given that our matrix of interest has the decomposition of the form B plus U1 outer V1 plus U2 outer V2 plus U3 outer V3, et cetera, it turns out that it's not too hard to find the pairs Uj Vj using what is called the singular value decomposition, or SVD, as long as they are not too many such pairs, even if we only observe a small subset of the matrix. This is because the SVD is very robust to noise. In a way, each pair corresponds to a stereotypical rating pattern. If we allow too many such pairs, we have no hope to recover the underlying matrix. But in practice, these matrices are nice. It is believed that there are about 15 such pairs for the Netflix matrix. To be more precise, the Netflix matrix has probably many more pairs, but it is well approximated by such a simple matrix. In the case studies, we will see how the singular value decomposition can tell us how to identify how many pairs are relevant for a given problem.",createdAt:mI,updatedAt:mI,publishedAt:"2023-03-16T01:24:16.051Z"}},{id:1182,attributes:{title:"Using Comparisons, Rankings and User-Items",slug:"using-comparisons-rankings-and-user-items",duration:lb,videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience76UsingComparisonsRankingsandUserItems1",objective:"Start to bring together all previous knowledge to form your recommendation algorithm",englishTranscript:"Is personalization using comparison data. We have come quite far in learning about recommendation or personalization. Starting from the simplest personalization algorithm that utilized population level information. In the last few chunks, we have discussed various ways to develop highly personalized recommendation algorithms. The algorithms we discuss they're primarily doing matrix completion. I suggest you do check the part of this module on ranks centrality again, especially if you want to remind yourself how to calculate the chance of sun not rising tomorrow. Now coming to the question of interest for the day. We shall discuss this question again in the context of Netflix. There are a collection of users and movies. Each user expresses preferences over movies in the form of pairwise comparisons.\n\nImagine Netflix showing the user a pair of movies and asking user which of these two movie did she like more? Of course, Netflix already has such data, as a user might have rated few movies already. And the rating of movies implicitly provides comparison between the rated movies. For example, if I rated movie Casablanca, 5, and One Flew Over the Cuckoo's Nest, 4. Then I'm suggesting that I look Casablanca over One Flew Over The Cuckoo's Nest. Okay, so for each user, we know that she has expressed preferences in form of comparisons between a few pair of movies. From this data, for each user, we want to figure out the ranking over all the movies for which she has not provided any preferences or ranking. In the setting of matrix completion, we wanted to predict unknown ratings. Here, we want to predict the order or ranking between all the movies for which we have no preference data for each user. So clearly this seems like a much taller order than just matrix completion. I will convince you that that is not really any different from matrix completion. It is almost like matrix completion with few twists and turns. First let us transform our data in matrix form.\n\nAs before, in our matrix we have rows that correspond to users. Each column does not correspond to movies anymore. Instead, each column corresponds to an ordered pair of movies. For example, consider an entry. Let us suppose it's row corresponds to me as a user. And it column corresponds to a pair of movies. Casablanca or One Flew Over the Cuckoo's Nest. Therefore if there are N movies in total, in principle we have N multiplied by N-1 divided by 2 columns. For N equals to 3, this will turn out to be 3. For N equals to 10, this will be 45 and so on. Now each entry in the matrix is either plus 1 or minus 1. Of course, if it is missing, then we do not know what it is. But it will always be, either plus one or minus one. For example, if entry for row corresponding to me as a user, and column corresponding to Casablanca or One Flew Over The Cuckoo's Nest is equal to plus one. Then it means that, I like Casablanca over One Flew Over The Cuckoo's Nest. And -1 means that I like One Flew Over the Cuckoo's Nest over Casablanca. Therefore for each user we will have a row with some +1s and some -1 entries.\n\nThe remainder of the row will be blanks, there is unknown values. Ideally we're going to predict +1 or -1 for each of the blank entry. So that it leads to ordering of movies for which user has not expressed preferences. For example, if for movie Iron Man and Superman, I have not provided preferences. That is the entry corresponding to column, Ironman compared to Superman is unknown, we want to predict it to be plus 1 or minus 1. As we shall see, we may end up predicting it to be a number between plus 1 and minus 1 rather than a number equals to plus 1, or minus 1. And from this you would like to produce ranking over all movies with unknown preferences. Again in the example above, if there were only two movies, say Ironman and Superman, with unknown preferences we're really trying to figure out whether Ironman is preferred over Superman or the other way around? And this is easy because we can look at the predicted value which will be between minus 1 and 1. If it is greater than 0 we shall say Superman is preferred over Iron Man.\n\nIf it's less than 0 then we shall say Iron Man is preferred over Superman and that's it. The question is what happens when we have more than two movies. In a nutshell, we have two challenges. One, how to predict this value between minus 1 and 1, for each pair of movies. Two, given these predicted values, how to compute ordering, or ranking, between all the movies with unknown preferences. Short for one, we will use the matrix completion algorithm like collaborative filtering. And for two, we will use rank centrality. Nice, right? No extra work today. We will use what we have already learned. I love when this happens, and today is that day.",createdAt:mJ,updatedAt:mJ,publishedAt:"2023-03-16T01:24:14.343Z"}},{id:1181,attributes:{title:"Matrix Completion Algorithm",slug:"matrix-completion-algorithm",duration:"7:14",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience77UsingComparisonsRankingsandUserItems2",objective:"Finish off learning about how all previous methods combine",englishTranscript:"Okay, so let me start by convincing you why we can use matrix completion algorithm like collaborative filtering. Note that after the transformation we did with our data, we have a matrix with unknown entries being plus one or minus one. The unknown entries that we want to predict is really matrix completion problem for that reason. There is, recall, one such algorithm, user-user collaborative filtering. Suppose you and I are two users. We have provided preferences for a collection of movies.\n\nSome movies for which you have provided preference but I have not, some movies for which I have provided preference but you have not, and some movies for which you and I share preferences. Let us suppose there are three movies for which both you and I have provided preferences. Let us say those three movies are A, B, and C. The corresponding pairs would be A compared with B, B compared with C, and C compared with A. These correspond to three different columns in our matrix. For each of these columns, we have provided values of plus 1 or -1. I like A over B, I like B over C, and I like A over C. On the other hand, you might have liked A over B, C over B, and A over C. Looking at the overlap of these three columns, it seems that you and I agree on two out of these three entries. After computing similarity using cosine of our vectors restricted to the overlap of these three columns, we get similarity of 1 over 3.\n\nNow suppose you have provided ranking of Iron Man preferred over Superman, but my ranking for Iron Man versus Superman is unknown. Then we can use similarity between you and me to transport your plus 1, that is, you liking Iron Man over Superman, to my setting with the similarity rate of one-third that we just computed. Now suppose in addition, there is our common friend, say, Philippe, who has also provided preferences over A, B, and C, then his similarity to myself is -1\u002F3. And now suppose Philippe does not like Iron Man over Superman, that is, he has given preference for Iron Man compared to Superman as -1, then we can use his -1 with weight of -1\u002F3. Putting all of these things together, we obtain prediction as follows. 1\u002F3 for similarity between you and me, multiplied by plus 1 coming from you, plus -1\u002F3 for similarity between Philippe and me, times -1 coming from Philippe's preferences.\n\nThis whole thing divided by 2\u002F3 comes out to be 1. That is, our prediction is plus 1. That's nice. Effectively, you are like me, and you like Iron Man. On the other hand, Philippe is not like me, and he did not like Iron Man. And hence, when we combine both of these weighted preferences together, they end up suggesting that Iron Man is preferred over Superman. That is plus 1. But now suppose Philippe actually had liked Iron Man, and Philippe's similarity to me was -1 rather than -1\u002F3. Then, the prediction would be, just quickly recalling the same calculation with different numbers, is 1\u002F3 times plus 1 plus -1 times plus 1, and this whole thing divided by 4\u002F3. This after a simple calculation will turn out to be -1\u002F2. This example provides intuition why we will see predicted values that are not necessarily plus 1 and -1, but values most likely in between.\n\nThis example also explains how collaborative filtering like matrix completion algorithm can be directly used in our setting. Now we will address the second challenge of obtaining ranking for movies using these predicted scores for movie pairs. Let us look at any two movie pairs, say Iron Man and Superman as before. The score of -1\u002F2 between them suggests that Superman is preferred over Iron Man. One way to interpret this is in the form of random outcome of games. To understand what -1\u002F2 means, let us consider special values of -1, 0, and plus 1 first. -1 means that Iron Man always loses compared to Superman. Plus 1 means Iron Man always wins. And 0 means that Iron Man wins half of the time, Superman wins half of the time. This suggests that if a predicted number between -1 and 1 is, say, x, then it can be thought of as tossing a coin with bias B, where B is equal to 1+x\u002F2. When outcome of coin toss is head, which happens with probability B, Iron Man wins. When outcome is tail, Iron Man loses. Therefore, we can convert each of the predicted value for a given user row by adding plus 1 to it and then dividing the resulting value by 2. This gives us probability of one of the movie winning against the other movie amongst the pair of the movies that the column corresponds to.\n\nIf it was minus half, then B equals to one-quarter, that is, Iron Man wins only 25% of the time against Superman. Now we are back to the setting where we have a collection of movies, and between a pair of these movies, we know what fraction of time one movie wins over another movie. This is exactly the setting for which rank centrality was designed, to obtain global ranking. That means that we can now apply rank centrality over all movies and obtain a global ranking among them for each user separately. And this gives us personalized ranking for movies. In summary, we discussed how to obtain personalized ranking when preference data is available in form of comparisons rather than ratings. There is nothing new about the algorithm. It is about putting two algorithms that we have learned earlier, matrix completion algorithm like collaborative filtering and rank centrality for obtaining global ranking between objects using pairwise comparison data. Next, we shall discuss other approaches for personalization beyond what we have already discussed. Thank you.",createdAt:mK,updatedAt:mK,publishedAt:"2023-03-16T01:24:12.258Z"}},{id:1180,attributes:{title:"Graphical Models and Neural Networks",slug:"graphical-models-and-neural-networks",duration:"6:08",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience78GraphicalModelsandNeuralNetworks1",objective:"Learn about some more complicated methods of recommendation systems",englishTranscript:"Today we are going to discuss how to design recommendation systems using graphical models and neural networks. The primary algorithmic approach for recommendation or personalization that we have discussed so far has been based on the view of matrix completion. We view user preference data as a matrix. And the goal of the recommendation algorithm is to complete the matrix by filling missing entries in this matrix. The view of matrix completion provides us a way to fill missing preference data.\n\nThis view, however, has few limitations. First, the matrix completion view assumes that the relationship between preferences of users is somewhat simplistic. It could be explained by simple relationship that is behind collaborative filtering or matrix factorization algorithm. In reality this relationship could be quite complicated. Second, the matrix completion view assumes that the user preferences are static. In reality user preferences evolve over time. For example, movies that may seem excellent choice now may not remain excellent choice a decade later. Today we shall discuss how to address these two challenges. We will discuss an approach to address them using formalism of what is known as graphical models.\n\nOkay so, what are graphical models? Well, in a nut shell, graphical models provide succinct representation that can capture a generic dependency relationship between various objects of interest. This representation is of particular interest in statistics and machine learning because it is amendable to scalable computation. Precisely what we shall discuss is how to use graphical model to address these two challenges. First, probabilistic graphical models will help us capture intricate relationships between user preferences. Second, graphical model, with Markovian structure, can capture the temporal aspects of user preferences in recommendation system.\n\nLet us start by discussing how probabilistic graphical models can help capture intricate relationship between user preferences. For the purpose of explanation, we shall rely on our favorite Netflix example. We have users and movies. We shall assume that each user either likes a movie or dislikes a movie. For example, suppose we have four movies, Casablanca, Iron Man, Godfather, and Batman. Then we have four nodes in the graph, the edges between these nodes should capture the relationship between the movie preferences across all users. Intuitively an edge between Casablanca and Godfather might suggest that there is some form of relationship between preferences of users for Casablanca and Godfather. For each user each node is assigned value plus one or minus one depending on whether the particular user likes that movie or does not like that movie.\n\nAs a collection, all users are defining a distribution over +1 and -1 value assigned to all nodes. It is this distribution that the graphical model captures using appropriate edge structure and associated parameters. And it is this distribution that is truly capturing the relationship between preferences of all users. So now let us discuss, what are the parameters associated with the graph? To describe them, we shall consider a specific parameterization for such a graphical model, also known as Pairwise Graphical Model. In this parameterization, there is a distinct parameter associated with each node and edge of the graph. In the four node graph example let us say theta one is the parameter associated with node one. And so on. There are a total of six pairs of edges and hence in principle there are six edge parameter associated with them. These parameters describe distribution of the four nodes taking plus one or minus one values as follows. So let us say sigma one denotes the value associated with node one, could be plus one or minus one, and so on. Then, the probability of the four variables having this particular assignment is proportional to the following formula. You'll get positive formalized learning. Probability is proportional to exponential of summation of two terms.\n\nThe first term is the summation of four elements, each corresponding to one of the four nodes. The second term is the summation of six elements each corresponding to each of the edges. The graphical model does describe and record the relationship between user preferences, or movies, through these node and edge parameters. For example, if user population is such that all like all the movies, the parameterization where edge parameters are equal to zero and node parameters equal to infinity. Describes the corresponding graphical model. That is the graph has no edges. In practice, the parameter need to be learned from the observations. The model capture relationship across all movies therefore at first glance it may appear that we will need to observe user's preferences over all movies to learn all these parameters. Can we learn parameters of the graphical model from sparse user preferences? The answer is somewhat surprisingly, yes. It suffices to observe very little data as long as across all users we have observed preferences between all pairs of movies. We should be able to estimate the pairwise moments of all nodes. It means that we should be able to estimate the following. The fraction of users that like both movies, that like one movie but not the other one, and the fraction of users that dislike both movies. Amazing isn't it.",createdAt:mL,updatedAt:mL,publishedAt:"2023-03-16T01:24:10.034Z"}},{id:1179,attributes:{title:"Graphical Model Estimation",slug:"graphical-models-estimation",duration:"6:59",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience79GraphicalModelsandNeuralNetworks2",objective:"Wrap up learning about recommendation algorithms involving graphical models",englishTranscript:"So how does one precisely estimate the parameters? One approach to estimate graphical model is called moment matching. We start by assuming some choice of parameters for each node and edge. We first compute the induced moment by the graphical model using given choice of parameters, and compare it with the empirically observed moment. If there is a mismatch, it's likely change the node and edge parameters to reduce the mismatch. And repeat this process til we find a good match. That's it, very simple. It is important to compute the moments induced by a graphical model in a tractable manner. A popular approach to do so is the belief propagation algorithm. I would suggest an interested listener to follow other parts of this course where graphical models are discussed in detail.\n\nSo we have leaned a probabilistic graphical model of presentation of user preferences. How can we use it for the purpose of recommendation? Consider a user suppose she has liked only Casablanca, and that's all we know about her preferences. Now we want to recommend her movies from the remaining movies. Using graphical model, we can compute probability of her liking any other movie, conditioned on the observation that she liked Casablanca. This is effectively assigning scores to each movie. We can use these scores to order movies and recommend them to her. That's it. Computing the score, as described earlier, requires computing distribution of each node, subject to few observations from the entire graphical model. This is not a straightforward computational problem, a good computationally efficient algorithm for this is again the belief propagation, and this is exactly where graphical model representation comes to our rescue to allow for efficient computation.\n\nNow we shall discuss the second challenge we had outlined in the beginning, how to capture the temporal relationship between user preferences using graphical models. If you like movie Goodfellas, then a good recommendation algorithm such as the probabilistic graphical model. A bow may suggest that you will like Godfather Part I, Part II, and Part III. The question is, which one of these should be recommended first? As it is popularly believed Godfather Part II was the best among all three parts, and I think I believe that too. And if this was captured by user preferences, then our system might end up recommending Part II as the movie to watch for a user who has not yet watched Godfather Part I. And this does not make sense, because If you have seen Godfather trilogy then you know that watching them in order is crucial. To make such a distinction, the recommendation system has to take the temporal aspects into account. Most user who would have provided their preferences for Godfather movies would have watched part one first and then part two later. Therefore, if system utilized the temporal aspect of the user preferences, then such a problem would be solved.\n\nAnd that is what we will do next. Precisely, we shall describe how to use a hidden Markov model to solve this problem. We know that the so-called recursive neural network can be similarly utilized for this purpose as well. And it seems like companies like Spotify uses such an approach for recommending songs to its users. In hidden Markov model, there is a notion of time. For each time instance there is a hidden state and there's an observed state. The hidden state is assumed to take one of the finitely many values. The value of hidden state at time t plus 1 depends on the value of the hidden state at time t. The probabilistic relationship between consecutive hidden state is homogenous across time. In this sense, the evolution of hidden state can be viewed as a Markov chain and this is why it is called the hidden Markov model.\n\nNow hidden states are not observed, well that is the reason why they are called hidden. Each time depending on the value of hidden state an observation is made, in our movie example, an observation would correspond to the movie preference of a user. For example, suppose a user has expressed the first preference as she liked Goodfellas and the second preference that she disliked Iron Man. Then the observed state at time one correspond to she liking Goodfellas. The observed state at time two corresponds to she disliking Iron Man. In a nutshell, each user's preferences over time provide assignment to observed state of the hidden Markov model. Different user preferences are of different length, and hence we have observation of a hidden Markov model of different length, depending upon each user. The goal is to learn the probabilistic relationship describing the evolution of hidden states as well as the probabilistic relationship between the observed state and the hidden state at any given time.\n\nA popular approach to learn this relationship is known as the Baum-Welch algorithm. An interested listener will be able to find the precise algorithm description in other parts of the course. Now we shall discuss how to use of such a hidden Markov model for recommendation to a given user. As before, suppose a user has expressed liking for Goodfellas and disliking for Iron Man. Then we know that the observed state at time one is Goodfellas is liked, observed state at time two is Iron Man, disliked. Given this, using the learned hidden Markov model, we can compute the likelihood of any given model and liking being observed as a state at time three.\n\nThis provides score for each of the movie, for which user has not provided her preferences. You can use the score to provide recommendations. If the data is rich enough and algorithms have done their job right, then in the example above, it may suggest higher score for Godfather Part I, rather than for Godfather Part II. Even though at population level Godfather Part II might be liked more. That is the recommendation system would have worked well by understanding the importance of order in which sequels should be watched. In summary, recommendation system based on view of matrix completion have two major limitations. One, they do not capture complex user preference relationship. Two, they do not capture temporal aspects. Graphical model can help us address these limitations in computationally efficient manner. We can use pairwise probabilistic graphical model to capture more intricate relationship. We can use hidden Markov model to capture the temporal aspects of user preferences. The use of neural network is very similar to the way we describe the use of graphical models today.",createdAt:mM,updatedAt:mM,publishedAt:"2023-03-16T01:24:08.845Z"}},{id:1178,attributes:{title:"Using Side-Information",slug:"using-side-information",duration:"8:14",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience710UsingSideInformation",objective:"Learn about how side information can help bolster the effectiveness of your recommendation algorithm",englishTranscript:"So far we have identified users and items to their corresponding row or column, respectively. For example, for us, Frank was only a partially filled row of movie ratings. Frank is so much more than this row, but what really matters is what the recommendation system knows about him. Since we live in the era of big data, trust me, the system knows a lot. Given more data, it seems natural that one would just want to increase prediction accuracy, right? In the Netflix example, if we just had more user movie ratings, then one would simply be able to build a recommendation algorithm for Netflix using the same user item based recommendation techniques that we have developed previously. What if the extra data does not consist of rating but still contains relevant information? It should still be able to help.\n\nThe main guiding principle to use such additional information is to refine your model. Indeed, just like we started with a completely homogeneous and admittedly dull model that assumed that every user movie pair was giving the same rating, all the way to the more refined user item based recommendation algorithm, we can refine this model even further. The idea is to use site information to first group users and items into more homogeneous groups and then work on each group separately. To illustrate this point, consider the following diagram. We start with a large pool of users and a large pool of movies. Then we use that information to group or cluster homogeneous users into, say, three groups and homogeneous movies into, say, four groups. Then on each pair of groups we perform a user-item based recommendation as we've learned how to do. That's using only pass rating. So what kind of information is available? One thing for sure is that it should be relevant to our clustering purposes, which is to say that it should have enough information to discriminate users and items according to preferences, at least partially. In this sense, not all data is created equal. And it makes sense to keep in mind that when deciding which data to collect for this purpose. In practice however we collect as much data as possible and apply a variable selection or dimension reduction technique to keep the relevant information. Take for example the Amazon recommendation system.\n\nWhat does it know about Frank? Well, it knows not only products that he purchased but also what product he's looked at-- in particular the dance-related items-- for how long he's read reviews, how much money he spends on weekends, et cetera. Overall, by now, Amazon has probably figured out that Frank is into dancing, what his income is, and whether he is a compulsive buyer. Frank is probably not the only Amazon customer with these features, and Amazon may be able to identify users that are similar to Frank with respect to those features. The same is true for items. Amazon knows a lot about items beyond their ratings-- price, category, weight, color, et cetera. This is the information used to cluster items. Let us take a closer look at how clustering can be done. There are many clustering techniques available out there, but all of them start from one simple idea. Represent the objects to be clustered by a vector-- this is just an ordered list of numbers-- and then measure the similarity using inner products. We've done this in this very module when the vectors were the rows or columns of ratings, completing with the average rating three when entries were missing. When we have site information, we can create a new vector.\n\nFor example, we can summarize the site information about Frank in the vector F equals 1, 0, 1, 24, 152, et cetera. Which means, for example, that Frank had looked at gardening equipment, the first number 1, has not looked at cooking books, the number 0, has looked at movies, the second number 1, has spent 24 minutes browsing through dance equipment, spent $152 on this last purchase, et cetera. Amazon may even have convinced Frank to subscribe to their credit card and even know his buying pattern outside of the Amazon web site. At the end of the day, the vector F that contains the site information that Amazon has collected about Frank is gigantic with hundreds of thousands of entries, and some are more relevant than others. Let's say that one of these entries, for example, is that Frank has purchased a flight to Vegas with his Amazon credit card. Is this relevant or not? You could sit here and debate about this question from a socioeconomic point of view, but we might as well let the data speak. There are many ways to select variable in a data-driven fashion, with the ultimate goal to get a better clustering. Some of them are covered in other modules of this course, but to name a few, we could use spectral clustering where the goal is to find a projection of the data before clustering; sparse methods where the goal is to simply keep only the most relevant site information; locality-sensitive hashing, which is part of the broader family of approximate nearest neighbors. Most of these methods are coded in standard software, but a few tweaks may be needed to adapt them to your specific purposes. To conclude, let us note that the clustering described above is a bit of a blunt tool and often needs arbitrary decisions.\n\nWhat if we have a continuum of users? How do we decide when to stop putting users in the first cluster and start putting them in the second one? Rather than making this ad hoc choice, we can simply weigh users. We put more weights on users that are more similar to a given user. There are many ways to incorporate weights into recommendation systems, but here is a rather simple one to fix ideas. Recall that in absence of site information we computed similarity between users by looking at inner products between their rows of ratings, and that we can do the same with their vectors of site information, perhaps after variable selection. This gives us two inner products that we call IPR, for inner product between ratings, and IPS for inner product between site information.\n\nWe can now combine the two by looking at the new hybrid similarity measure defined by IPR plus t times IPS. Here, the parameter t is positive and represents how much weight we want to put on the site information. For small t, the similarity measure essentially disregards site information. And for larger and larger t, it takes more and more site information into account. Choosing t is more of an art than a science. The best choice of t will depend on how good your site information is and should be done in light of data. Interestingly, there is an other interpretation of this hybrid similarity measure. This is the measure we get if for each user we concatenate the row of ratings with the vector of site information, where each coordinate is multiplied by the factor square root of t. It is easy to see that when we take the inner product between the two longer rows, we recover the hybrid similarity measure. We can also use different values of t for different users and choose this value from site information itself. We can use more complicated combinations.\n\nThe hybrid measure that we've just defined is set to be linear. It is a linear combination of IPR and IPS. We could instead use nonlinear functions. And this is what we do when we cluster first and then recommend. In this case, the rule to compute a hybrid similarity measure is as follows. If IPS is larger than a threshold, then use IPR as a measure of similarity. If IPS is smaller than a threshold, then you zero as a measure of similarity. The role of t here is replaced by the threshold which has to be chosen in a data-driven way too. To summarize, we have seen that recommendation systems often have site information about users and items available. This information can be used to refine our model. That is remove some homogeneity assumptions. Practically, this is done by incorporating site information when computing similarity between users or between items.",createdAt:mN,updatedAt:mN,publishedAt:"2023-03-16T01:24:07.620Z"}},{id:1177,attributes:{title:"20 Questions and Active Learning",slug:"20-questions-and-active-learning",duration:"7:45",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience71120QuestionsandActiveLearning",objective:"Find out how active learning plays a role in recommendation systems",englishTranscript:"Netflix or Amazon are both web-based services, and this is why they are able to collect vast amounts of data with minimal efforts. However, our algorithms themselves did not exploit any aspect of being online. Indeed, even if Netflix suddenly lost internet access and, therefore, connection with their users, they could still run the user item based recommendation system and predict missing ratings accurately. What ability do we gain by being online as opposed to offline? Well, we can interact with the users, get the information we need to move on to our next goal. Consider the following silly example. Let's say that Netflix has just bought the rights to add the movie Moneyball to their catalog, but they are not sure what to make of the site information that comes with it. Is it a sports movie? A finance movie? A teen movie? It has Jonah Hill in it. A chick flick? It has Brad Pitt in it.\n\nLet's say it's difficult to automatically predict who is going to like this movie. What Netflix could do is to recommend it to key users and adjust recommendations according to their feedback in real time. This is called active learning. This is in contrast to passive learning, where Netflix would select a large group of users and wait for everyone's feedback before making adjustments. The advantage of active learning over passive learning has been demonstrated empirically and mathematically. Active is always better and is implemented in many aspects of our lives. Perhaps one of the first places where it was used was in clinical trials that are now run in phases. If a drug performs well on the first patients, it will be used more aggressively on subsequent ones. But if it has disastrous effects, the dosage may be adjusted in future phases or the trial may be canceled altogether.\n\nThe idea behind active learning is quite simple. Collect formation as often as possible, not only to make future decisions, but also to guide future information collection. Web interfaces are a wonderful tool to perform exactly that. Perhaps the most primitive form of active learning is the 20 questions game, where a person chooses a subject, say, an actor, and a player must guess in 20 questions who this person is. If you've played this game, you know that you should split the population you have in mind by about half at every question. For example, let's say the first question may be, is this person a man or woman? Or is this person dead or alive? Et cetera. As you are getting further down the game, you refine your questions. Let's say that you have used your first questions to establish that we are talking about a dead male actor who plays in Westerns. You may want to ask whether he was involved in politics. We train at this game from a young age by playing guess who, for example. And there are websites that are freakishly good at this game actually.\n\nSo how are computers so good at this game? Well, they know how to split the candidates into halves very accurately. This is called the bisection method. Why is it working so well? Let's say that you were playing this game and start with a population of n candidates. After one good question, you are left with n\u002F2 plausible candidates. After another good question, we are left with n\u002F4 plausible candidates, etc. After k questions, we are left with n over 2 to the k plausible candidates. To be sure of your guess, you need to have isolated at most one subject. In other words, you need to ask k questions such that n over 2 to the k is at most 1. Solving for k by taking logarithms, this translate into k larger than log n, where the logarithm is in base 2. For example, if you started with 1 million candidates, you actually need 20 questions to be sure to win. The main difficulty of the game is, of course, to pick questions that split the remaining candidates exactly in half. But computers are good at this. Let us go back to a more serious game, Tinder. There are about 50 million regular users on Tinder. So in principle, your soul mate could be identified in 26 questions. However, the success of Tinder is based on precisely asking a very simple question, yes or no, say, hot or not. Note that the question is not, is your soul mate body mass index above or below 20? This would be boring and Tinder would lose customers.\n\nFinding your soul mate based on yes\u002Fno questions seems like an impossible task. It's like having a 3-year-old play the 20 question game. Is it Nemo? Is it grandpa? Is it mittens? Et cetera. It's the complete opposite of the bisection method. Instead of splitting the population of size n into two parts of size roughly n\u002F2, it splits it into two parts of size n minus 1 and 1 respectively. Unless we're very lucky, it's clear that we need n questions, rather than log n questions in this case. Yet, Tinder does it and others also do it. Kittenwar.com is a website that ranks the cutest kittens by only asking which is nicest out of two photos. The beer mapper is another such example. It's an app that's trying to find what kind of beer you like by only asking which beer you prefer out of two choices. For the beer mapper algorithm, the goal is to find two beers, say beer A and beer B, such that by knowing if a user prefer beer A to beer B, one gets a lot of information about the beer preference of that user. Let's say that the goal is to identify the user's favorite beer.\n\nThen in the spirit of the bisection method, the goal is to eliminate half of the candidate beers after each comparison. How can we do this? Well, the beer mapper starts from a geometric representation of beers that is based on ratings and descriptions of beers that were written by beer experts on the website ratebeer.com. It looks like this. Beer mapper picks two beers that split the landscape of beers into halves. Say, for example, the user prefers the beer on the left. So we know that the user's favorite beer is in this region. Then we pick two more beers from that new region and repeat until we have narrowed our candidate beers to one beer, the user's favorite beer. Using slightly more complicated techniques, we can find a user's ranking of beers rather than a single beer. Here, the goal is to place the user in this landscape and declare the closest beer his favorite beer, the second closest, second favorite, etc. Placing this user can be done using a method similar to the bisection method that we've just described. The bisection method can go a long way when we're searching for a needle in a haystack, a soul mate in Tinder, or beer in an app. In this case, it is good to keep in mind that active learning can be very useful, in particular, it can be used to establish preferences in the context of personalized ranking.",createdAt:mO,updatedAt:mO,publishedAt:"2023-03-16T01:24:06.443Z"}},{id:1176,attributes:{title:"Building Recommendation Systems",slug:"building-recommendation-systems",duration:"10:57",videoUrl:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FDataScience712BuildingRecommendationSystems",objective:"Learn how to build an actual recommendation system",englishTranscript:"Today, our goal is to discuss how one might go about building an actual recommendation system. A recommendation system in production environments such as that in Amazon, Netflix, or YouTube is a lot more than what we have discussed thus far in this module. To begin with, recommendation system is a service that is integrated as part of an interactive data driven system that operates in real time and at scale. For example, in the context of YouTube, the primary purpose of the system is to deliver different media content to the user. The rule of the recommendation system is to identify which media content get displayed on the user's interactive interface every time the user performs an action. Now any such real time interactive data human system has three key function components that strongly interact with each other.\n\nOne, sensing platform. Two ,storage and computation infrastructure. Three, intelligence processing algorithms. The sensing platform is the one that provides ability to interact with the end user or customer in real time in terms of both collecting data as well as delivering appropriate data including recommendations. The storage and computation infrastructure makes it feasible to store the collected information about user behavior in real time, as well as perform meaningful computation in real time and at scale. The intelligence processing algorithms utilize the data collected via sensing platforms and store it in the storage infrastructure. It performs appropriate data processing using clever algorithms such as those discussed in this module to extract meaningful information and make decisions such as recommendations. These decisions including recommendations, are delivered to the end user through sensing platform while storing or logging them in the storage infrastructure for future use. The primary purpose of this module and the course at large has been explaining what are the appropriate algorithmic approaches for intelligence processing.\n\nThe singular focus of this course has been the use of foundations from statistics and machine learning to arrive at these algorithms in a principled manner. We believe that this approach would enable the listener to be able to develop principled algorithms for intelligence processing, for the task faced by her. Today, we shall briefly discuss few guidelines on how one might go about actually building such an interactive system, that can scale and operate in real time. Imagine that your system such as Amazon or Netflix or YouTube is serving tens of or hundreds of millions or even billions of customers. An excellent example is that of Google search engine. As per some statistics, more than 2 million searches are performed every second. It is this web scale that we are thinking of designing the system for. For such a system, let us start by thinking about the sensing platform. In the modern environment, end users of the system end up interacting through effectively, some form of web interface. The end user might be using a web browser or a mobile app, but in a nutshell, in any of these contexts, our system interfaces through what is known as API, that is Application Program Interface. The key here is to make sure that the user interface, be it mobile app or web interface, utilizes the correct API in the correct place. For example, when a user clicks on a media content displayed, a request is sent to the API with appropriate message. In response, the API should provide streaming connection to the content of interest, and the user interface needs to change so as to display this incoming stream of content and potentially displaying additional information such as the recommendations received. This aspect of system design falls under what is known as developing user interface or in short, UI. Currently, the use of JavaScript framework is very popular for designing highly interactive web interfaces. The creative aspect of designing UI is an extremely important topic and way beyond my artistic skills.\n\nA good system designer may want to think very carefully about the design aspect. In a nutshell, from the perspective of building system, is all about providing the right set of APIs and defining them carefully. Now that we have APIs that interact with users by exchanging appropriate data between users and system, it is time to discuss how should the API support it's promised functionality to the user interface. We wanna think of designing APIs such that potentially, hundreds of thousands requests per second are received. Usually, the APIs are hosted through an array of web servers sitting behind appropriate load balancing infrastructure. The web servers upon receiving request, end up performing various tasks. This primarily includes storing or logging the information received and obtaining information within the system which might be precomputed to send the response back to the user. The logging or storing of data, requires a good infrastructure that can handle a high volume of data, and request reliably. Apache Kafka is an open source example of such an infrastructure, also known as Publish and Subscription, or Pub\u002FSub infrastructure. Data may be stored in a robust, distributed file system as a data log for both archival purposes as well as making structure data analysis. Open source example of such a file system is the hadoop file system. To make structured create efficiently, such data maybe stored in a file format such as Arcade.\n\nThese files can be queried in a standard database form using open source infrastructure like Spark SQL. For the purpose of computation, the data that is logged or some processed form of it is required to be accessed in a random access manner. For this purpose, use of robust distributed databases are needed. An open source example of such a database is Apache Cassandra. Now moving to computation infrastructure. The computation or processing infrastructure is required to perform various computational tasks. On one hand, it may be needed to perform basic tasks in running the data pipeline. On the other hand, it may be needed to perform complex algorithmic tasks to produce meaningful recommendations. In either case, to deal with the scale of data and hence of computation, it is essential to utilize distributed computation infrastructure. Architecturally, the Map Reduce is a popular solution. It is no surprise that engineers at Google have popularized such an architecture. In fact they have been the pioneers in developing such web scale infrastructure. The efficient realization of such architecture is provided by open sources like hadoop or, more recently, Spark.\n\nIn a nutshell, this requires thinking of algorithm as composition of small, atomic tasks, each operating on small amounts of data. Cleverly interleaved collection of such tasks, performed in a distributed and parallel environment, can lead to a desired solution. For example, let us consider implementing recommendation algorithms in such a framework. If we think of basic algorithm based on population averages in the context of restaurant recommendation, effectively, we want to compute averages rating for each restaurant. One way to divide this into multiple small tasks that can be performed in parallel is to create a separate task for computing average rating for each restaurant separately. Each of these small tasks have to make query to get all ratings related to the restaurant of interest and then take average of these ratings, that's it. Such computation can be done easily. After all, we are potentially adding a small collection of numbers. We have a distributed implementation of our first algorithm this way.\n\nNow let us think about a more complicated algorithm like item-item collaborative filtering, in this context. We can do the same, but it will require performing some pre-processing beforehand. Recall that in the item-item collaborative filtering algorithm and apply to our restaurant setting. We estimate the missing rating of a given restaurant for a given user by taking the weighted average of ratings of other restaurants that are similar. Therefore, if we had a way to find top few similar restaurants for given restaurant very quickly, maybe through one query to some form of data base, then we can create tasks, one for each restaurant and run each of these tasks in a distributed parallel environment. The big question is, how do we pre-process data so that we can find restaurants similar to a given restaurant of interest very quickly? As if we are making a query database, and we want to make sure that this pre-processing can be performed in a distributed parallel environment as well. Well the answer to this question lies in a very deep connection between computer science and mathematical analysis. More precisely, this boils down to creating a data structure known as Nearest Neighbor Index on all restaurants with respect to the similarity that collaborative filtering algorithm cares about. In effect, this requires mapping each restaurant to a point in Euclidian space, of very small dimension.\n\nRemember our world is three dimensional in Euclidian space. This mapping respects the similarity structure induced by collaborative filtering algorithm, that is, for a given restaurant, the closest restaurant in the Euclidian space are those that are most similar to it. Once we have such mapping, searching for closest restaurant is very easy. It's just as if I asked you to go and look for your closest neighbor, all you have to do is take a few steps in every direction and then find out the first one you meet, and then report back to me, that's it. Now one such class of pre-processing algorithms that achieve this magical mapping are known as Locality Sensitive Hashing and they're pretty efficient to implement. Finally, let us think about implementing algorithm based on finding the singular value decomposition of matrix. Now at the core of such linear algebraic algorithms, one requires performing multiplication between matrices. The matrix could be very large. For example, think of a rating matrix with hundreds of millions to maybe billions of users and tens to hundreds of millions of product. This matrix has 10 to the power 17 entries in principle. This is prohibitively large. Therefore, to perform matrix multiplication at such a scale, it requires clever division of data and computation in smaller tasks. Indeed, such a division can be performed and can be scaled using the Map Reduce like architecture.\n\nWe will not discuss the details here, but it can be an interesting exercise for a listener to think about how to multiply two extremely large matrices using very small memory footprint. Summarizing, to build a recommendation system usually is an integral part of an interactive real time system, requires carefully thinking about sensing platform, storage & computation infrastructure, and intelligence processing algorithms. The primary purpose of this course at large has been, developing principled approach for processing algorithms. Today, our interest was to briefly discuss how these algorithms fit in the broader ecosystems of the overall data processing system. We briefly touched upon various popular open source solutions for different aspects of the system design. I would strongly recommend an interested listener to look at various open sources available under the Apache software foundation. Finally, a parting remark. Today, we have computation infrastructure available for rent through various cloud platforms. Therefore, it may make sense to utilize them to get off the ground quickly, without worrying much about maintaining such systems. And as time progresses, depending upon the scale and requirement, one may evolve to maintaining such infrastructure on one's own.",createdAt:mP,updatedAt:mP,publishedAt:"2023-03-16T01:24:05.299Z"}}]},instructors:[{id:634,name:ir,position:a,nationality:a,achievements:a,focus:a,bio:is,learnMoreURL:a},{id:633,name:it,position:a,nationality:a,achievements:a,focus:a,bio:iu,learnMoreURL:a}]}}]},instructors:[{id:526,name:lq,position:a,nationality:a,achievements:a,focus:a,bio:lr,learnMoreURL:a,image:{data:a}},{id:530,name:io,position:a,nationality:a,achievements:a,focus:a,bio:ip,learnMoreURL:a,image:{data:a}},{id:529,name:lN,position:a,nationality:a,achievements:a,focus:a,bio:lO,learnMoreURL:a,image:{data:a}},{id:532,name:me,position:a,nationality:a,achievements:a,focus:a,bio:mf,learnMoreURL:a,image:{data:a}},{id:531,name:mg,position:a,nationality:a,achievements:a,focus:a,bio:mh,learnMoreURL:a,image:{data:a}},{id:533,name:mp,position:a,nationality:a,achievements:a,focus:a,bio:mq,learnMoreURL:a,image:{data:a}},{id:527,name:ir,position:a,nationality:a,achievements:a,focus:a,bio:is,learnMoreURL:a,image:{data:a}},{id:528,name:it,position:a,nationality:a,achievements:a,focus:a,bio:iu,learnMoreURL:a,image:{data:a}}]}}]},impactSpotlights:{data:[{id:I,attributes:{title:aY,status:b,slug:fS,publishedOn:fT,readingTime:n,commentsEnabled:c,french:c,createdAt:ay,updatedAt:ay,publishedAt:fU,logo:{data:{id:hB,attributes:{name:hC,alternativeText:a,caption:a,width:aU,height:aU,formats:a,hash:hD,ext:r,mime:s,size:gX,url:hE,previewUrl:a,provider:j,provider_metadata:a,createdAt:gP,updatedAt:gP}}},contentBlocks:[{id:4265,__component:f,text:bv,slug:bw},{id:8341,__component:d,text:"The MIT Office of Sustainability."},{id:4263,__component:f,text:bx,slug:by},{id:8344,__component:d,text:"Collected sustainability-related data from multiple MIT facilities into a single repository and used big data analytics to study, visualize, and share the information. The MIT DataHub gathers data from many sources and stores it in the cloud. For instance, meters at buildings record and upload energy and water usage, ID card readers at parking lots record the number of unique parkers each day, and waste management companies weigh different waste streams (such as trash and recycling) and manually upload the data. Anyone at MIT can access the information through an online portal, where the data can be downloaded directly or visualized with bar charts, trend graphs, and maps."},{id:4262,__component:f,text:bz,slug:bA},{id:8342,__component:d,text:"MIT uses the data collected in the DataHub in many ways. [Energy usage data] is used by our planning group on a regular basis, Julie Newman, MITs director of sustainability, tells MIT Horizon. The platform breaks down energy usage by building and calculates the greenhouse gas emissions caused by each source of energy. This information is used by the Office of Sustainability and shared with consultants to guide MITs plans to cut emissions. The Office of Sustainability has used other data to create storytelling infographics to educate on the importance of sustainability. For example, a set of maps that show how the MIT campus may flood in future conditions guided the construction of MITs new Schwarzman College of Computing. The DataHub has also informed research papers and was central to an undergraduate class on best practices in reducing carbon emissions."},{id:4264,__component:f,text:mQ,slug:gK},{id:8343,__component:d,text:"The Office of Sustainability built the cloud-based platform with standard tools for big data analytics, including Amazon Web Services to host the data, Tableau to create interactive visualizations, and the programming language R for basic statistical analysis. Using big data analytics tools is helpful not just for managing an influx of data from multiple sources, but also for making applications flexible enough to handle additional data gathering after theyre deployed. One of MITs long-term goals for its sustainability analytics platform is to make the system adaptable to future technologies, as well for use by other organizations. We're going to create something that's scalable, explains Newman. No matter what institute you are around the world, I want to create something that's accessible. For more on big data analytics techniques, see [How Big Data Analytics Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-works-bd). For the fundamentals of cloud computing, which the DataHub platform is built on, see [How Cloud Computing Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-cloud-computing-works)."}],badges:[{id:298,__component:q,title:A},{id:302,__component:q,title:iv},{id:299,__component:q,title:mR},{id:301,__component:q,title:iw},{id:aN,__component:q,title:ix}],relatedLinks:[{id:149,__component:N,title:"Using Sensors to Spot Pollution Sources ",url:"https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-sensors-to-spot-pollution-sources"},{id:hr,__component:N,title:"Using Big Data and AI to Monitor Industrial Equipment ",url:mS}],tags:[]}},{id:fV,attributes:{title:aZ,status:b,slug:fW,publishedOn:fX,readingTime:n,commentsEnabled:c,french:c,createdAt:az,updatedAt:az,publishedAt:fY,logo:{data:{id:hB,attributes:{name:hC,alternativeText:a,caption:a,width:aU,height:aU,formats:a,hash:hD,ext:r,mime:s,size:gX,url:hE,previewUrl:a,provider:j,provider_metadata:a,createdAt:gP,updatedAt:gP}}},contentBlocks:[{id:4269,__component:f,text:bv,slug:bw},{id:8348,__component:d,text:"Department of Veterans Affairs (VA)."},{id:4266,__component:f,text:bx,slug:by},{id:8347,__component:d,text:"Used an artificial intelligence (AI) model to analyze the health records of U.S. veterans, identifying those at increased risk of suicide so they can receive early intervention. The model considers medical, behavioral, and demographic data for patients of the Veterans Health Administration. It draws fresh data monthly and, based on statistical patterns, identifies patients in the top 0.1% for suicide risk. Veterans in this top risk tier die by suicide at 30 times the rate of veterans overall.\n\nWhen the model flags a veteran as high risk, a local program coordinator contacts his or her health care provider through an interactive dashboard. The clinician reaches out to the veteran to check in proactively. The program, called Recovery Engagement and Coordination for Health  Veterans Enhanced Treatment, or REACH VET, has operated nationally since 2017."},{id:4267,__component:f,text:bz,slug:bA},{id:8346,__component:d,text:"In the U.S., suicide rates are higher among veterans than other adults, a problem that has persisted for decades. An average of nearly 17 U.S. veterans die by suicide daily, [according to the VA](https:\u002F\u002Fwww.mentalhealth.va.gov\u002Fdocs\u002Fdata-sheets\u002F2022\u002F2022-National-Veteran-Suicide-Prevention-Annual-Report-FINAL-508.pdf). Other resources, such as the VA Veterans Crisis Line, typically depend on a veteran or someone close to them recognizing a crisis as it occurs and calling for help. By providing a data-driven approach for predicting risk, REACH VET helps identify signs of a crisis before it occurs, allowing more and earlier opportunities for aid. The program does not replace the judgments of a veteran or their clinician, but complements it. REACH VET is a game changer in our effort to reduce veteran suicide, Dr. Caitlin Thompson, then national director of the VAs Office for Suicide Prevention, said in 2017 when the program launched. Early intervention can lead to better recovery outcomes [and] lessen the likelihood of challenges becoming crises. \n\nAccording to the VA, REACH VET is the first clinical use of an algorithm to help identify suicide risk in the U.S. In 2021, a study led by scientists from the VA and the National Institute of Mental Health (NIMH) examined the AI models performance on data from more than 150,000 veterans. The study found that veterans included in the REACH VET program had fewer documented suicide attempts, fewer ER visits, fewer inpatient mental health admissions, and fewer missed health care appointments. Results of the study were [announced](https:\u002F\u002Fwww.nimh.nih.gov\u002Fnews\u002Fscience-news\u002F2022\u002Fstudy-shows-reach-vet-program-effective-for-veterans-at-high-risk-for-suicide) by NIMH and published in [JAMA Network Open](https:\u002F\u002Fjamanetwork.com\u002Fjournals\u002Fjamanetworkopen\u002Ffullarticle\u002F2785078)."},{id:4268,__component:f,text:hF,slug:hG},{id:8345,__component:d,text:"AI systems can be trained to analyze large volumes of data with many variables in play and to detect patterns too subtle, complex, or interdependent for a human (or a simpler computer program) to identify. Organizations can use this capability to spot correlations and make predictions. In some scenarios, an AI system can interpret data more effectively than other methods, especially when the relationships among variables are unknown, temporary, or volatile.\n\nFor more on the use of AI to help make sense of complex conditions and to make predictions, see [Benefits of AI](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-ai). For more on how organizations are using those predictions to improve their operations, see [How AI is Used in Industry Today](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-ai-is-used-today-in-industry#making-predictions-2)."}],badges:[{id:305,__component:q,title:gY},{id:303,__component:q,title:A},{id:306,__component:q,title:iy},{id:304,__component:q,title:hH}],relatedLinks:[{id:151,__component:N,title:bp,url:mS},{id:jF,__component:N,title:"Using AI for Medical Diagnosis",url:"https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-ai-for-medical-diagnosis"}],tags:[]}},{id:fZ,attributes:{title:bp,status:b,slug:f_,publishedOn:f$,readingTime:n,commentsEnabled:c,french:c,createdAt:aA,updatedAt:aA,publishedAt:ga,logo:{data:{id:hB,attributes:{name:hC,alternativeText:a,caption:a,width:aU,height:aU,formats:a,hash:hD,ext:r,mime:s,size:gX,url:hE,previewUrl:a,provider:j,provider_metadata:a,createdAt:gP,updatedAt:gP}}},contentBlocks:[{id:2142,__component:w,title:"cepsa",type:x,caption:"Image by Tasos Mansour via Unsplash.",image:{data:{id:2333,attributes:{name:"cepsa_155a31d653.png",alternativeText:a,caption:a,width:ht,height:gI,formats:{large:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_cepsa_155a31d653_8c9e099e6b.png",hash:"large_cepsa_155a31d653_8c9e099e6b",mime:p,name:"large_cepsa_155a31d653.png",path:a,size:657.28,width:P,height:mT},small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_cepsa_155a31d653_8c9e099e6b.png",hash:"small_cepsa_155a31d653_8c9e099e6b",mime:p,name:"small_cepsa_155a31d653.png",path:a,size:169.13,width:y,height:mU},medium:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_cepsa_155a31d653_8c9e099e6b.png",hash:"medium_cepsa_155a31d653_8c9e099e6b",mime:p,name:"medium_cepsa_155a31d653.png",path:a,size:382.08,width:B,height:mV},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_cepsa_155a31d653_8c9e099e6b.png",hash:"thumbnail_cepsa_155a31d653_8c9e099e6b",mime:p,name:"thumbnail_cepsa_155a31d653.png",path:a,size:42.98,width:aI,height:in0}},hash:"cepsa_155a31d653_8c9e099e6b",ext:o,mime:p,size:333.55,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fcepsa_155a31d653_8c9e099e6b.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:mW,updatedAt:mW}}}},{id:4291,__component:f,text:bv,slug:bw},{id:8372,__component:d,text:"Cepsa, a Spanish oil and gas company."},{id:4295,__component:f,text:bx,slug:by},{id:8371,__component:d,text:"Used a type of repository called a *data lake* to gather massive amounts of data from across eight facilities for analysis. Cepsas data lake included information collected from more than 300,000 sensors attached to industrial equipment, which generate over 170 million data points a day on factors like temperature, pressure, energy consumption, and flow rate. By making this sensor data accessible within a single repository, along with information on weather patterns, prices, and maintenance schedules, the companys data scientists could more easily visualize data and track trends from these facilities. Cepsa also used artificial intelligence (AI) to look for patterns in the information gathered in its data lake. AI could detect signs that a piece of equipment was behaving unusually and needed maintenance before a costly breakdown, and it could optimize processes, predicting how to maximize output or minimize energy usage."},{id:4293,__component:f,text:bz,slug:bA},{id:8370,__component:d,text:"Cepsa used big data analytics techniques to create the data lake and AI to analyze the data, which allowed it to identify potential problems and improve processes throughout its facilities. For example, an AI algorithm used sensor data and maintenance logs to detect an anomaly in a turboexpanderan industrial cooling system used in refining natural gasestimating that it would suffer a serious malfunction in 45 days. Another algorithm uses real-time data from equipment at a phenol plant in Spain to find the optimal conditions for production. The system provides recommendations to plant operators every 15 minutes, maximizing output at the facility by 2.5% while cutting carbon dioxide emissions by 1,500 metric tons per year. Overall, Cepsa says its AI analysis has reduced the companys carbon dioxide emissions by an estimated 70,000 metric tons per year and cut overall energy consumption by 2 percent, while increasing output at multiple plants."},{id:4294,__component:f,text:mQ,slug:gK},{id:8374,__component:d,text:"Big data analytics tools handle massive amounts of data: more data than can fit on a single computer, and generally data of multiple types gathered continuously. Sensors on industrial equipment can gather a tremendous amount of data, but big data analytics is needed to make it useful. Cepsa used a complex architecture of multiple databases to clean and sort the sensor data, match it up with information from other sources (such as weather reports and maintenance logs) and connect it to visualization tools and AI systems. For more, see [How Big Data Analytics Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-works-bd)."},{id:4292,__component:f,text:hF,slug:hG},{id:8373,__component:d,text:"AI excels at finding patterns in data, especially the common type of AI referred to as machine learning. The more data, the more useful machine learning systems are at finding patterns that a human observer might miss. Machine learning is particularly suited to the diverse and loosely structured nature of big data: Some systems can find patterns in data even without a programmed understanding of what the data relates to. For instance, the AI system that detected a problem with the turboexpander wasnt programmed with physical simulations of the equipment or information on how it worked; it simply detected an anomalous pattern in the refrigerators sensor data. For more, see [How AI Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-ai-works)."}],badges:[{id:325,__component:q,title:gY},{id:326,__component:q,title:A},{id:328,__component:q,title:iv},{id:jR,__component:q,title:mX},{id:329,__component:q,title:mR},{id:327,__component:q,title:"Predictive Maintenance"}],relatedLinks:[{id:163,__component:N,title:mY,url:mZ},{id:164,__component:N,title:m_,url:m$}],tags:[]}},{id:gb,attributes:{title:gc,status:b,slug:gd,publishedOn:ge,readingTime:u,commentsEnabled:c,french:c,createdAt:aB,updatedAt:aB,publishedAt:gf,logo:{data:{id:hB,attributes:{name:hC,alternativeText:a,caption:a,width:aU,height:aU,formats:a,hash:hD,ext:r,mime:s,size:gX,url:hE,previewUrl:a,provider:j,provider_metadata:a,createdAt:gP,updatedAt:gP}}},contentBlocks:[{id:2145,__component:w,title:"aclima",type:x,caption:"Photo by Photoholgic via Unsplash.",image:{data:{id:2336,attributes:{name:"aclima_0be9387467.png",alternativeText:a,caption:a,width:ht,height:gI,formats:{large:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_aclima_0be9387467_411ec32bf7.png",hash:"large_aclima_0be9387467_411ec32bf7",mime:p,name:"large_aclima_0be9387467.png",path:a,size:503.84,width:P,height:mT},small:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_aclima_0be9387467_411ec32bf7.png",hash:"small_aclima_0be9387467_411ec32bf7",mime:p,name:"small_aclima_0be9387467.png",path:a,size:126.94,width:y,height:mU},medium:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_aclima_0be9387467_411ec32bf7.png",hash:"medium_aclima_0be9387467_411ec32bf7",mime:p,name:"medium_aclima_0be9387467.png",path:a,size:287.83,width:B,height:mV},thumbnail:{ext:o,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_aclima_0be9387467_411ec32bf7.png",hash:"thumbnail_aclima_0be9387467_411ec32bf7",mime:p,name:"thumbnail_aclima_0be9387467.png",path:a,size:34.48,width:aI,height:in0}},hash:"aclima_0be9387467_411ec32bf7",ext:o,mime:p,size:152.46,url:"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Faclima_0be9387467_411ec32bf7.png",previewUrl:a,provider:j,provider_metadata:a,createdAt:na,updatedAt:na}}}},{id:4304,__component:f,text:bv,slug:bw},{id:8384,__component:d,text:"Aclima, an air-quality monitoring and mapping company."},{id:4308,__component:f,text:bx,slug:by},{id:8383,__component:d,text:"Aclima outfitted a fleet of vehicles with inexpensive microsensors that measure air quality, detecting common particulate pollutants, greenhouse gasses, and air toxins. The vehicles drove block by block throughout the West Oakland neighborhood in Oakland, California, at different times of day and night, during different days of the week and seasons of the year, to measure air quality at the individual address level. As measurements were taken, they were streamed to the cloud for storage. Experts then analyzed them to uncover patterns of hyperlocal and sometimes time-variable sources of pollution."},{id:4306,__component:f,text:bz,slug:bA},{id:8385,__component:d,text:"The volume of data, block by block, at various hours, in various types of weather, can help to uncover patterns of pollution that otherwise might not be detected by traditional stationary pollution monitoring, which more often gives a broader view of air pollution in a region. You can see one street, my street, might be different from the street next over or the street over from that, said Melissa Lunden, Ph.D., Aclimas chief scientist. And the difference, depending on local sources, can be large.\n\nThe resulting data may help community leaders uncover such issues as factory emissions settling more thickly in one neighborhood than another or a high level of vehicle emissions from weekly deliveries to a local business.\n\nThe project also meets a need established by law. In 2017, the California State Assembly passed an act mandating that the state work with community leaders to find ways to combat pollution in neighborhoods with historically high pollution levels and poor health outcomes. The West Oakland Environmental Indicators Project (WOCAP) is one of the first success stories to come from the new law. Utilizing the data collected by Aclima, community and business leaders in West Oakland were able to identify local sources of pollution, and partner with local businesses and officials to adopt more than 80 new strategies to create cleaner air for neighborhoods with poorer air quality. Strategies included changes in zoning rules to create greater buffer zones between residential and industrial zones and enhanced training for city workers to cut down on idling time of vehicles."},{id:4305,__component:f,text:"Why IoT",slug:"why-iot"},{id:8386,__component:d,text:"Microcomputing technology has allowed innovators to shrink the size and lower the price of air-quality sensors and to reduce the level of expertise needed to operate them, creating a more complete picture of air quality. For more on how IoT can gather information and detect problems, see [Benefits of IoT](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-iot#the-iot-can-detect-problems-early-reducing-their-harm)."},{id:4307,__component:f,text:"Why cloud computing",slug:"why-cloud-computing"},{id:8387,__component:d,text:"As data is collected by Aclimas fleet of vehicles, it is immediately transmitted to the cloud to later be analyzed by data scientists. Transmitting such a high volume of data in real time is only possible with high-speed 4G and 5G internet connections and cloud storage capabilities, said Lunden. For more on how organizations can analyze more data at greater speeds using cloud computing, see [Benefits of Cloud Computing](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-cc#organizations-can-analyze-more-data-at-greater-speeds-in-the-cloud)."},{id:4309,__component:f,text:gT,slug:gK},{id:8388,__component:d,text:"Once the data is collected onto cloud servers, Aclimas big data analytics software, Aclima Pro, automatically searches for patterns and creates visualizations for data scientists and community leaders to analyze. A more granular and hyperlocal picture of air quality provides the opportunity for a bottom-up approach to pollution mitigation, as community leaders are armed with data and local knowledge to identify pollution sources and work with local businesses to improve neighborhood health. For more on how organizations use big data analytics, see [Benefits of Big Data Analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-bd)."},{id:4310,__component:f,text:"What's next",slug:"whats-next"},{id:8389,__component:d,text:"We now are digging into the data in slightly different ways to come up with, for instance, more granular information around a hot spot or a methane leak, Lunden said. Aclima is also expanding operations in cities and states across the United States, including New York, Chicago, and Buffalo, and internationally."}],badges:[{id:ij,__component:q,title:mX},{id:339,__component:q,title:A},{id:341,__component:q,title:iv},{id:342,__component:q,title:ix},{id:340,__component:q,title:"Sensors"}],relatedLinks:[{id:170,__component:N,title:mY,url:mZ},{id:169,__component:N,title:m_,url:m$}],tags:[]}},{id:gg,attributes:{title:bq,status:b,slug:gh,publishedOn:gi,readingTime:gj,commentsEnabled:c,french:c,createdAt:aC,updatedAt:aC,publishedAt:gk,logo:{data:a},contentBlocks:[{id:4436,__component:f,text:bv,slug:bw},{id:8469,__component:d,text:"Researchers at the University of South Florida, the Institute for Global Environmental Strategies, and the Wilson Center."},{id:4437,__component:f,text:bx,slug:by},{id:8471,__component:d,text:"Standardized disparate data sets from four mosquito-tracking apps and combined them into the [Global Mosquito Observations Dashboard](https:\u002F\u002Fexperience.arcgis.com\u002Fexperience\u002F7228a5a27442468494caec2934c2b73d). The dashboard, which helps mosquito-control workers and public health professionals manage the risk of mosquito-borne disease, takes in citizen-submitted photos and tracks larval and adult mosquito specimens, breeding habitats, and bites. Anyone who wants to analyze the data in their own way can download it from the dashboard.\n\nUsing images from the citizen-science apps, the team has also trained experimental artificial intelligence algorithms to identify mosquito species based on certain anatomical features."},{id:4438,__component:f,text:bz,slug:bA},{id:8470,__component:d,text:"Mosquitoes [kill nearly a million people per year](https:\u002F\u002Fwww.mdpi.com\u002F2075-4450\u002F13\u002F8\u002F675), but only around 3 percent of the more than 3,500 species transmit diseases to humans. To prevent the spread of disease, researchers and health officials need not only a massive, global surveillance operation but also the ability to precisely identify specimens, Ryan Carney, a USF biology professor and a leader of the project, tells MIT Horizon. \n\nBut pinpointing potential disease carriers is time-consuming and challenging. Photographic reports from the public can help, but citizen-science apps tend to generate data sets that are not interoperable, meaning they cant be combined with others, making it hard to piece together the whole picture, Carney says. They typically structure it, as far as the data standards, based on whatever their project rules are as opposed to making it interoperable and more usable down the road, he adds. Combining the data ensures that crucial real-time surveillance data can be widely seen, accessed, and compared with as much additional information as possibleas opposed to siloed within a single app."},{id:4439,__component:f,text:gT,slug:gK},{id:8472,__component:d,text:"The citizen-science apps have received tens of thousands of photographic reports of mosquitoes, breeding habitats, and bites. Carney and his colleagues used sophisticated software tools to wrangle and harmonize all of this information, making it much easier for public health officials to analyze and apply it toward risk management. For more on how big data analytics can help organizations analyze large volumes of data, see [Benefits of Big Data Analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-bd)."}],badges:[{id:405,__component:q,title:A},{id:kf,__component:q,title:gY},{id:kd,__component:q,title:"Science"},{id:id,__component:q,title:ix}],relatedLinks:[{id:203,__component:N,title:aY,url:nb},{id:204,__component:N,title:"Using Lightweight Sensors To Follow Animal Movements",url:"https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-lightweight-sensors-to-follow-animal-movements"}],tags:[]}},{id:gl,attributes:{title:br,status:b,slug:gm,publishedOn:gn,readingTime:n,commentsEnabled:c,french:c,createdAt:aD,updatedAt:aD,publishedAt:go,logo:{data:a},contentBlocks:[{id:4448,__component:f,text:bv,slug:bw},{id:8481,__component:d,text:"The Sorenson Impact Center at the University of Utah."},{id:4451,__component:f,text:bx,slug:by},{id:8482,__component:d,text:"Created a data science project that analyzes several types of information to understand and forecast enrollment trends in higher education across the United States. The projectcalled Model, Analyze, Prototype, and Share innovative solutions, or MAPSincorporates data from a variety of sources, including the U.S. Census Bureau and the U.S. Department of Educations College Scorecard and Integrated Postsecondary Education Data System.\n\nMAPS findings are primarily shared through [an interactive dashboard tool](https:\u002F\u002Fmapsapps.azurewebsites.net\u002Fstep\u002F) called the Student Trends and Enrollment Projections (STEP) dashboard. The dashboard is geared to higher education administrators and admissions personnel to help guide their resource allocation and planning. Users can filter the information on the dashboard by categories such as institution type, state, and student gender and race. In the first five months after its April 2023 launch, more than 4,000 people have used the STEP dashboard. "},{id:4450,__component:f,text:bz,slug:bA},{id:8483,__component:d,text:"MAPS provides a way for many colleges and universities to stay on top of changes in enrollment trends and plan their resources appropriately. Higher education leaders cannot rely on the same recruitment and retention strategies if they are to remain viable in the changing national landscape, Allison Boxer, managing director at the Sorenson Impact Center, tells MIT Horizon.\n\nThe STEP data dashboard is especially useful for schools that do not have the resources to conduct their own analyses of declining enrollment. The dashboard visualizes key data insights that speak to opportunities, including enrollment and population trends, how students flow across states in pursuing higher ed, and the role of online-only education, Boxer says. The MAPS project also has separate data tools to help college and university administrative officials gauge how racially and financially diverse their institutions student body is."},{id:4449,__component:f,text:gT,slug:gK},{id:8484,__component:d,text:"An abundance of data exists today, but it is not always grouped or organized in a way that makes it easy to spot patterns and trends. The MAPS project makes it possible to combine multiple sources of data to tell the student enrollment story. Through the STEP dashboard, college and university administrators can tailor the information and insights to guide their planning.\n\nGoing forward, the MAPS project team hopes to include more data, make technical enhancements, and develop more sophisticated statistical models. The MAPS project team is now exploring how the tools can continue to be made available to the sector as a public good and how institutions can be engaged to use the MAPS tools in their strategic discussions, planning, and decision-making, Boxer says. For more on how using big data analytics can inform decision-making, see [How Big Data Analytics Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-works-bd)."}],badges:[{id:416,__component:q,title:A},{id:417,__component:q,title:iw},{id:418,__component:q,title:hH}],relatedLinks:[{id:209,__component:N,title:a_,url:nc},{id:210,__component:N,title:bs,url:"https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-big-data-analytics-to-improve-clinical-trial-recruitment"}],tags:[]}},{id:gp,attributes:{title:bs,status:b,slug:gq,publishedOn:gr,readingTime:n,commentsEnabled:c,french:c,createdAt:aE,updatedAt:aE,publishedAt:gs,logo:{data:a},contentBlocks:[{id:2163,__component:w,title:"041-verana-health",type:x,caption:"Photo by National Cancer Institute via Unsplash",image:{data:a}},{id:4460,__component:f,text:bv,slug:bw},{id:8493,__component:d,text:"Verana Health, a digital health company based in San Francisco."},{id:4463,__component:f,text:bx,slug:by},{id:8494,__component:d,text:"Developed an application that draws on mountains of real-world electronic health data to connect trial sponsors with appropriate, targeted patient populations. Verana Health created VeraSite, an electronic data dashboard that includes patient information provided by more than 15,000 eye doctors. The data is *deidentified*, meaning it cannot be easily traced back to individuals, and updated daily.\n\nWith VeraSite, clinical trial sponsors have access to patient information across various clinics and data on whether those clinics have prior experience hosting trials or specialize in particular research areas. Artificial intelligence (AI) tools within VeraSite help sort the large data set. Trial sponsors supply key requirements for their study, such as diagnoses, treatment patterns, and demographics, and VeraSite curates a list of optimal potential candidates or sites for each trial."},{id:4462,__component:f,text:bz,slug:bA},{id:8495,__component:d,text:"Although trials are critical for developing treatments across medicine, methods of identifying suitable patients for trials can be slow and inaccurate. According to the National Institutes of Health, 80% of clinical trials are not able to find enough participants by their planned start date, resulting in delays. The leading cause of trial terminations is sponsors falling short of the number of patients needed to complete the trial, according to a study by GlobalData. \n\nBy incorporating records from community-based medical practices, VeraSite broadens the potential recruits for trials, diversifying medical research and making it more accurately representative of the population. The site is also able to tap into parts of medical records not normally accessible to clinical trials, such as the less standardized but valuable observations and insights of clinicians. There is a lot of value in physicians notes, Sujay Jadhav, Verana Healths chief executive officer, tells MIT Horizon. Physicians can also use VeraSite as a diagnostic tool. Through keyword searches and image scanning, VeraSites AI algorithms suggest possible diagnoses to physicians, who can confirm or reject them."},{id:4461,__component:f,text:gT,slug:gK},{id:8496,__component:d,text:"With access to more than 78 million ophthalmology patient records, VeraSite allows clinical trial sponsors to analyze far more data across many more doctors offices than they would typically have access to. VeraSite also leverages AI tools to search for key phrases to identify suitable patients for a clinical trials specific needs. As a result, VeraSite can help trial sponsors recruit patients even in instances where the requirements are complex and do not fit into a particular diagnosis or treatment plan. Overall, the technology can examine a tremendous amount of material more quickly than even a large group of humans could. The result is that institutions and individuals seeking patients for research can do so in a shorter time, at less cost, and with improved efficiency, Jadhav says. For more on how big data analytics can help organizations examine large volumes of data, see [Benefits of Big Data Analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fbenefits-bd)."}],badges:[{id:427,__component:q,title:A},{id:429,__component:q,title:gY},{id:ic,__component:q,title:iy},{id:430,__component:q,title:nd}],relatedLinks:[{id:215,__component:N,title:a_,url:nc},{id:216,__component:N,title:aZ,url:ne}],tags:[]}},{id:gt,attributes:{title:a_,status:b,slug:gu,publishedOn:gv,readingTime:u,commentsEnabled:c,french:c,createdAt:aF,updatedAt:aF,publishedAt:gw,logo:{data:a},contentBlocks:[{id:4476,__component:f,text:bv,slug:bw},{id:8509,__component:d,text:"The Urban Institute, a nonprofit that provides data to policymakers on socioeconomic trends."},{id:4477,__component:f,text:bx,slug:by},{id:8511,__component:d,text:"Used data from a major credit bureau to understand the different types and levels of debt within communities throughout the United States. Urban Institute researchers cross-reference credit bureau data that has had identification information removed with demographic statistics from the U.S. Census Bureaus American Community Survey to annually create Debt in America, [an interactive map](https:\u002F\u002Fapps.urban.org\u002Ffeatures\u002Fdebt-interactive-map\u002F?type=overall&variable=totcoll) showing how debt is distributed across U.S. communities. The map depicts how debt may affect communities differently based on demographics. For example, the institute has created charts from the maps data showing that communities where people of color are the majority often carry a heavier debt burden than other communities."},{id:4478,__component:f,text:bz,slug:bA},{id:8512,__component:d,text:"Traditional consumer surveys create economic snapshots based on a sampling of respondents. However, the results normally cant provide a community-level picture because of the need to protect survey respondents anonymity. The Urban Institutes map uses *deidentified* dataor information that is difficult to trace back to specific individuals. As a result, the Urban Institutes map can present a more granular and comprehensive representation of debt and peoples ability to pay it back. \n\nThe Urban Institutes data highlights how debt can be a burden on communities, says Kassandra Martinchek, a research associate with the institute. If you have high levels of debt, it will continue to be really hard for you to build wealth, Martinchek tells MIT Horizon. Not being able to repay [debt] is often an indicator of financial distress. \n\nPolicymakers can use the Urban Institutes data to determine how best to target economic support to struggling communities and demographic groups and to see if government interventions have the intended effect, Martinchek adds. For example, in 2019, Urban Institute researchers, among others, helped San Francisco officials [evaluate](https:\u002F\u002Fwww.urban.org\u002Fresearch\u002Fpublication\u002Frelief-government-owed-child-support-debt-and-its-effects-parents-and-children) a pilot program that paid off public assistance debt owed by low-income parents as part of child support payments. As a result, 100% of the parents child support payments went directly to their children instead of much of it going to the government. The researchers found that relief from the government-owed debt led to more consistent and on-time child support payments. Armed with this data, California lawmakers [have proposed various bills](https:\u002F\u002Fcalmatters.org\u002Fprojects\u002Flegislature-child-support-debt-collections\u002F) in the past few years to offer debt relief for government-owed child support debt in an effort to build on the success of this pilot program. \n\nThe map also can help uncover surprising ripple effects caused by policy interventions. For example, the Urban Institutes data showed that communities in the southeast U.S. with majority residents of color experienced an unexpected improvement in their credit scores during the COVID-19 pandemic. Urban Institute researchers were able to link this improvement to the economic stimulus checks the federal government provided to many Americans during the pandemic, showing the gains these communities could make with a small boost in discretionary income."},{id:4480,__component:f,text:gT,slug:gK},{id:8510,__component:d,text:"Credit bureaus and other organizations collect an immense amount of data about consumer habits. With the Debt in America effort, the Urban Institute partnered with a credit bureau to unlock that data for the public good. The institutes data analysts refined the data using input from other sources and statistical analysis. By visualizing U.S. debt data and connecting it to demographics, the Urban Institutes map allows researchers and policymakers to glean valuable insights that might not otherwise be revealed. To learn more ways in which big data analytics is applied, see [How Big Data Analytics Is Used Today in Industry](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-is-used-today-in-industry)."},{id:4479,__component:f,text:hF,slug:hG},{id:8513,__component:d,text:"Data analysis often benefits from the use of machine learninga type of artificial intelligence that, when presented with many pieces of data, can learn patterns and make informed predictions. Every statistical model, no matter how comprehensive, is going to have some holes in it. The Census Bureau survey that the institute uses to identify community demographics falls short in sparsely populated regions. Urban Institute researchers feed the existing data into a machine learning algorithm, creating a predictive model that can fill in the gaps where data is scarce. For more on how AI is used in data analysis, see [How Big Data Analytics Works](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fhow-big-data-analytics-works-bd)."}],badges:[{id:445,__component:q,title:A},{id:447,__component:q,title:gY},{id:446,__component:q,title:"Machine Learning"},{id:450,__component:q,title:iw},{id:449,__component:q,title:iz},{id:448,__component:q,title:hH}],relatedLinks:[{id:223,__component:N,title:aY,url:nb},{id:224,__component:N,title:aZ,url:ne}],tags:[]}},{id:gx,attributes:{title:gy,status:b,slug:gz,publishedOn:gA,readingTime:a,commentsEnabled:c,french:c,createdAt:aG,updatedAt:aG,publishedAt:gB,logo:{data:{id:nf,attributes:{name:ng,alternativeText:a,caption:a,width:aU,height:aU,formats:a,hash:nh,ext:r,mime:s,size:gX,url:ni,previewUrl:a,provider:j,provider_metadata:a,createdAt:hI,updatedAt:hI}}},contentBlocks:[{id:4515,__component:f,text:bv,slug:bw},{id:8544,__component:d,text:"Air Mobility Command, a team of Pacific Air Forces Airmen called Tron, and the Department of the Air Force (DAF)-MIT AI Accelerator."},{id:4512,__component:f,text:bx,slug:by},{id:8545,__component:d,text:"Developed a system to digitize flight scheduling for Air Force personnel, transforming a time-consuming process conducted on physical whiteboards into one that is nimble and portable. The new scheduling software, called Puckboard, incorporates information about aircrew members availability, training they are due to complete, and other important details into an interactive digital dashboard. Schedulers instantly have access to this information as they are searching for personnel to fill a flight crew. Individual aircrew members can also view their upcoming flight schedules. Around 25,000 Air Force personnel use Puckboard for scheduling.\n\nPuckboard will soon add dynamic scheduling capabilities using artificial intelligence (AI). The DAF-MIT AI Accelerator is supporting development of an intelligent plug-in that recommends specific personnel for flights based on their availability and required training. The plug-ins algorithm will provide explanations for those suggestions and human schedulers will make final decisionsboth critical steps to ensure staff members are comfortable using the technology."},{id:4513,__component:f,text:bz,slug:bA},{id:8547,__component:d,text:"With our testing, we found a drastic reduction in the amount of time it would take to solve the assignment problem, Mike Snyder, the projects lead technical staff member at the MIT Lincoln Lab, tells MIT Horizon. Whos going to sit in what seat for a typical two to three week squadron schedule? We can solve that in seconds. A human scheduler, by contrast, would need to pull information from multiple systems to understand which crew members are available, what training they might be overdue for, and other relevant details. Puckboard allows schedulers to instantly view information that previously may have taken several hours, days, or even weeks to collect. Because Puckboard is digital, staff can access it from anywhere and schedulers can make changes on the gofeats that werent possible with the physical whiteboards the Air Force previously relied on\n\nThe AI plug-in is likely to make scheduling even more efficient. Its algorithm can process much more data at once than a human scheduler can, allowing Puckboard to more easily account for the trade-offs of juggling personnel across multiple missions and training requirements. This will free up schedulers time to focus on interpersonal aspects such as soliciting and addressing staff feedback on scheduling decisions."},{id:4514,__component:f,text:gT,slug:gK},{id:8546,__component:d,text:"The phrase big data analytics has connotations beyond the size of the data set. It also implies data gathered quickly and continuously, from many sources and in many different formats. For example, much of the Air Forces training data is housed in a legacy records system, and the U.S. Air Force-MIT AI Accelerator worked with a vendor to integrate that system with Puckboard. Now were able to take advantage of that to get live snapshots of real training data, Eric Robinson, development chief for AI-assisted scheduling at the Air Force-MIT AI Accelerator, tells MIT Horizon. When organizations are able to access and analyze their data in one digital location, they can find opportunities to become more efficient. For more on how big data analytics can help organizations operate more efficiently, see [Benefits of Big Data Analytics](\u002Farticle\u002Fbenefits-bd). \n"},{id:4516,__component:f,text:hF,slug:hG},{id:8548,__component:d,text:"AI is useful for analyzing data with many variables in play, including different types of variables that may be interconnected and interdependent. Armed with that data, Puckboards AI plug-in can identify patterns that might not be discernible by a human scheduler. The project team is exploring ways to improve the intelligent plug-in with techniques such as reinforcement learninga method of training in which an algorithm essentially learns by doing and is rewarded when it behaves in the desired way. It would be awesome if Puckboard is able to learn what a good schedule means from existing data and from habit patterns, Robinson says. For example, using reinforcement learning, schedulers could provide the AI system with the feedback it receives from aircrew members. For more on how AI can help make sense of complex conditions, see [Benefits of AI](\u002Farticle\u002Fbenefits-ai#ai-can-help-make-sense-of-complex-conditions-2)."}],badges:[{id:465,__component:q,title:gY},{id:466,__component:q,title:A},{id:468,__component:q,title:"Defense"},{id:469,__component:q,title:iz},{id:467,__component:q,title:hH}],relatedLinks:[{id:j_,__component:N,title:"Using AI to Fill Gaps in Global Weather Sensing",url:"\u002Fimpact-spotlight\u002Fusing-ai-to-fill-gaps-in-global-weather-sensing"}],tags:[]}},{id:gC,attributes:{title:gD,status:b,slug:gE,publishedOn:gF,readingTime:n,commentsEnabled:c,french:c,createdAt:aH,updatedAt:aH,publishedAt:gG,logo:{data:{id:nf,attributes:{name:ng,alternativeText:a,caption:a,width:aU,height:aU,formats:a,hash:nh,ext:r,mime:s,size:gX,url:ni,previewUrl:a,provider:j,provider_metadata:a,createdAt:hI,updatedAt:hI}}},contentBlocks:[{id:4546,__component:f,text:bv,slug:bw},{id:8580,__component:d,text:"RecoveryOhio, a state-led initiative to combat substance use and mental health disorders."},{id:4547,__component:f,text:bx,slug:by},{id:8577,__component:d,text:"Created a dashboard to collect and visualize data related to opioid use, overdoses, and deaths from across the states 88 counties. RecoveryOhio pulled together 50 data streams collected from four state agenciesthe first time the state of Ohio has gathered and organized disparate data sets and made them available to the public. The initiative built on a study funded by the National Institutes of Health and focused on effective tools to address opioid use at the local level. As part of that study, Ohio-based researchers created locally focused data dashboards for 18 of the states counties. To expand the work, Ohio agencies then asked local communities to identify metrics that would be most helpful to include in the dashboards, and then expanded the project to include data for the entire state. The dashboard initially focuses solely on opioids, but in the next phase of the project, the state hopes to expand the information to include all substance use."},{id:4548,__component:f,text:bz,slug:bA},{id:8578,__component:d,text:"In Ohio, local communities make their own decisions about mental health and substance use services for residents, so providing data and comparison points with the rest of the state are essential, Aimee Shadwick, the director of RecoveryOhio, tells MIT Horizon. We heard a lot from our local community about the need for more data to make more informed decisions at the local community level, she says. The dashboards show county details for overdoses and overdose deaths, distribution of naloxone (a medication that can reverse opioid overdose), and many other metrics. \n\nPresenting those data points in a central online location allows communities to see patterns and find ways to provide services to people who need them. For instance, one community noticed that many people treated with naloxone by EMS teams chose not to visit the hospital, Shadwick says. In response, the community developed a program to leave naloxone with people treated by EMS for overdose, so they could have the medication readily available in case they experienced another overdose. \n\nThe dashboard has also allowed for coordination across state and local jurisdictions. For example, when a county noticed through the data that overdose deaths were increasing at its rest areas, it was able to coordinate with the state, which is responsible for rest areas, to install a store of naloxone at those roadside stops. Unfortunately, we dont have an infinite amount of resources, and so using data to make informed decisions about where to target those resources, both at the state and local levels, is really important, Shadwick says. "},{id:4545,__component:f,text:gT,slug:gK},{id:8579,__component:d,text:"Thousands of people in Ohio die each year from drug overdoses. From 2019 to 2021, drug overdose deaths in the state grew by more than a quarter. Data analytics helps local and state authorities see those individual events for their larger trends, so they can direct community support resources to the areas with the greatest need in order to save lives. Presenting the data online also allows transparent public engagement with the dashboard and its insights. For more on how companies and communities are using data to guide their decisions, see [Recent Developments in Big Data Analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Frecent-developments-bd)."}],badges:[{id:502,__component:q,title:A},{id:501,__component:q,title:iy},{id:499,__component:q,title:iz},{id:y,__component:q,title:nd}],relatedLinks:[{id:aI,__component:N,title:br,url:"https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-data-analytics-to-understand-enrollment-trends"},{id:246,__component:N,title:bq,url:"https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-big-data-analytics-to-track-deadly-mosquitoes"}],tags:[]}}]}}}},technical_topic:{data:a},localizations:[]},topics:[{id:I,fields:{title:A,slug:bd,subtitle:be,underDevelopment:c,hidden:a,knowledgeChecks:bf,createdAt:bg,updatedAt:bh,publishedAt:bi},sys:{id:I}}],featured:a}],fetch:{"data-v-76081bd4:0":{banner:a}},error:a,state:{isSidebarOpen:c,isModalOpen:c,showModal:c,user:{userId:"a2d09df0-8697-4f16-b6f5-387ea85fa55d",userEmail:"hiren.patel@afit.edu",userMetadata:{},reAuthAt:1701912013,userPermissions:["read:video-collections"]},learningTrack:{error:a,consent:{},track:{},tasks:{},tracks:[]}},serverRendered:i,routePath:"\u002Farticle\u002Fcommon-misconceptions-bda",config:{AIR_FORCE:c,_app:{basePath:"\u002F",assetsPath:"\u002F_nuxt\u002F",cdnURL:a}}}}(null,"Released",false,"content-blocks.article-paragraph","en","content-blocks.article-heading","2020-12-18","image\u002Fjpeg",true,"aws-s3","30 min",".jpg","Introductory","3 min read",".png","image\u002Fpng","impact-spotlights.badges",".svg","image\u002Fsvg+xml","2021-06-25","4 min read","content-blocks.article-subheading","content-blocks.article-image","body",500,156,"Big Data Analytics",750,"Big Data, Little Data, No Data","High-Performance Big Data Computing","content-blocks.article-term","2021-04-07","2023-10-18T21:40:12.845Z",600,75,"The Smart Enough City","Self-Tracking (MIT Press Essential Knowledge series)","2016","Reality Mining","impact-spotlights.related-links","2023-10-18T21:48:35.352Z",1000,263,"2023-10-18T21:48:36.166Z","2023-10-18T21:45:12.132Z","2023-10-18T21:45:15.336Z","2023-10-18T21:45:19.966Z","2023-10-18T21:45:20.549Z","2023-10-18T21:45:21.327Z","2023-10-18T21:45:22.663Z","2023-10-18T21:45:23.205Z","2023-10-18T21:45:29.169Z","2023-10-18T21:45:37.402Z","2023-10-18T21:37:30.869Z","2023-10-18T21:39:59.970Z","13 min read","2023-10-18T21:39:56.185Z","2021-02-09","2023-10-18T21:39:53.332Z","2021-04-12","2023-10-18T21:39:52.265Z","2021-05-28","8 min read","2023-10-18T21:37:30.199Z","2023-10-18T21:41:37.388Z","2020-10-15","2023-10-18T21:42:06.525Z","2020-09-28","2023-10-18T21:42:05.624Z","2020-05-27","2023-10-18T21:42:05.411Z","2023-10-18T21:40:25.612Z","2023-10-18T21:42:05.117Z","2023-10-18T21:40:12.278Z","2023-10-18T21:42:05.193Z","2019","Intermediate","Discover how to use data more effectively by incorporating data science with big data analytics.","2023-10-18T21:49:07.278Z","2023-10-18T21:49:07.366Z","2023-10-18T21:49:16.388Z","2023-10-18T21:49:19.427Z","2023-10-18T21:49:36.313Z","2023-10-18T21:49:36.504Z","2023-10-18T21:49:36.710Z","2023-10-18T21:49:37.020Z","2023-11-16T14:06:04.364Z","2023-11-16T14:06:04.991Z",245,"Trusted Data, Revised And Expanded Edition: A New Framework for Identity and Data Sharing","Streaming Sharing Stealing: Big Data and the Future of Entertainment","Data Science","content-blocks.article-list",300,"2023-10-18T21:44:47.061Z",157,461,138,"Glossary","content-blocks.article-web",200,139,"data-science","2018","Tracking Sustainability with Big Data Analytics","Using AI to Prevent Suicide","Using Big Data Analytics To Visualize Demographic Disparities","Common Misconceptions","common-misconceptions-bda","6 min read","2023-02-20T18:42:28.219Z","big-data-analytics","How to make sense of extremely large data sets","https:\u002F\u002Fdigitalu.af.mil\u002Fapp\u002Fassessments\u002Fdabe206f-9bb3-4302-b61c-c651e14c7767\u002F9efc94f0-c630-4e0e-b225-7056d61d305d","2023-10-18T21:48:36.209Z","2023-11-16T14:06:20.126Z","2023-02-22T14:45:18.714Z",9.83,"Benefits","Limitations",572,141,123,"Using Big Data and AI to Monitor Industrial Equipment","Using Big Data Analytics to Track Deadly Mosquitoes","Using Data Analytics to Understand Enrollment Trends","Using Big Data Analytics to Improve Clinical Trial Recruitment",".gif","image\u002Fgif","Who did it","who-did-it","What they did","what-they-did","How it helped","how-it-helped","content-blocks.article-audio",2304,"Big_Data_ICON_f441e5ba46.svg","Big_Data_ICON_f441e5ba46_b26d9ad7fe","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBig_Data_ICON_f441e5ba46_b26d9ad7fe.svg",2305,"Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe.jpg",4000,1400,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Flarge_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d.jpg","large_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d","large_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe.jpg",34.46,350,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fsmall_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d.jpg","small_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d","small_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe.jpg",12.31,175,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fmedium_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d.jpg","medium_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d","medium_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe.jpg",22.25,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fthumbnail_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d.jpg","thumbnail_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d","thumbnail_Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe.jpg",4.24,86,"Big_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d",509.29,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBig_Data_HERO_pexels_manuel_geissinger_325229_4befdbe7fe_e7493c793d.jpg",250,"Using Yesterdays Medical Data to Train Tomorrows AI","using-yesterdays-medical-data-to-train-tomorrows-ai","2021-12-02T16:00:00.000Z","In the last few years, hospitals have moved to digitize large volumes of patient health records. This mass digitization includes data such as how individual patients respond to certain treatments, but most of the data is unstructured and hard to interpret. Recent MIT PhD graduate Ruizhi Ray Liao will join us to discuss his work on an AI that finds value in that data and provides new insights into what interventions and treatments work best. After the talk, Ray will take questions from the audience.","__Ruizhi Ray Liao__ earned a computer science PhD at MITs Computer Science & Artificial Intelligence Lab. For his doctoral work, he studied machine learning and developed new algorithms driven by clinical problems. Prior to coming to MIT, Ray received a bachelor's degree from Tsinghua University, in China. Currently, he is building the AI system [Empallo](https:\u002F\u002Fempallo.com\u002F), whose goal is to revolutionize the fight against heart failure, and he is excited about ubiquitous computing and its potential to advance health care.","https:\u002F\u002Fmit.zoom.us\u002Fwebinar\u002Fregister\u002FWN_jpf9EtYdReiasl4pnzTzYw","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FUsingYesterdaysMedicalDatatoTrainTomorrowsAI","2023-05-18T03:13:41.536Z","Storing the Worlds Data in a Coffee Mug Using DNA Data Storage","storing-the-worlds-data-in-a-coffee-mug-using-dna-data-storage","2021-08-18T15:00:00.000Z","As our societys data needs and data generation continue to grow, we are approaching a datageddon, in which data generation will outpace our ability to store data. Researchers are searching for innovative ways to store the maximum amount of data using the least number of atoms. One option may be to use DNA, which can potentially store the entire worlds data in the space of a coffee mug. MIT postdoc James Banal will join us to discuss his teams work on using DNA to store and retrieve files. He will explain the process and describe some challenges that need to be overcome before it can be widely adopted. After the talk, James will take questions from the audience.","__James Banal__ is a postdoctoral research associate in Professor Mark Bathes lab in the MIT Department of Biological Engineering. A chemist by training, he is passionate about cutting-edge science that involves multidisciplinary approaches, ranging from synthesizing molecules to shooting lasers and harnessing the power of biology. He earned his PhD in chemistry at the University of Melbourne, in Australia, developing strategies to make any flat surface harvest sunlight to generate electricity.","https:\u002F\u002Fmit.zoom.us\u002Fwebinar\u002Fregister\u002FWN_wwU8giMUQVOPsgUZVsTzTQ","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FStoringtheWorldsDatainaCoffeeMugUsingDNADataStorage","2023-05-18T03:14:02.947Z",281,"I Know What You Bought at Chipotle: How Small Data Unlocks Market Intelligence That Big Data Cant","i-know-what-you-bought-at-chipotle-how-small-data-unlocks-market","2021-03-24T18:00:00.000Z","Michael Fleder (MIT PhD 19) will join us to discuss his research on algorithms that break down financial transactions. His methods let anyone estimate corporations hidden financials from widely available anonymized data. This research makes it possible to estimate total sales at retailers and infer the details of customers spending. For example, given the total of a single consumers bill, the algorithm automatically infers the number and prices of products purchased. Michaels techniques help businesses track competitors, investments, consumers, supply chains, and themselves through these small data algorithms that focus on individuals spending. After the presentation, Michael will take questions from the audience. ","__Michael Fleder__ is the founder of [Covariance.ai](https:\u002F\u002Fwww.covariance.ai\u002F)a machine-learning startup based on the work underlying his PhD. His work has been featured in MIT News ([2019](https:\u002F\u002Fnews.mit.edu\u002F2019\u002Fmodel-beats-wall-street-forecasts-business-sales-1219), [2021](https:\u002F\u002Fnews.mit.edu\u002F2021\u002Fi-know-what-you-bought-at-chipotle-consumer-algorithm-0202)) and leading modeling conferences ([2020](https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3366694), [2021](https:\u002F\u002Fdl.acm.org\u002Fdoi\u002F10.1145\u002F3428332)). His prior background is in algorithmic trading and in robotics (MIT, NASA\u002FJPL). Michael received his doctorate under advisor Devavrat Shah.","https:\u002F\u002Fapp.livestorm.co\u002Fmit-horizon\u002Fi-know-what-you-bought-at-chipotle-how-small-data-unlocks-market-intelligence-that-big-data-cant?type=detailed","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FIKnowWhatYouBoughtatChipotleHowSmallDataUnlocksMarketIntelligenceThatBigDataCant","2023-05-18T03:14:31.507Z",283,"How Big Data Can Benefit Small Businesses With Clark Twiddy","how-big-data-can-benefit-small-businesses-with-clark-twiddy","2021-03-03T16:00:00.000Z","Clark Twiddy, president of [Twiddy & Company](https:\u002F\u002Fwww.twiddy.com\u002F), will join us to discuss how big data enabled his business to model a path to success during 2020 and into 2021, the most challenging period in the company's 40-plus-year history. He will explore the benefits and challenges of small to medium-sized businesses utilizing technology effectively, often while working with limited resources. After the presentation, Clark will take questions from the audience.","__Clark Twiddy__ is the president of Twiddy & Company, a hospitality and asset management firm along North Carolinas Outer Banks. Managing more than a billion dollars worth of clients real estate investment and employing more than 150 full-time staff, Twiddy & Company is a second-generation family business.\n\nPrior to his time at Twiddy & Company, Clark worked in global intelligence as a naval intelligence officer and with the U.S. Treasury Departments financial intelligence group in combating global terrorism through financial channels. He is on the boards of public, private, nonprofit, and government groups.","https:\u002F\u002Fapp.livestorm.co\u002Fmit-horizon\u002Fhow-big-data-can-benefit-small-businesses-with-clark-twiddy?type=detailed","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FHowBigDataCanBenefitSmallBusinessesWithClarkTwiddy","2023-05-18T03:14:34.650Z",286,"Optimization, Machine Learning, and Big Data in Healthcare With David Scheinker","optimization-machine-learning-and-big-data-in-healthcare-with-david","2021-01-28T19:00:00.000Z","Stanford professor David Scheinker and founder of Systems Utilization Research for Stanford Medicine will join us to explore the difference between theory and practice in big data usage in the health care sector. He will discuss ways in which data science and operations research can be used for relevant research and measurable improvement to care. After the presentation, David will take questions from the audience.","__David Scheinker__ is the director of systems design and collaborative research at the Stanford [Lucile Packard Childrens Hospital](https:\u002F\u002Fwww.stanfordchildrens.org\u002F). He is the founder and director of [SURF Stanford Medicine](https:\u002F\u002Fsurf.stanford.edu\u002F), a group that brings together students and faculty from the university with physicians, nurses, and administrators from the hospitals to improve the quality of care using operations research methodology. He received a PhD in theoretical math from the University of California San Diego under Jim Agler. Before coming to Stanford, he was a joint research fellow at the MIT Sloan School of Management and Massachusetts General Hospital. His current areas of research include applications of operations research in health care, health care policy, mathematical control theory, and functional analysis.\n\nConcurrently with his university appointments, David has spent time teaching theoretical math to gifted 11- and 12-year-old students for the Johns Hopkins Center for Talented Youth. He is writing a popular math book titled *Infinity in Wonderland* with the intent of bringing the material of these courses to a wider audience.\n","https:\u002F\u002Fapp.livestorm.co\u002Fmit-horizon\u002Foptimization-machine-learning-and-big-data-in-healthcare-with-david-scheinker","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FOptimizationMachineLearningandBigDatainHealthcareWithDavidScheinker","2023-05-18T03:14:39.295Z",290,"Understanding the Business and Technology Behind E-sports With Baro Hyun","understanding-the-business-and-technology-behind-e-sports-with-baro-hyun","2021-01-13T21:00:00.000Z","Dr. Baro Hyun will join us to discuss the world of e-sports. He will explore the business environment behind e-sports and explain how e-sports uses a variety of emerging technologies, from data analytics to 5G and more. After the presentation, Baro will take questions from the audience.","__Baro Hyun__ is the founder of an e-sports advisory practice at a Tokyo-based Big Four consulting firm, with clients including the Japanese government. Baro teaches e-sports business at Keio University, and his writing frequently appears in major media outlets including The Nikkei, the worlds largest-circulation financial newspaper. He recently published the book *Demystifying Esports: A Personal Guide to the History and Future of Competitive Gaming*.","https:\u002F\u002Fapp.livestorm.co\u002Fmit-horizon\u002Funderstanding-the-business-and-technology-behind-e-sports-with-baro-hyun","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FUnderstandingtheBusinessandTechnologyBehindEsportsWithBaroHyun","2023-05-18T03:14:45.898Z",292,"Using Big Data to Understand User Behavior With Matthieu Di Mercurio","using-big-data-to-understand-user-behavior-with-matthieu-di-mercurio","2020-12-10T16:00:00.000Z","Data scientist Matthieu Di Mercurio will join us to discuss using data to drive product and design decisions. He will talk about understanding which features to build, how to eliminate key user pain points, and opportunities for improvement. He will also explore how linking small and big data can help to gain richer insights into your users behavior and how to leverage these insights to build a better, more user-friendly product. Finally, he will cover key tactics to leverage data in road map prioritization and product feature evaluation. After the presentation, Matthieu will take questions from the audience.","__Matthieu Di Mercurio__ is director of data science at [Dialogue](https:\u002F\u002Fwww.dialogue.co\u002Fen\u002F), an online health platform in Montreal, where he develops data pipelines and tools to support decision-making and platforms based on data, and also supports the companys analytics needs. Matthieu is also a co-founder of MTL DATA, the largest community of data scientists in Montreal. Prior to joining Dialogue, Matthieu worked at Intel Security and Ubisoft, leading analytics on the Assassin's Creed brand.","https:\u002F\u002Fapp.livestorm.co\u002Fmit-horizon\u002Fusing-big-data-to-understand-user-behavior-with-matthieu-di-mercurio","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FUsingBigDatatoUnderstandUserBehaviorWithMatthieuDiMercurio","2023-05-18T03:14:49.157Z",315,"The Impact of Big Data on Cybersecurity With Michael Gioia","the-impact-of-big-data-on-cybersecurity-with-michael-gioia","2020-06-19T15:00:00.000Z","Cybersecurity expert Michael Gioia will explore the intersection between cybersecurity and big data and will help us understand the benefits and drawbacks that data has for privacy. As always, we'll open up to live questions in the final portion of the event.","__Michael Gioia__ is an information security leader with 17 years of experience delivering security solutions across multiple industries. He was an officer in the U.S. Air Force and has worked in higher education, the Department of Defense, retail food services, and security consulting. He has performed most of his information security work in higher education as an information security officer at Eastern Illinois University, Rose-Hulman Institute of Technology, and now Bentley University.\n\nMichael is credentialed as a Certified Information Security Manager (CISM) from ISACA, Certified Information System Security Professional (CISSP) from ISC2, GIAC Security Leadership Certification (GSLC) from SANS, and Payment Card Industry Professional (PCIP) from the PCI Security Standards Council. ","https:\u002F\u002Fapp.livestorm.co\u002Fmit-horizon\u002Fthe-impact-of-big-data-on-cybersecurity-ith-michael-gioia","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FTheImpactofBigDataonCybersecurityWithMichaelGioia","2023-05-18T03:15:26.905Z",346,"Fireside Chat | Organizational Structures Around Data","fireside-chat-or-organizational-structures-around-data","2024-12-31T17:00:00.000Z","Harnessing data has become imperative for organizations seeking growth, expansion, and even survival. In addition to recruiting the right talent and embracing new skill sets, companies must think about the way teams and functions are structured around data. While there is no one-size-fits-all approach, creating a strong organizational framework for data is paramount. This framework facilitates a shift in mindset, enhances internal expertise, optimizes data utilization, and fosters exploration of new business opportunities. In this event, __Pablo Emilio Fernndez Benavidez__, head of business intelligence in Spain for Santander, an international banking firm, will tackle the unique challenges each company faces in making a well-designed data-oriented structure.\n\nThis event already took place. Check out the event recording below.","__Pablo Emilio Fernndez Benavidez__ serves as the head of business intelligence at Santander Spain. With over three decades of experience in the banking industry, he has held numerous influential roles in business and data analytics, spanning countries such as Mexico, Brazil, and Argentina.","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Fvideo\u002FFiresideChatOrganizationalStructuresAroundData","2023-08-16T13:48:24.711Z",380,"Executive Summary","executive-summary-bd","2023-02-08T23:40:15.000Z",436,"How Big Data Analytics Works","how-big-data-analytics-works-bd","11 min read","2023-02-08T23:40:57.004Z",435,"benefits-bd","2023-02-08T23:40:56.432Z",434,"limitations-bd","2023-02-08T23:40:55.831Z",433,"Recent Developments","recent-developments-bd","12 min read","2023-02-08T23:40:54.768Z",379,"How Big Data Analytics Is Used Today in Industry","how-big-data-analytics-is-used-today-in-industry","2023-02-08T23:40:14.672Z",547,"glossary-bd","2023-02-20T19:36:08.435Z",573,"Data Sharing 2.0: New Data Sharing, New Value Creation","data-sharing-2-0-new-data-sharing-new-value-creation","10 min read","2023-02-20T19:38:11.541Z","Getting Serious About Data and Data Science","getting-serious-about-data-and-data-science","2023-02-20T19:38:10.883Z",571,"How to Build a Data Analytics Dream Team","how-to-build-a-data-analytics-dream-team","9 min read","2023-02-20T19:38:10.293Z",478,"Courses","courses-bd","2023-02-20T19:35:24.449Z",569,"Books","books-bd","2023-02-20T19:38:09.193Z",453,"Conferences","conferences-bd","2023-02-20T18:23:19.063Z",570,"On the Web","on-the-web-bd","2023-02-20T19:38:09.728Z",193,"Data Science (MIT Press Essential Knowledge series)","A concise introduction to the emerging field of data science, explaining its evolution, relation to machine learning, current uses, data infrastructure issues, and ethical challenges.","2023-10-18T21:44:44.602Z","2023-11-16T14:06:19.177Z","2023-03-02T20:53:01.547Z","big-data-little-data-no-data","An examination of the uses of data within a changing knowledge infrastructure, offering analysis and case studies from the sciences, social sciences, and humanities.\n\nBig Data is on the covers of *Science*, *Nature*, the *Economist*, and *Wired* magazines, on the front pages of the *Wall Street Journal* and the *New York Times*. But despite the media hyperbole, as Christine Borgman points out in this examination of data and scholarly research, having the right data is usually better than having more data; little data can be just as valuable as big data. In many cases, there are no databecause relevant data don't exist, cannot be found, or are not available. Moreover, data sharing is difficult, incentives to do so are minimal, and data practices vary widely across disciplines.\n\nBorgman, an often-cited authority on scholarly communication, argues that data have no value or meaning in isolation; they exist within a knowledge infrastructurean ecology of people, practices, technologies, institutions, material objects, and relationships. After laying out the premises of her investigationsix provocations meant to inspire discussion about the uses of data in scholarshipBorgman offers case studies of data practices in the sciences, the social sciences, and the humanities, and then considers the implications of her findings for scholarly practice and research policy. To manage and exploit data over the long term, Borgman argues, requires massive investment in knowledge infrastructures; at stake is the future of scholarship.","2015","2023-10-18T21:44:26.969Z","2023-11-16T14:06:18.158Z","2023-03-02T20:52:40.206Z",179,"Trusted Data: A New Framework for Identity and Data Sharing","trusted-data","How to create an Internet of Trusted Data in which insights from data can be extracted without collecting, holding, or revealing the underlying data.","2023-10-18T21:44:38.004Z","2023-11-16T14:06:18.762Z","2023-03-02T20:52:52.626Z",168,"Smart Cities (MIT Press Essential Knowledge series)","smart-cities-mit-press-essential-knowledge-series","Key concepts, definitions, examples, and historical contexts for understanding smart cities, along with discussions of both drawbacks and benefits of this approach to urban problems.\n\nOver the past ten years, urban planners, technology companies, and governments have promoted smart cities with a somewhat utopian vision of urban life made knowable and manageable through data collection and analysis. Emerging smart cities have become both crucibles and showrooms for the practical application of the Internet of Things, cloud computing, and the integration of big data into everyday life. Are smart cities optimized, sustainable, digitally networked solutions to urban problems? Or are they neoliberal, corporate-controlled, undemocratic non-places? This volume in the MIT Press Essential Knowledge series offers a concise introduction to smart cities, presenting key concepts, definitions, examples, and historical contexts, along with discussions of both the drawbacks and the benefits of this approach to urban life.\n\nAfter reviewing current terminology and justifications employed by technology designers, journalists, and researchers, the book describes three models for smart city developmentsmart-from-the-start cities, retrofitted cities, and social citiesand offers examples of each. It covers technologies and methods, including sensors, public wi-fi, big data, and smartphone apps, and discusses how developers conceive of interactions among the built environment, technological and urban infrastructures, citizens, and citizen engagement. Throughout, the authorwho has studied smart cities around the worldargues that smart city developers should work more closely with local communities, recognizing their preexisting relationship to urban place and realizing the limits of technological fixes. Smartness is a means to an end: improving the quality of urban life.","2020","2023-10-18T21:44:32.377Z","2023-11-16T14:06:18.489Z","2023-03-02T20:52:46.320Z",161,"the-smart-enough-city","Why technology is not an end in itself, and how cities can be smart enough, using technology to promote democracy and equity.\n\nSmart cities, where technology is used to solve every problem, are hailed as futuristic urban utopias. We are promised that apps, algorithms, and artificial intelligence will relieve congestion, restore democracy, prevent crime, and improve public services. In *The Smart Enough City*, Ben Green warns against seeing the city only through the lens of technology; taking an exclusively technical view of urban life will lead to cities that appear smart but under the surface are rife with injustice and inequality. He proposes instead that cities strive to be smart enough: to embrace technology as a powerful tool when used in conjunction with other forms of social changebut not to value technology as an end in itself.\n\nIn a technology-centric smart city, self-driving cars have the run of downtown and force out pedestrians, civic engagement is limited to requesting services through an app, police use algorithms to justify and perpetuate racist practices, and governments and private companies surveil public space to control behavior. Green describes smart city efforts gone wrong but also smart enough alternatives, attainable with the help of technology but not reducible to technology: a livable city, a democratic city, a just city, a responsible city, and an innovative city. By recognizing the complexity of urban life rather than merely seeing the city as something to optimize, these Smart Enough Cities successfully incorporate technology into a holistic vision of justice and equity.","2023-10-18T21:44:29.010Z","2023-11-16T14:06:18.273Z","2023-03-02T20:52:42.414Z","self-tracking-mit-press-essential-knowledge-series","What happens when people turn their everyday experience into data: an introduction to the essential ideas and key challenges of self-tracking.\n\nPeople keep track. In the eighteenth century, Benjamin Franklin kept charts of time spent and virtues lived up to. Today, people use technology to self-track: hours slept, steps taken, calories consumed, medications administered. Ninety *million* wearable sensors were shipped in 2014 to help us gather data about our lives. This book examines how people record, analyze, and reflect on this data, looking at the tools they use and the communities they become part of. Gina Neff and Dawn Nafus describe what happens when people turn their everyday experiencein particular, health and wellness-related experienceinto data, and offer an introduction to the essential ideas and key challenges of using these technologies. They consider self-tracking as a social and cultural phenomenon, describing not only the use of data as a kind of mirror of the self but also how this enables people to connect to, and learn from, others.\n\nNeff and Nafus consider what's at stake: who wants our data and why; the practices of serious self-tracking enthusiasts; the design of commercial self-tracking technology; and how self-tracking can fill gaps in the healthcare system. Today, no one can lead an entirely untracked life. Neff and Nafus show us how to use data in a way that empowers and educates.","2023-10-18T21:44:19.314Z","2023-11-16T14:06:17.716Z","2023-03-02T20:52:31.177Z",131,"Reality Mining: Using Big Data to Engineer a Better World","reality-mining-using-big-data-to-engineer-a-better-world","A look at how Big Data can be put to positive use, from helping users break bad habits to tracking the global spread of disease.\n\nBig Data is made up of lots of little data: numbers entered into cell phones, addresses entered into GPS devices, visits to websites, online purchases, ATM transactions, and any other activity that leaves a digital trail. Although the abuse of Big Datasurveillance, spying, hackinghas made headlines, it shouldn't overshadow the abundant positive applications of Big Data. In *Reality Mining*, Nathan Eagle and Kate Greene cut through the hype and the headlines to explore the positive potential of Big Data, showing the ways in which the analysis of Big Data (Reality Mining) can be used to improve human systems as varied as political polling and disease tracking, while considering user privacy.\n\nEagle, a recognized expert in the field, and Greene, an experienced technology journalist, describe Reality Mining at five different levels: the individual, the neighborhood and organization, the city, the nation, and the world. For each level, they first offer a nontechnical explanation of data collection methods and then describe applications and systems that have been or could be built. These include a mobile app that helps smokers quit smoking; a workplace knowledge system; the use of GPS, Wi-Fi, and mobile phone data to manage and predict traffic flows; and the analysis of social media to track the spread of disease. Eagle and Greene argue that Big Data, used respectfully and responsibly, can help people live better, healthier, and happier lives.","2014","2023-10-18T21:44:14.296Z","2023-11-16T14:06:17.436Z","2023-03-02T20:52:25.676Z","Streaming, Sharing, Stealing: Big Data and the Future of Entertainment","streaming-sharing-stealing","How big data is transforming the creative industries, and how those industries can use lessons from Netflix, Amazon, and Apple to fight back.\n\nTraditional network television programming has always followed the same script: executives approve a pilot, order a trial number of episodes, and broadcast them, expecting viewers to watch a given show on their television sets at the same time every week. But then came Netflix's *House of Cards*. Netflix gauged the show's potential from data it had gathered about subscribers' preferences, ordered two seasons without seeing a pilot, and uploaded the first thirteen episodes all at once for viewers to watch whenever they wanted on the devices of their choice.\n\nIn this book, Michael Smith and Rahul Telang, experts on entertainment analytics, show how the success of *House of Cards* upended the film and TV industriesand how companies like Amazon and Apple are changing the rules in other entertainment industries, notably publishing and music. We're living through a period of unprecedented technological disruption in the entertainment industries. Just about everything is affected: pricing, production, distribution, piracy. Smith and Telang discuss niche products and the long tail, product differentiation, price discrimination, and incentives for users not to steal content. To survive and succeed, businesses have to adapt rapidly and creatively. Smith and Telang explain how.\n\nHow can companies discover who their customers are, what they want, and how much they are willing to pay for it? Data. The entertainment industries, must learn to play a little moneyball. The bottom line: follow the data.","2023-10-18T21:44:09.226Z","2023-11-16T14:06:17.208Z","2023-03-02T20:52:21.325Z",102,"high-performance-big-data-computing","An in-depth overview of an emerging field that brings together high-performance computing, big data processing, and deep learning. Over the last decade, the exponential explosion of data known as big data has changed the way we understand and harness the power of data. The emerging field of high-performance big data computing, which brings together high-performance computing (HPC), big data processing, and deep learning, aims to meet the challenges posed by large-scale data processing. This book offers an in-depth overview of high-performance big data computing and the associated technical issues, approaches, and solutions.\n\nThe book covers basic concepts and necessary background knowledge, including data processing frameworks, storage systems, and hardware capabilities; offers a detailed discussion of technical issues in accelerating big data computing in terms of computation, communication, memory and storage, codesign, workload characterization and benchmarking, and system deployment and management; and surveys benchmarks and workloads for evaluating big data middleware systems. It presents a detailed discussion of big data computing systems and applications with high-performance networking, computing, and storage technologies, including state-of-the-art designs for data processing and storage systems. Finally, the book considers some advanced research topics in high-performance big data computing, including designing high-performance deep learning over big data (DLoBD) stacks and HPC cloud technologies.","2022","2023-10-18T21:43:58.973Z","2023-11-16T14:06:16.686Z","2023-03-02T20:52:09.042Z",37,"Data Science and Big Data Analytics: Making Data Driven Decisions","data-science-and-big-data-analytics","On-demand videos created by MIT xPRO.","MIT xPro","2023-10-18T21:49:02.202Z","2023-10-18T21:49:43.788Z","2023-03-16T17:36:17.384Z","tracking-sustainability-with-big-data-analytics","2023-04-27","2023-07-07T17:41:33.826Z",76,"using-ai-to-prevent-suicide","2023-04-25","2023-07-07T17:41:35.227Z",82,"using-big-data-and-ai-to-monitor-industrial-equipment","2023-04-14","2023-07-07T17:41:43.969Z",85,"Using Sensors to Spot Pollution Sources","using-sensors-to-spot-pollution-sources","2023-04-10","2023-07-07T17:41:48.445Z",114,"using-big-data-analytics-to-track-deadly-mosquitoes","2023-10-05","2 min read","2023-10-12T02:53:40.531Z",117,"using-data-analytics-to-understand-enrollment-trends","2023-09-29","2023-10-12T02:53:45.998Z",120,"using-big-data-analytics-to-improve-clinical-trial-recruitment","2023-09-05","2023-10-12T02:53:51.258Z",124,"using-big-data-analytics-to-visualize-demographic-disparities","2023-08-09","2023-10-12T02:53:59.006Z",128,"Using Big Data and AI to Improve Flight Scheduling","using-big-data-and-ai-to-improve-scheduling","10\u002F23\u002F2023","2023-10-23T18:27:29.915Z",136,"Using Data Analytics to Inform Community Health Priorities","using-data-analytics-to-inform-community-health-priorities","2023-10-30","2023-11-08T21:37:41.176Z",".jpeg",550,"Smart Cities","why-big-data-analytics","Next up",".pdf","application\u002Fpdf","Index","2023-10-18T21:49:06.796Z","content-blocks.article-course",47,"2023-03-03T20:51:04.299Z","Why big data analytics","2023-10-18T21:37:30.508Z","content-blocks.article-book","Notes",5.3,"AI","paragraph",3350,"heading",6315,1687,"Big Data Analytics Can't Answer All Questions Directly","A large and complicated question such as \"How can we increase sales?\" can be broken down into simpler queries for which the data is measurable and specific.",1212,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fbd_103_01_specific_queries_1f6f78cc88_a2c219730e.svg",3353,6317,3352,6319,3351,6316,67,"content-blocks.article-block-quote",6318,1754,1019,150,"2023-10-18T21:37:30.454Z",1200,"What happened?","Why does it matter?","None","Acknowledgments","References",105,"10",2327,"impact_spotlight_logo_v4_4f9d70024f.svg","impact_spotlight_logo_v4_4f9d70024f_dbf2f8be7f","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fimpact_spotlight_logo_v4_4f9d70024f_dbf2f8be7f.svg","Why AI","why-ai","Predictive Analytics","2023-10-18T21:49:37.260Z","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Faudio\u002FMITHorizonAudioCommonmisconceptionsaboutBigDataAnalytics.mp3","Data analytics can answer every question","data-analytics-can-answer-every-question","*The reality: Data analytics can help in answering narrow, specific questions. It is not as helpful for broad or abstract questions.*\n\nData analytics can provide answers to well-defined questions, such as Where do customers shop? It isnt as useful for broader or vaguer questions, such as How can we increase sales? Data analytics *can* still help an organization answer a question like thatbut incrementally, if it can be broken into smaller, more manageable queries, such as where do customers shop, what do they buy, and when do they buy it. Combining the answers to those narrow questions can allow an organization to begin addressing the larger one.\n\nEven when a large or abstract question can be divided into narrow, specific queries, however, data analytics may still be unable to provide useful answers. Some real-world circumstances include too many variables, in relationships too complex or mysterious, for experts to quantify. For example, in a 2019 [study](https:\u002F\u002Fwww.pnas.org\u002Fcontent\u002F117\u002F15\u002F8398#sec-1) published by PNAS, the official journal of the National Academy of Sciences, 160 teams of scientists were invited to answer a very big question: How would the lives of a cohort of children turn out? To help the scientists predict outcomes such as grade-point average and family evictions, they were provided with massive data sets collected over 15 years, including information about each childs health and development, education, family income, parental discipline, and sibling relationships; and transcripts of interviews with parents and caregivers. Scientists were invited to use any analytical techniques on that data, ranging from statistical analysis to artificial intelligence. Nonetheless, the accuracy of their predictions proved low. Despite having access to a rich data set, and despite using modern {%machine learning%}|A sophisticated type of artificial intelligence. For more, see our Artificial Intelligence topic, specifically \"How AI Works.\"| methods that are optimized for prediction, none of the 160 teams were able to make very accurate predictions, Matthew Salganik, a Princeton professor and one of the studys authors, tells MIT Horizon. The most accurate models were generally not much better than naive guessing.\n","bd_103_01_specific_queries_1f6f78cc88.svg","bd_103_01_specific_queries_1f6f78cc88_a2c219730e",63.08,"The more data the better","the-more-data-the-better","*The reality: A small amount of good data, paired with an effective process, can yield better insights than a lot of data managed badly.*\n\nA larger data set does not necessarily translate into better insights. In fact, many of the problems that organizations use big data analytics to solve can also be solved with smaller data sets and practical thinking. Focusing on *quantity* over *quality* is therefore a common mistake. Poor-quality data is a huge problem, Bruce Rogers, then chief insights officer at Forbes Media, [said](https:\u002F\u002Fwww.forbes.com\u002Fsites\u002Fforbespr\u002F2017\u002F05\u002F31\u002Fpoor-quality-data-imposes-costs-and-risks-on-businesses-says-new-forbes-insights-report\u002F?sh=560c5e7c452b) in 2017. It leaves many companies trying to navigate the information age in the equivalent of a horse and buggy.\n\nPoor-quality data includes typos, inaccuracies, and missing entries. (Such data may have been improperly *cleaned*. For more on data cleaning, see [Limitations](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Flimitations-bd).) But even if data is clean and accurate, it may simply be irrelevant to the question an organization is trying to answer. An organization hoping to better engage its customers doesnt need the customers every biographical detail before it can reach them. It needs only the details that correlate highly with their engagement.\n\nEven when data is clean, accurate, and relevant, lots of it may still prove too cumbersome (or resource intensive) for an organization to manage. Rather than race to collect more data, therefore, an organization should carefully consider the goal of its analytics project, what question it is trying to answer, and what information is necessary to answer it. Translating the real-world problem into the exact right engineering problem should be 80% of the effort, David Scheinker, a Stanford professor who works with hospitals to implement lessons from data, tells MIT Horizon. Using fancy mathematical tools should be 5%, and the remaining 15% should be iterating.\n","Analyzing big data requires building custom tools in-house","analyzing-big-data-requires-building-custom-tools-in-house","*The reality: Many forms of data analytics can be done with off-the-shelf products.*\n\nOrganizations embarking on big data analytics projects often assume they must build all the tools theyll use in-house. Doing so is expensive and laborious and can be daunting for a company that doesnt specialize in tech. But in many circumstances, its unnecessary. The most common mistake I see is people still running their own infrastructure, Abel Sanchez, an MIT research scientist who specializes in helping companies use big data analytics, tells MIT Horizon.\n\nA variety of off-the-shelf products are available for storing and analyzing data. Common examples come from Amazon, Google, and Microsoft. These providers and others typically deliver their services using remote servers, rather than depending on the equipment on the customers premises. Remotely accessed resources like this are often called in the cloud; for more, see [Cloud Computing](https:\u002F\u002Fhorizonapp.mit.edu\u002Ftopic\u002Fcloud-computing), and for an overview specifically of major providers and their services, see [Major Cloud Providers](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Fmajor-cloud-computing-service-providers).\n\nOne available service, for example, is *data warehousing*collecting and storing data in one secure place. Most organizations can outsource their data warehousing to a professional provider for $20 80 per {%terabyte%}|A terabyte stores roughly 250,000 photos or 6.5 million documents.| per month, depending on their needs. Those rates are significantly cheaper than what it would cost an organization to purchase, install, and maintain its own servers. (These costs are more difficult to average, since they include up-front equipment purchases and do not scale linearly in proportion to data. However, experts estimate a typical cost per terabyte in the $50120 range.)\n\nIn addition to saving on storage, organizations using off-the-shelf tools can also save on personnel costs. Data scientists typically command salaries of $100,000 to $200,000. For an organization that cannot afford its own expert staff, the same providers that offer data warehousing offer data analytics platforms that automatically perform some of the tasks a data scientist or other personnel would. All told, Sanchez, of MIT, estimates that by using off-the-shelf products instead of building tools in-house, organizations can save as much as 30%. \n","Big data analytics is only for large organizations","big-data-analytics-is-only-for-large-organizations","*The reality: Organizations of all sizes can usually access enough data for analytics, and learn from it.*\n\nA small or medium-sized organization can almost always learn from more data than it alone generates. It can do so by pairing the data it collects with data supplied by others. This allows small entities to combine their knowledge and learn from others in similar situations. \n\nBut even small organizations can often generate enough data to learn from through analytics. For example, Twiddy & Company, a family-owned business in North Carolina that employs about 130 full-time staff, is a longtime user of big data analytics. Twiddy & Company rents vacation homes on the Outer Banks; beginning in 2008, it worked with SAS, a data analytics platform, to help determine what price to rent homes at as well as when, where, and what to advertise. Eventually, the company hired its own internal data analytics team. (To learn more about Twiddy & Companys journey, see MIT Horizons archived [event](https:\u002F\u002Fhorizonapp.mit.edu\u002Fevent\u002Fhow-big-data-can-benefit-small-businesses-with-clark-twiddy) with company president Clark Twiddy.)\n","At our own small company   like big companies, weve used data at scale to make a measurably valuable impact to our customer experience. [Clark Twiddy to MIT Horizon](https:\u002F\u002Fhorizonapp.mit.edu\u002Fevent\u002Fhow-big-data-can-benefit-small-businesses-with-clark-twiddy)","Twiddy & Company [credits its data analytics strategy](https:\u002F\u002Fhospitalitytech.com\u002Fhow-vacation-rental-company-used-data-analytics-decrease-vacancies-twice-rate-its-competitors) with helping the company survive the COVID-19 pandemic, when the bridge linking the Outer Banks to the mainland closed to visitors, cutting off renters and revenue. While competitors (including national brands like Arbnb and Vrbo) reduced their advertising during the bridge closure, Twiddy used data from Google Analytics and other tools to continue advertising, but in a targeted way. If a potential customer searching for rentals had previously stayed at Twiddy property in a particular North Carolina town, Twiddy showed them ads for rentals in that town specifically. According to the company, efforts like this one quadrupled the value it got from its advertising budget. During the eight weeks the bridge was closed in spring 2020, Twiddy calculates it booked twice as many rentals as some competitors did, and was better positioned for both demand and price when the bridge reopened. Clark Twiddy says that data analytics helped the company make profitable decisions despite uncertain circumstances. We were all dealing with more stress, more anxiety than we ever had, he [said](https:\u002F\u002Fsmallbiztrends.com\u002F2020\u002F11\u002Fclark-twiddy-twiddy-co-interview.html) in November 2020. But we trusted our ability to make data-driven decisions.","- [Recent developments in big data analytics](https:\u002F\u002Fhorizonapp.mit.edu\u002Farticle\u002Frecent-developments-bd)",313,1211,428,406,"Introduction",142,70,"content-blocks.article-conference","11",338,"22","116","14",112,"Stefanie Jegelka","is an X-Consortium Career Development Assistant Professor in the Department of EECS at MIT. Her research interests lie in algorithmic machine learning, in particular in designing methods that exploit combinatorial, geometric or algebraic structure. She obtained her Ph.D. from ETH Zurich and the Max Planck Institutes Tuebingen and spent some time as a postdoctoral researcher at UC Berkeley. She has received an NSF CAREER Award, a DARPA Young Faculty Award, a Google Faculty Research Award, the German Pattern Recognition Award, and a Best Paper Award at the International Conference on Machine Learning.","5:39","Devavrat Shah",", his current research focus on large-scale statistical inference and stochastic networks. He earned a Ph.D. in Computer Science from Stanford University under the supervision of Balaji Prabhakar. He has received prestigious awards along his career such as the Presidents of India Gold Medal, the 2005 INFORMS George B. Dantzig best dissertation award, the 2008 ACM SIGMETRICS Rising Star award for his work on network scheduling algorithm, the 2010 INFORMS Erlang Prize, as well as various publications awards. He is a distinguished young alumni of his alma-mater IIT Bombay. He researched at Stanford, Berkeley, and MSRI, and he started teaching at MIT in Fall 2005. He co-founded Celect, Inc. in 2013 to commercialize his research at MIT.","Philippe Rigollet","works at the intersection of statistics, machine learning, and optimization, focusing primarily on the design and analysis of statistical methods for high-dimensional problems. His recent research focuses on the statistical limitations of learning under computational constraints. At the University of Paris VI, Rigollet earned a BS in statistics in 2001, a BS in applied mathematics in 2002, and a Ph.D. in mathematical statistics in 2006 under the supervision of Alexandre Tsybakov. He has held positions as a visiting assistant professor at the Georgia Institute of Technology and then as an assistant professor at Princeton University.","Cloud Computing","Education","Sustainability","Healthcare","Government",{},{},{},{},"",{},{},{},{},{},{},{},{},{},{},{},"2023-10-18T21:45:12.104Z","2023-10-18T21:45:15.311Z","2023-10-18T21:45:19.940Z","2023-10-18T21:45:20.520Z","2023-10-18T21:45:21.300Z","2023-10-18T21:45:22.612Z","2023-10-18T21:45:23.180Z","2023-10-18T21:45:29.142Z",258,"2023-10-18T21:45:37.375Z","BDA 101 Article Image Data Gathered","BD_101_01_big_data_growth_v5_a7c952941c.svg",438,"BD_101_01_big_data_growth_v5_a7c952941c_57dfcb4a3a",39.04,"https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002FBD_101_01_big_data_growth_v5_a7c952941c_57dfcb4a3a.svg",1213,"2023-10-18T21:37:30.617Z","Analyzing trends","Recommending products","Setting prices","Improving logistics and transportation","Structured data","Unstructured data","2023-10-18T21:39:57.064Z",2501,"2023-10-18T21:39:59.784Z",714,"2023-10-18T21:39:57.274Z","2023-10-18T21:39:56.877Z","2023-10-18T21:39:56.752Z","2023-10-18T21:39:54.244Z",1716,638,"2023-10-18T21:39:56.095Z",563,"2023-10-18T21:39:54.909Z","2023-10-18T21:39:53.236Z","2023-10-18T21:39:52.973Z","2023-10-18T21:39:53.034Z","2023-10-18T21:39:52.961Z",487,"2023-10-18T21:39:53.047Z",152,"2023-10-18T21:39:50.231Z","2023-10-18T21:39:51.183Z","2023-10-18T21:39:50.476Z","2023-10-18T21:39:52.147Z","Regulators criticized big data analytics companies on privacy grounds",1209,"2023-10-18T21:37:29.919Z",1210,"2023-10-18T21:37:30.085Z","2023-10-18T21:42:05.919Z","introduction",330,"2023-10-18T21:42:06.398Z",646,184,16.46,6.46,22.33,"2023-10-18T21:42:05.563Z","2023-10-18T21:42:05.355Z",231,"Free","$49 per month","Approximately 2 months","MIT Press",407,"2017",408,"- Weekly\n","2023-10-18T21:44:44.492Z","2023-10-18T21:44:44.456Z","15","9","Preface","17","167",814,507,"2023-10-18T21:44:26.828Z","2023-10-18T21:44:26.903Z","30","58","82","188","230","388","2023-10-18T21:44:37.935Z","2023-10-18T21:44:37.657Z","90","224","2023-10-18T21:44:32.317Z","2023-10-18T21:44:32.161Z","Internet of Things \u002F IoT","Additional Resources","2023-10-18T21:44:28.782Z","2023-10-18T21:44:28.948Z","Artificial Intelligence","106","132","180","206","2023-10-18T21:44:19.216Z","118","148","2023-10-18T21:44:13.049Z","2023-10-18T21:44:14.217Z","2023-10-18T21:44:09.084Z","2023-10-18T21:44:09.073Z",640,639,"2023-10-18T21:43:58.868Z","2023-10-18T21:43:58.744Z","2023-10-18T21:49:02.130Z","1 hr 10 min","2023-10-18T21:48:54.635Z","5:55","2023-10-18T21:48:54.616Z","5:46","2023-10-18T21:48:54.589Z","6:24","2023-10-18T21:48:54.559Z","5:24","2023-10-18T21:48:54.527Z","2023-10-18T21:48:54.498Z","2023-10-18T21:48:54.470Z","3:24","2023-10-18T21:48:54.443Z","2023-10-18T21:48:54.416Z","2023-10-18T21:48:54.395Z","2023-10-18T21:48:54.374Z","2023-10-18T21:48:54.345Z","2023-10-18T21:48:54.317Z","Tamara Broderick","is the ITT Career Development Assistant Professor at MIT. She earned a Ph.D. in statistics at UC Berkeley in 2014, an MS in computer science (UC Berkeley, 2013), an MPhil by research in physics (U Cambridge, 2009), a Master of Advanced Study in Mathematics (U Cambridge, 2008), and an AB in mathematics (Princeton University, 2007). Her recent research has focused on developing and analyzing models for scalable Bayesian machine learning---especially Bayesian nonparametrics. She has been awarded a Google Faculty Research Award, the ISBA Lifetime Members Junior Researcher Award, the Savage Award, the Evelyn Fix Memorial Medal and Citation, and a Marshall Scholarship.","2023-10-18T21:48:54.287Z","2023-10-18T21:48:54.260Z","2023-10-18T21:48:54.230Z","2023-10-18T21:48:54.203Z","2023-10-18T21:48:54.174Z","2023-10-18T21:48:54.142Z","2023-10-18T21:48:54.116Z","5:45","2023-10-18T21:48:54.083Z","2023-10-18T21:48:54.050Z","2023-10-18T21:48:54.012Z","2023-10-18T21:48:53.982Z","2023-10-18T21:48:53.957Z","2023-10-18T21:48:53.926Z","2023-10-18T21:48:53.896Z","2023-10-18T21:48:53.866Z","2023-10-18T21:48:53.835Z","2023-10-18T21:48:53.816Z","2023-10-18T21:48:53.787Z","2023-10-18T21:48:53.758Z","2023-10-18T21:48:53.729Z","Victor Chernozhukov","Victor Chernozhukov works in econometrics, mathematical statistics, and machine learning, with much of recent work focusing on the quantification of uncertainty in very high dimensional models. He is a fellow of The Econometric Society and a recipient of The Alfred P. Sloan Research Fellowship and The Arnold Zellner Award. He was elected to the American Academy of Arts and Sciences in April 2016.","2023-10-18T21:48:53.706Z","2023-10-18T21:48:53.691Z","2023-10-18T21:48:53.676Z","2023-10-18T21:48:53.655Z","2023-10-18T21:48:53.622Z","2023-10-18T21:48:53.589Z","2023-10-18T21:48:53.558Z","2023-10-18T21:48:53.531Z","2023-10-18T21:48:53.504Z","2023-10-18T21:48:53.479Z","2023-10-18T21:48:53.462Z","2023-10-18T21:48:53.435Z","2023-10-18T21:48:53.403Z","2023-10-18T21:48:53.375Z","2023-10-18T21:48:53.357Z","2023-10-18T21:48:53.335Z","2023-10-18T21:48:53.310Z","David Gamarnik","Professor, Sloan School of Management, IDSS, and the Operations Research Center David Gamarnik is a Nanyang Technological University Professor of Operations Research at the Operations Research and Statistics Group, Sloan School of Management of Massachusetts Institute of Technology. He received B.A. in mathematics from New York University in 1993 and Ph.D. in Operations Research from MIT in 1998. Since then he was a research staff member of IBM T.J. Watson Research Center, before joining MIT in 2005. His research interests include probability, theory of random graphs, optimization and algorithms, statistics and machine learning, stochastic processes and queueing theory. He is a recipient of the Erlang Prize and the Best Publication Award from the INFORMS 000 Probability Society, IBM Faculty Partnership Award and several NSF sponsored grants. He is currently an area editor of Operations Research journal, associate editor of Mathematics of Operations Research, and he has been an associate editor of Annals of Applied Probability, Queueing Systems and Stochastic Systems journals in the past.","Jon Kelner","received the B.A. in Mathematics from Harvard in 2002  and received the David Mumford Award as the top Harvard graduate in mathematics. He completed the M.S. and Ph.D. degrees from MIT in EECS in 2005 & 2006. Daniel Spielman was his thesis advisor. Kelner was a Member of IAS 2006-07, before joining the MIT faculty in applied mathematics as an assistant professor in 2007. He was promoted to associate professor in 2012. He continues to be a member of MIT-CSAIL. A theoretical computer scientist, Professor Kelners research focuses on fundamental mathematical problems related to algorithms and complexity theory. He received the Best Student Paper Award at STOC 04. In 2007, He was selected by the MIT School of Science for support from the NEC Corporation Fund for research in computers and communications. He received an Alfred P. Sloan research fellowship in 2010. In 2011, he was selected by MIT for the Harold E. Edgerton Faculty Achievement Award, given to a junior member of the MIT faculty, for exceptional distinction in teaching, research, and scholarship. In 2013, he received the School of Sciences Teaching Prize for Undergraduate Education.","2023-10-18T21:48:53.286Z","2023-10-18T21:48:53.259Z","6:31","2023-10-18T21:48:53.231Z","2023-10-18T21:48:53.200Z","2023-10-18T21:48:53.172Z","2023-10-18T21:48:53.147Z","Ankur Moitra","Ankur Moitra is an Assistant Professor in the Department of Mathematics at MIT and a Principal Investigator in the Computer Science and Artificial Intelligence Lab (CSAIL) who works at the intersection of theoretical computer science and machine learning. Before joining MIT, he was an NSF Computing and Innovation Fellow at the Institute for Advanced Study. He completed his doctorate and masters degrees at MIT in and 2009 respectively. He is the recipient of an Alfred P. Sloan Fellowship, the Edmund F. Kelley Research Award, a Google Research Award, an NSF CAREER award, a Fannie and John Hertz Foundation Fellowship and the George M. Sprowls Award and the William A. Martin Award for his doctoral and masters dissertations.","2023-10-18T21:48:53.119Z","2023-10-18T21:48:53.091Z","2023-10-18T21:48:53.066Z","2023-10-18T21:48:53.041Z","2023-10-18T21:48:53.004Z","2023-10-18T21:48:52.976Z","2023-10-18T21:48:52.951Z","2023-10-18T21:48:52.923Z","5:22","2023-10-18T21:48:52.896Z","2023-10-18T21:48:52.871Z","2023-10-18T21:48:52.840Z","2023-10-18T21:48:52.806Z","2023-10-18T21:48:52.778Z","2023-10-18T21:48:52.755Z","2023-10-18T21:48:52.730Z","2023-10-18T21:48:52.706Z","2023-10-18T21:48:52.688Z","2023-10-18T21:48:52.661Z","2023-10-18T21:48:52.633Z","2023-10-18T21:48:52.604Z","2023-10-18T21:48:52.580Z","2023-10-18T21:48:52.556Z","2023-10-18T21:48:52.529Z","2023-10-18T21:48:52.495Z","Why Big Data Analytics","Energy","https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-big-data-and-ai-to-monitor-industrial-equipment",458,229,344,"2023-10-18T21:49:16.310Z","IoT","Using IoT and AI to Monitor and Maintain Equipment","https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-iot-and-ai-to-monitor-and-maintain-equipment","Using AI to Detect Faulty Infrastructure","https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-ai-to-detect-faulty-infrastructure","2023-10-18T21:49:19.272Z","https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Ftracking-sustainability-with-big-data-analytics","https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-big-data-analytics-to-visualize-demographic-disparities","Medicine","https:\u002F\u002Fhorizonapp.mit.edu\u002Fimpact-spotlight\u002Fusing-ai-to-prevent-suicide",2352,"impact_spotlight_logo_v4_d501cc8a66.svg","impact_spotlight_logo_v4_d501cc8a66_53d9cf8936","https:\u002F\u002Flearn.cce.af.mil\u002F_media\u002Ffiles\u002Fimpact_spotlight_logo_v4_d501cc8a66_53d9cf8936.svg"));</script><script src="./Common Misconceptions _ MIT Horizon_files/9fc7f3d.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/834a546.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/80fc188.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/6c1e899.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/2148f53.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/6bb4c98.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/c60dc67.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/c5d4f58.js.download" defer=""></script><script src="./Common Misconceptions _ MIT Horizon_files/7dfbd03.js.download" defer=""></script>
  

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>